{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e522aa3f",
   "metadata": {},
   "source": [
    "# Progetto Few-Shot Logo Recognition: Valutazione Baseline ResNet50\n",
    "\n",
    "### 1. Abstract e Obiettivo\n",
    "Questo notebook ha l'obiettivo di stabilire una **baseline (linea di base)** per il task di *Few-Shot Logo Recognition* utilizzando il dataset **LogoDet-3K**.\n",
    "Prima di procedere con il fine-tuning o la modifica dell'architettura della rete, è fondamentale misurare le performance di un modello standard pre-addestrato. I risultati ottenuti in questo notebook serviranno come metro di paragone per valutare l'efficacia delle future ottimizzazioni.\n",
    "\n",
    "### 2. Metodologia\n",
    "L'approccio utilizzato in questa fase è di tipo **\"Off-the-shelf Feature Extraction\"**:\n",
    "* **Modello:** Viene utilizzata una **ResNet50** pre-addestrata su *ImageNet*.\n",
    "* **Feature Extraction:** Il livello di classificazione finale (Fully Connected) viene rimosso e sostituito con una funzione identità. Invece di predire le 1000 classi di ImageNet, il modello restituisce il vettore delle feature (embedding) di dimensione **2048**.\n",
    "* **Inferenza:** La classificazione avviene tramite **Similarità del Coseno**. Viene calcolata la distanza tra l'embedding dell'immagine di *query* (da classificare) e l'embedding dell'immagine di *supporto* (esempio noto del brand).\n",
    "* **Protocollo:** Il testing segue un approccio *episodico* (N-Way, K-Shot) simulato su 1000 episodi per garantire robustezza statistica.\n",
    "\n",
    "### 3. Struttura del Notebook\n",
    "Il codice è organizzato nelle seguenti sezioni logiche:\n",
    "\n",
    "1.  **Configurazione e Setup:** Definizione dei parametri globali (seed, path, device) e setup dell'ambiente (Google Drive).\n",
    "2.  **Gestione del Dataset:**\n",
    "    * `DatasetTest`: Classe PyTorch per il caricamento delle immagini.\n",
    "    * `FewShotIterator`: Logica per la creazione degli episodi (Support Set vs Query Set).\n",
    "3.  **Metriche di Valutazione:** Implementazione della classe `MetricEvaluator` per il calcolo di Accuracy, mAP, F1-Score, Precision, Recall e Discriminant Ratio (J).\n",
    "4.  **Definizione Modello Baseline:** Funzione `get_baseline_resnet50` per il caricamento del modello e la modifica dell'head.\n",
    "5.  **Loop di Valutazione:** Esecuzione del test e aggregazione dei risultati finali.\n",
    "\n",
    "---\n",
    "**Dataset:** LogoDet-3K | **Modello:** ResNet50 (Frozen) | **Metrica Principale:** Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcfec627",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import cycle\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from google.colab import drive\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "class Config:\n",
    "    # 1. SETUP\n",
    "    project_name = \"FewShot_Evaluation\"\n",
    "    seed = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 2. DATASET PATH\n",
    "    dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
    "    csv_index_path = \"LogoDet-3K/brand_to_index.csv\"\n",
    "\n",
    "    # 3. MODEL PARAMETERS\n",
    "    embedding_dim = 128\n",
    "    pretrained = True\n",
    "    freeze_layers = 5\n",
    "    trained_model_path = \"\"\n",
    "\n",
    "    # 4. EVALUATION SETTINGS\n",
    "    prediciton_threashold = 0.5\n",
    "    n_shot = 1\n",
    "    num_episodes = 1000\n",
    "\n",
    "torch.manual_seed(Config.seed)\n",
    "random.seed(Config.seed)\n",
    "\n",
    "def setup_dataset(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    Mounts Google Drive and extracts the dataset if not already present.\n",
    "    \"\"\"\n",
    "    # 1. Mount Google Drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "    # 2. Check if the folder already exists\n",
    "    if os.path.exists(extract_to):\n",
    "        print(f\"Dataset folder '{extract_to}' already exists. Skipping extraction.\")\n",
    "    else:\n",
    "        print(f\"Extracting dataset from {zip_path}...\")\n",
    "        if os.path.exists(zip_path):\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(extract_to)\n",
    "            print(\"Extraction complete.\")\n",
    "        else:\n",
    "            print(f\"ERROR: Zip file not found at {zip_path}. Check your path.\")\n",
    "\n",
    "setup_dataset(\"/content/drive/MyDrive/LogoDet-3K-divided.zip\", \"/content/LogoDet-3K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed7a0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/Lenovo/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class DatasetTest(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    # Load label string to index mapping\n",
    "        df = pd.read_csv(Config.csv_index_path)\n",
    "        self.label_to_id = {row['brand']: int(row['index']) for _, row in df.iterrows()}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        img = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        image_label = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "        label_idx = self.label_to_id[image_label]\n",
    "\n",
    "        return {\"image\": img, \"label\": label_idx}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.load_image(self.file_list[idx])\n",
    "\n",
    "\n",
    "def getTestPaths(root_dir, total_set_size=None, min_images_per_brand=2):\n",
    "    test_path = os.path.join(root_dir, 'test')\n",
    "    test_brand_list = []\n",
    "\n",
    "    # Collect brand folders\n",
    "    if not os.path.exists(test_path):\n",
    "        print(f\"Warning: {test_path} not found.\")\n",
    "        return []\n",
    "\n",
    "    for category in os.listdir(test_path):\n",
    "        cat_path = os.path.join(test_path, category)\n",
    "        if os.path.isdir(cat_path):\n",
    "            for brand in os.listdir(cat_path):\n",
    "                brand_full_path = os.path.join(cat_path, brand)\n",
    "                if os.path.isdir(brand_full_path):\n",
    "                    test_brand_list.append(brand_full_path)\n",
    "\n",
    "    test_data_list = []\n",
    "\n",
    "    # Sampling Logic\n",
    "    if total_set_size is not None:\n",
    "        images_per_brand = round(total_set_size / len(test_brand_list))\n",
    "\n",
    "        if images_per_brand < min_images_per_brand:\n",
    "            new_test_brand_count = round(total_set_size / min_images_per_brand)\n",
    "            test_brand_list = random.sample(test_brand_list, min(len(test_brand_list), new_test_brand_count))\n",
    "            images_per_brand = min_images_per_brand\n",
    "\n",
    "        for brand in test_brand_list:\n",
    "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
    "\n",
    "            if len(imgs) < min_images_per_brand:\n",
    "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the TEST set\")\n",
    "\n",
    "            test_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
    "    else:\n",
    "        for brand in test_brand_list:\n",
    "            test_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
    "\n",
    "    return test_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_embeddings(model, file_list, transform, device):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Simple linear dataset of all unique test images\n",
    "    dataset = DatasetTest(file_list, transform)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images = data[\"image\"].to(device)\n",
    "            labels = data[\"label\"]\n",
    "\n",
    "            # Extract and move to CPU to save VRAM\n",
    "            embeddings = F.normalize(model(images), p=2, dim=1).cpu()\n",
    "\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_embeddings), torch.cat(all_labels)\n",
    "\n",
    "def cosine_similarity(averaged_support_embeddings, query_embeddings_tensor):\n",
    "\n",
    "    # Normalize embeddings if you want cosine similarity\n",
    "    support_emb_norm = F.normalize(averaged_support_embeddings, p=2, dim=0)       # [embedding_dim]\n",
    "    query_emb_norm = F.normalize(query_embeddings_tensor, p=2, dim=1)             # [num_queries, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sims = torch.matmul(query_emb_norm, support_emb_norm)  # [num_queries]\n",
    "    return sims\n",
    "\n",
    "def evaluate_few_shot(model, fewshot_iterator, transform, device, num_episodes=100):\n",
    "    evaluator = MetricEvaluator(device=device)\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    r_at_95p = []\n",
    "    ap_scores = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    unique_paths = list(fewshot_iterator.all_files_set)\n",
    "    embs, labels = compute_global_embeddings(model, unique_paths, transform, device)\n",
    "    j_score = evaluator.compute_discriminant_ratio(embs, labels)\n",
    "\n",
    "    # 2. Set to eval mode and disable gradient tracking\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_episodes):\n",
    "\n",
    "            task = fewshot_iterator()\n",
    "\n",
    "            if task is None:\n",
    "                print(f\"Stopped early at episode {i} because we ran out of brands.\")\n",
    "                break\n",
    "\n",
    "            support_paths = task[\"support_set\"]\n",
    "            query_paths = task[\"query_set\"]\n",
    "\n",
    "            # Build datasets and loaders\n",
    "            support_dataset = DatasetTest(support_paths, transform)\n",
    "            query_dataset = DatasetTest(query_paths, transform)\n",
    "\n",
    "            support_loader = DataLoader(support_dataset, batch_size=32)\n",
    "            query_loader = DataLoader(query_dataset, batch_size=64)\n",
    "\n",
    "            # Extract embeddings\n",
    "            support_embeddings = []\n",
    "            query_embeddings = []\n",
    "            query_labels = []\n",
    "            support_labels = []\n",
    "\n",
    "            # Compute embeddings for support set\n",
    "            for data in support_loader:\n",
    "                images = data[\"image\"].to(device)\n",
    "                support_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
    "\n",
    "                batch_labels = data[\"label\"]\n",
    "                support_labels.append(batch_labels)\n",
    "                support_brand = batch_labels[0]\n",
    "\n",
    "            support_embeddings_tensor = torch.cat(support_embeddings)\n",
    "            support_labels_tensor = torch.cat(support_labels)\n",
    "\n",
    "            # Average embeddings\n",
    "            averaged_support_embeddings = support_embeddings_tensor.mean(dim=0)\n",
    "            averaged_support_embeddings_for_ap = support_embeddings_tensor.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # Compute embeddings for query set\n",
    "            for data in query_loader:\n",
    "                images = data[\"image\"].to(device)\n",
    "                query_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
    "\n",
    "                batch_labels = data[\"label\"]\n",
    "                query_labels.append(batch_labels)\n",
    "\n",
    "            # query_embeddings and query_labels are list of tensors, this unrolls them\n",
    "            query_embeddings_tensor = torch.cat(query_embeddings)\n",
    "            query_labels_tensor = torch.cat(query_labels)\n",
    "\n",
    "            # mAP\n",
    "            ap_score_single = evaluator.compute_map(\n",
    "                query_emb=averaged_support_embeddings_for_ap,\n",
    "                gallery_emb=query_embeddings_tensor,\n",
    "                query_labels=support_labels_tensor,\n",
    "                gallery_labels=query_labels_tensor\n",
    "            )\n",
    "\n",
    "            ap_scores.append(ap_score_single)\n",
    "\n",
    "            # Compute similarity\n",
    "            sims = cosine_similarity(averaged_support_embeddings, query_embeddings_tensor)\n",
    "\n",
    "            # Ground truth: query belongs to support brand?\n",
    "            gt = (query_labels_tensor == support_brand).float()\n",
    "\n",
    "            # Predictions, does the model predict it is the same brand?\n",
    "            pred = (sims >= Config.prediciton_threashold).float().cpu()\n",
    "\n",
    "            # Accuracy\n",
    "            acc = (pred == gt).float().mean().item()\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            # Precision, Recall, F1\n",
    "            prec, rec = evaluator.compute_precision_recall(sims, gt, threshold=Config.prediciton_threashold)\n",
    "            f1 = evaluator.compute_f1_score(prec, rec)\n",
    "            r95 = evaluator.compute_recall_at_fixed_precision(sims, gt, min_precision=0.95)\n",
    "\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "            f1_scores.append(f1)\n",
    "            r_at_95p.append(r95)\n",
    "\n",
    "    # Aggregate results\n",
    "    results = {\n",
    "        \"accuracy\": sum(accuracies) / len(accuracies),\n",
    "        \"precision\": sum(precisions) / len(precisions),\n",
    "        \"recall\": sum(recalls) / len(recalls),\n",
    "        \"f1\": sum(f1_scores) / len(f1_scores),\n",
    "        \"r@95p\": sum(r_at_95p) / len(r_at_95p),\n",
    "        \"map\": sum(ap_scores) / len(ap_scores),\n",
    "        \"J\": j_score,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "class MetricEvaluator:\n",
    "    \"\"\"\n",
    "    A class to calculate evaluation metrics for Few-Shot Learning and Metric Learning.\n",
    "\n",
    "    Implements:\n",
    "    1. Discriminant Ratio (J): Optimized scalar implementation (O(d) memory).\n",
    "    2. Mean Average Precision (mAP): Ranking quality metric.\n",
    "    3. Recall at Fixed Precision (R@P): Operational metric.\n",
    "    4. Precision & Recall: Raw metrics at a specific similarity threshold.\n",
    "    5. F1 Score: Harmonic mean of Precision and Recall.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator.\n",
    "\n",
    "        Args:\n",
    "            device (str): 'cuda' or 'cpu'. If None, detects automatically.\n",
    "        \"\"\"\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.epsilon = 1e-6  # For numerical stability\n",
    "\n",
    "    def compute_discriminant_ratio(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Calculates the Discriminant Ratio (J) using the optimized Scalar approach.\n",
    "\n",
    "        Theory:\n",
    "            J = Tr(Sb) / Tr(Sw)\n",
    "            Using the Trace Trick: Tr(Sw) = Tr(St) - Tr(Sb)\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): Tensor of shape (Batch_Size, Dimension).\n",
    "            labels (torch.Tensor): Tensor of class labels.\n",
    "\n",
    "        Returns:\n",
    "            float: The Discriminant Ratio score.\n",
    "        \"\"\"\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        # 1. Global Mean Computation\n",
    "        global_mean = embeddings.mean(dim=0)\n",
    "\n",
    "        # 2. Calculate Trace of Total Scatter (St)\n",
    "        # Sum of squared Euclidean distances of all points from the global mean.\n",
    "        tr_st = torch.sum((embeddings - global_mean) ** 2)\n",
    "\n",
    "        # 3. Calculate Trace of Between-Class Scatter (Sb)\n",
    "        tr_sb = 0\n",
    "        unique_classes = torch.unique(labels)\n",
    "\n",
    "        for c in unique_classes:\n",
    "            class_mask = (labels == c)\n",
    "            class_embeddings = embeddings[class_mask]\n",
    "            n_c = class_embeddings.size(0)\n",
    "\n",
    "            if n_c > 0:\n",
    "                mu_c = class_embeddings.mean(dim=0)\n",
    "                tr_sb += n_c * torch.sum((mu_c - global_mean) ** 2)\n",
    "\n",
    "        # 4. Calculate Trace of Within-Class Scatter (Sw)\n",
    "        tr_sw = tr_st - tr_sb\n",
    "\n",
    "        # Calculate J\n",
    "        j_score = tr_sb / (tr_sw + self.epsilon)\n",
    "\n",
    "        return j_score.item()\n",
    "\n",
    "    def compute_map(self, query_emb, gallery_emb, query_labels, gallery_labels):\n",
    "        \"\"\"\n",
    "        Calculates Mean Average Precision (mAP).\n",
    "        \"\"\"\n",
    "        query_emb = query_emb.to(self.device)\n",
    "        gallery_emb = gallery_emb.to(self.device)\n",
    "        query_labels = query_labels.to(self.device)\n",
    "        gallery_labels = gallery_labels.to(self.device)\n",
    "\n",
    "        # L2 Normalize for Cosine Similarity\n",
    "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "        gallery_emb = F.normalize(gallery_emb, p=2, dim=1)\n",
    "\n",
    "        # Similarity Matrix: S = Q * G^T\n",
    "        similarity_matrix = torch.matmul(query_emb, gallery_emb.T)\n",
    "\n",
    "        num_queries = query_labels.size(0)\n",
    "        average_precisions = []\n",
    "\n",
    "        for i in range(num_queries):\n",
    "            scores = similarity_matrix[i]\n",
    "            target_label = query_labels[i]\n",
    "\n",
    "            # Ranking\n",
    "            sorted_indices = torch.argsort(scores, descending=True)\n",
    "            sorted_gallery_labels = gallery_labels[sorted_indices]\n",
    "\n",
    "            # Relevance Mask\n",
    "            relevance_mask = (sorted_gallery_labels == target_label).float()\n",
    "\n",
    "            total_relevant = relevance_mask.sum()\n",
    "            if total_relevant == 0:\n",
    "                average_precisions.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Cumulative Precision\n",
    "            cumsum = torch.cumsum(relevance_mask, dim=0)\n",
    "            ranks = torch.arange(1, len(relevance_mask) + 1).to(self.device)\n",
    "            precisions = cumsum / ranks\n",
    "\n",
    "            # Average Precision (AP)\n",
    "            ap = (precisions * relevance_mask).sum() / total_relevant\n",
    "            average_precisions.append(ap.item())\n",
    "\n",
    "        if not average_precisions:\n",
    "            return 0.0\n",
    "        return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    def compute_precision_recall(self, similarity_scores, is_match, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Calculates raw Precision and Recall at a specific similarity threshold.\n",
    "\n",
    "        Definitions:\n",
    "            Precision = TP / (TP + FP)\n",
    "            Recall    = TP / (TP + FN)\n",
    "\n",
    "        Args:\n",
    "            similarity_scores (torch.Tensor): 1D tensor of scores (0.0 to 1.0).\n",
    "            is_match (torch.Tensor): 1D binary tensor (Ground Truth).\n",
    "            threshold (float): Cutoff for deciding if a retrieval is Positive.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (precision, recall)\n",
    "        \"\"\"\n",
    "        similarity_scores = similarity_scores.to(self.device)\n",
    "        is_match = is_match.to(self.device)\n",
    "\n",
    "        # Binarize predictions: 1 if score >= threshold (Positive), else 0 (Negative)\n",
    "        predicted_positive = (similarity_scores >= threshold).float()\n",
    "\n",
    "        # True Positives (TP): Predicted Positive AND Actually Match\n",
    "        tp = (predicted_positive * is_match).sum()\n",
    "\n",
    "        # False Positives (FP): Predicted Positive BUT Actually Non-Match\n",
    "        fp = (predicted_positive * (1 - is_match)).sum()\n",
    "\n",
    "        # False Negatives (FN): Predicted Negative BUT Actually Match\n",
    "        # (We invert the prediction mask to find negatives)\n",
    "        fn = ((1 - predicted_positive) * is_match).sum()\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        return precision.item(), recall.item()\n",
    "\n",
    "    def compute_recall_at_fixed_precision(self, similarity_scores, is_match, min_precision=0.95):\n",
    "        \"\"\"\n",
    "        Calculates Recall at a Fixed Precision (R@P).\n",
    "        Finds the lowest threshold where Precision >= min_precision.\n",
    "        \"\"\"\n",
    "        similarity_scores = similarity_scores.to(self.device)\n",
    "        is_match = is_match.to(self.device)\n",
    "\n",
    "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "        sorted_matches = is_match[sorted_indices]\n",
    "\n",
    "        tps = torch.cumsum(sorted_matches, dim=0)\n",
    "        total_retrieved = torch.arange(1, len(sorted_matches) + 1).to(self.device)\n",
    "\n",
    "        precisions = tps / total_retrieved\n",
    "\n",
    "        # Find indices where Precision satisfies the constraint\n",
    "        valid_indices = torch.where(precisions >= min_precision)[0]\n",
    "\n",
    "        if len(valid_indices) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        cutoff_index = valid_indices[-1]\n",
    "\n",
    "        # Recall = TP_at_cutoff / Total_Relevant_In_Dataset\n",
    "        total_relevant_in_dataset = is_match.sum()\n",
    "\n",
    "        if total_relevant_in_dataset == 0:\n",
    "            return 0.0\n",
    "\n",
    "        recall = tps[cutoff_index] / total_relevant_in_dataset\n",
    "\n",
    "        return recall.item()\n",
    "\n",
    "    def compute_f1_score(self, precision, recall):\n",
    "        \"\"\"\n",
    "        Calculates F1 Score (Harmonic Mean).\n",
    "        \"\"\"\n",
    "        if (precision + recall) == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "\n",
    "class FewShotIterator:\n",
    "    def __init__(self, file_list, n_shot):\n",
    "        \"\"\"\n",
    "        Initializes the iterator class.\n",
    "        It prepares the global testset and creates a cyclic iterator over the valid brands.\n",
    "        \"\"\"\n",
    "        self.n_shot = n_shot\n",
    "\n",
    "        # 1. Validation: Check if input list is empty\n",
    "        if not file_list:\n",
    "            raise ValueError(\"The test file list is empty.\")\n",
    "\n",
    "        #    (Dataset - SupportSet)  is significantly faster with sets (O(1)) compared to lists.\n",
    "        self.all_files_set = set(file_list)\n",
    "\n",
    "        # 3. Organize data by Brand\n",
    "        #    We create a dictionary mapping: { 'BrandName': [list_of_image_paths] }\n",
    "        self.brands_map = {}\n",
    "\n",
    "        for file_path in file_list:\n",
    "            # Extract brand name assuming structure: .../Category/Brand/Image.jpg\n",
    "            brand_name = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "            if brand_name not in self.brands_map:\n",
    "                self.brands_map[brand_name] = []\n",
    "            self.brands_map[brand_name].append(file_path)\n",
    "\n",
    "        self.valid_brands_list = list(self.brands_map.keys())\n",
    "\n",
    "        if not self.valid_brands_list:\n",
    "            raise ValueError(f\"No brand found with more than {n_shot} images.\")\n",
    "\n",
    "        #    'itertools.cycle' creates an infinite loop over the valid brands list.\n",
    "        self.brand_iterator = cycle(self.valid_brands_list)\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Executed when the class instance is called.\n",
    "        Logic:\n",
    "        1. Pick next brand (Sequential).\n",
    "        2. Pick Support Set (Random 5 images from that brand).\n",
    "        3. Pick Query Set (EVERYTHING else in the testset).\n",
    "        \"\"\"\n",
    "        # A. Get the next brand sequentially from the cycle\n",
    "        try:\n",
    "            selected_brand_name = next(self.brand_iterator)\n",
    "        except StopIteration:\n",
    "            # Gracefully signal that we are done\n",
    "            print(\"Iterator finished: All brands have been processed.\")\n",
    "            return None\n",
    "\n",
    "        # B. Retrieve all images specific to this chosen brand\n",
    "        images_of_current_brand = self.brands_map[selected_brand_name]\n",
    "\n",
    "        # Select a random number between 1 to 5 which is the number of images of the support brand guaranteed in the query set\n",
    "        num_query_guarantee_if_available = random.randint(1, 5)\n",
    "\n",
    "        # C. Create SUPPORT SET\n",
    "        #    Select 'n_shot' unique images randomly from the current brand.\n",
    "        support_set_list = random.sample(images_of_current_brand, self.n_shot)\n",
    "        support_set_set = set(support_set_list)\n",
    "\n",
    "        # D. Create QUERY SET (Global Subtraction)\n",
    "        #    Requirement: The Query Set contains 50 images of which at least 1 is from the support brand\n",
    "        #    Step 1: initialize the Query list\n",
    "        query_set_list = []\n",
    "\n",
    "        #    Step 2: Sample randomly the images to guarantee in the query set\n",
    "        remaining_brand_images = list(set(images_of_current_brand) - support_set_set)\n",
    "        guaranteed_images_in_query = random.sample(remaining_brand_images, min(num_query_guarantee_if_available, len(remaining_brand_images)))\n",
    "\n",
    "        if (len(remaining_brand_images) == 0):\n",
    "            print(f\"for the brand {selected_brand_name} {len(remaining_brand_images)} images where put in the query set\")\n",
    "\n",
    "        #    Step 3: Sample the Negative Queries (Distractors from OTHER brands)\n",
    "        # We subtract ALL images of the current brand to ensure zero accidental matches\n",
    "        remaining_images_in_query = list(self.all_files_set - set(images_of_current_brand) - set(guaranteed_images_in_query))\n",
    "\n",
    "        total_query_size = 50\n",
    "        num_remaining_query = total_query_size - min(num_query_guarantee_if_available, len(remaining_brand_images))\n",
    "        query_remaining = random.sample(remaining_images_in_query,  min(len(remaining_images_in_query),num_remaining_query))\n",
    "\n",
    "        #    Step 4: Combine and Shuffle\n",
    "        query_set_list = guaranteed_images_in_query + query_remaining\n",
    "        random.shuffle(query_set_list)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"brand_name\": selected_brand_name,\n",
    "            \"support_set\": support_set_list,\n",
    "            \"query_set\": query_set_list\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_baseline_resnet50(device):\n",
    "    \"\"\"\n",
    "    Scarica la ResNet50 originale pre-addestrata su ImageNet.\n",
    "    Sostituisce il layer fully connected (fc) con Identity per restituire\n",
    "    i feature embeddings (dimensione 2048) invece delle classi.\n",
    "    \"\"\"\n",
    "    print(\"Scaricamento ResNet50 originale (ImageNet Weights)...\")\n",
    "    \n",
    "    # 1. Carica i pesi di default (ImageNet)\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    \n",
    "    # 2. Rimuovi la testa di classificazione (1000 classi)\n",
    "    # Sostituendola con Identity, l'output del modello sarà l'output \n",
    "    # del layer precedente (avgpool), ovvero un vettore di dimensione 2048.\n",
    "    model.fc = nn.Identity()\n",
    "    \n",
    "    # 3. Configurazione per inferenza\n",
    "    model.to(device)\n",
    "    model.eval() # Imposta batchnorm e dropout in modalità valutazione\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # 1. Configurazione Dispositivo e Trasformazioni\n",
    "    # Usa le stesse trasformazioni standard di ImageNet usate nel training/testing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    device = torch.device(Config.device if hasattr(Config, 'device') else 'cuda')\n",
    "\n",
    "    # 2. Istanzia il modello Baseline\n",
    "    model = get_baseline_resnet50(device)\n",
    "\n",
    "    # 3. Preparazione Dati\n",
    "    # Recupera i path e crea l'iteratore usando le tue funzioni esistenti\n",
    "    print(\"Preparazione dati per il testing...\")\n",
    "    test_paths = getTestPaths(Config.dataset_root) \n",
    "    iterator = FewShotIterator(test_paths, n_shot=Config.n_shot)\n",
    "\n",
    "    # 4. Esecuzione del Loop di Testing\n",
    "    # Il loop 'evaluate_few_shot' riceverà vettori da 2048 dimensioni.\n",
    "    # Funzionerà correttamente perché usa cosine_similarity che è agnostica alla dimensione.\n",
    "    print(f\"Avvio calcolo Baseline su {Config.num_episodes} episodi...\")\n",
    "    \n",
    "    results = evaluate_few_shot(\n",
    "        model=model, \n",
    "        fewshot_iterator=iterator, \n",
    "        transform=transform, \n",
    "        device=device, \n",
    "        num_episodes=Config.num_episodes\n",
    "    )\n",
    "\n",
    "    # 5. Stampa dei Risultati\n",
    "    print(\"\\n=== Risultati Baseline (ResNet50 'Off-the-shelf') ===\")\n",
    "    for metrica, valore in results.items():\n",
    "        print(f\"{metrica.capitalize():<15}: {valore:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
