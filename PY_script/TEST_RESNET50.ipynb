{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e522aa3f",
   "metadata": {},
   "source": [
    "# Few-Shot Logo Recognition Project: ResNet50 Baseline Evaluation\n",
    "\n",
    "### 1. Abstract and Objective\n",
    "This notebook aims to establish a **baseline** for the *Few-Shot Logo Recognition* task using the **LogoDet-3K** dataset.\n",
    "Before proceeding with fine-tuning or modifying the network architecture, it is crucial to measure the performance of a standard pre-trained model. The results obtained in this notebook will serve as a benchmark to evaluate the effectiveness of future optimizations.\n",
    "\n",
    "### 2. Methodology\n",
    "The approach used in this phase is **\"Off-the-shelf Feature Extraction\"**:\n",
    "* **Model:** A **ResNet50** pre-trained on *ImageNet* is used.\n",
    "* **Feature Extraction:** The final classification layer (Fully Connected) is removed and replaced with an identity function. Instead of predicting the 1000 ImageNet classes, the model returns the feature vector (embedding) of dimension **2048**.\n",
    "* **Inference:** Classification is performed via **Cosine Similarity**. The distance between the *query* image embedding (to be classified) and the *support* image embedding (known brand example) is calculated.\n",
    "* **Protocol:** Testing follows an *episodic* approach (N-Way, K-Shot) simulated over 1000 episodes to ensure statistical robustness.\n",
    "\n",
    "### 3. Notebook Structure\n",
    "The code is organized into the following logical sections:\n",
    "\n",
    "1.  **Configuration and Setup:** Definition of global parameters (seed, path, device) and environment setup.\n",
    "2.  **Dataset Management:**\n",
    "    * `DatasetTest`: PyTorch class for loading images.\n",
    "    * `FewShotIterator`: Logic for creating episodes (Support Set vs Query Set).\n",
    "3.  **Evaluation Metrics:** Implementation of the `MetricEvaluator` class for calculating Accuracy, mAP, F1-Score, Precision, Recall, and Discriminant Ratio (J).\n",
    "4.  **Baseline Model Definition:** `get_baseline_resnet50` function for loading the model and modifying the head.\n",
    "5.  **Evaluation Loop:** Execution of the test and aggregation of final results.\n",
    "\n",
    "---\n",
    "**Dataset:** LogoDet-3K | **Model:** ResNet50 (Frozen) | **Main Metric:** Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcfec627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione file indice CSV da: ./LogoDet-3K...\n",
      "✅ File CSV generato con successo: LogoDet-3K/brand_to_index.csv (3000 brand mappati)\n",
      "✅ Dataset trovato correttemente in: c:\\Users\\Lenovo\\Desktop\\ProgettoFinale1\\PY_script\\LogoDet-3K\n",
      "✅ CSV index trovato.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import cycle\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "#from google.colab import drive\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "class Config:\n",
    "    # 1. SETUP\n",
    "    project_name = \"FewShot_Evaluation\"\n",
    "    seed = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 2. DATASET PATH\n",
    "    #dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
    "    dataset_root = \"./LogoDet-3K\"\n",
    "    \n",
    "    csv_index_path = \"LogoDet-3K/brand_to_index.csv\"\n",
    "    # 3. MODEL PARAMETERS\n",
    "    embedding_dim = 128\n",
    "    pretrained = True\n",
    "    freeze_layers = 5\n",
    "    trained_model_path = \"\"\n",
    "\n",
    "    # 4. EVALUATION SETTINGS\n",
    "    prediciton_threashold = 0.5\n",
    "    n_shot = 1\n",
    "    num_episodes = 1000\n",
    "\n",
    "torch.manual_seed(Config.seed)\n",
    "random.seed(Config.seed)\n",
    "\n",
    "#def setup_dataset(zip_path, extract_to):\n",
    "#    \"\"\"\n",
    "#    Mounts Google Drive and extracts the dataset if not already present.\n",
    "#    \"\"\"\n",
    "#    # 1. Mount Google Drive\n",
    "#    if not os.path.exists('/content/drive'):\n",
    "#        drive.mount('/content/drive')\n",
    "#\n",
    "#    # 2. Check if the folder already exists\n",
    "#    if os.path.exists(extract_to):\n",
    "#        print(f\"Dataset folder '{extract_to}' already exists. Skipping extraction.\")\n",
    "#    else:\n",
    "#        print(f\"Extracting dataset from {zip_path}...\")\n",
    "#        if os.path.exists(zip_path):\n",
    "#            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#                zip_ref.extractall(extract_to)\n",
    "#            print(\"Extraction complete.\")\n",
    "#        else:\n",
    "#            print(f\"ERROR: Zip file not found at {zip_path}. Check your path.\")\n",
    "def setup_dataset():\n",
    "    \"\"\"\n",
    "    Verifica semplicemente se la cartella del dataset esiste in locale.\n",
    "    Non scarica né decomprime nulla.\n",
    "    \"\"\"\n",
    "    # Verifica esistenza dataset\n",
    "    if os.path.exists(Config.dataset_root):\n",
    "        print(f\"✅ Dataset trovato correttemente in: {os.path.abspath(Config.dataset_root)}\")\n",
    "    else:\n",
    "        print(f\"❌ ERRORE: La cartella '{Config.dataset_root}' non è stata trovata.\")\n",
    "        print(\"Assicurati che il nome della cartella in Config.dataset_root corrisponda esattamente a quella sul tuo PC.\")\n",
    "        \n",
    "    # Verifica esistenza CSV (opzionale ma utile)\n",
    "    # Se il csv è dentro la cartella del dataset, aggiusta il path in Config\n",
    "    if os.path.exists(Config.csv_index_path):\n",
    "        print(f\"✅ CSV index trovato.\")\n",
    "    else:\n",
    "        print(f\"⚠️ ATTENZIONE: File CSV '{Config.csv_index_path}' non trovato. Controlla il percorso.\")\n",
    "\n",
    "# Esegui il setup (che ora è solo un controllo)\n",
    "def generate_brand_index_csv(root_dir, csv_output_path):\n",
    "    \"\"\"\n",
    "    Scansiona le cartelle dei brand e genera automaticamente il file CSV\n",
    "    brand_to_index.csv necessario per il mapping (Brand -> ID Numerico).\n",
    "    \"\"\"\n",
    "    print(f\"Generazione file indice CSV da: {root_dir}...\")\n",
    "    \n",
    "    brands = set()\n",
    "    \n",
    "    # Scansiona la struttura: root -> Categoria -> Brand\n",
    "    if os.path.exists(root_dir):\n",
    "        for category in os.listdir(root_dir):\n",
    "            cat_path = os.path.join(root_dir, category)\n",
    "            if os.path.isdir(cat_path):\n",
    "                for brand in os.listdir(cat_path):\n",
    "                    brand_path = os.path.join(cat_path, brand)\n",
    "                    if os.path.isdir(brand_path):\n",
    "                        brands.add(brand)\n",
    "    \n",
    "    if not brands:\n",
    "        print(\"❌ ERRORE: Nessun brand trovato per generare il CSV!\")\n",
    "        return\n",
    "\n",
    "    # Ordina alfabeticamente per avere indici consistenti\n",
    "    sorted_brands = sorted(list(brands))\n",
    "    \n",
    "    # Crea il DataFrame e salva\n",
    "    df = pd.DataFrame({\n",
    "        'brand': sorted_brands,\n",
    "        'index': range(len(sorted_brands))\n",
    "    })\n",
    "    \n",
    "    df.to_csv(csv_output_path, index=False)\n",
    "    print(f\"✅ File CSV generato con successo: {csv_output_path} ({len(brands)} brand mappati)\")\n",
    "\n",
    "generate_brand_index_csv(Config.dataset_root, Config.csv_index_path)\n",
    "setup_dataset()\n",
    "\n",
    "#setup_dataset(\"/content/drive/MyDrive/LogoDet-3K-divided.zip\", \"/content/LogoDet-3K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ed7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DatasetTest(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    # Load label string to index mapping\n",
    "        df = pd.read_csv(Config.csv_index_path)\n",
    "        self.label_to_id = {row['brand']: int(row['index']) for _, row in df.iterrows()}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        img = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        image_label = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "        label_idx = self.label_to_id[image_label]\n",
    "\n",
    "        return {\"image\": img, \"label\": label_idx}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.load_image(self.file_list[idx])\n",
    "\n",
    "\n",
    "def getTestPaths(root_dir, total_set_size=None, min_images_per_brand=2):\n",
    "    # MODIFICA: Puntiamo direttamente alla root, senza cercare la sottocartella 'test'\n",
    "    test_path = root_dir \n",
    "    test_brand_list = []\n",
    "\n",
    "    # Collect brand folders\n",
    "    if not os.path.exists(test_path):\n",
    "        print(f\"Warning: {test_path} not found.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Scansione cartelle in: {os.path.abspath(test_path)}\")\n",
    "\n",
    "    for category in os.listdir(test_path):\n",
    "        cat_path = os.path.join(test_path, category)\n",
    "        # Assicuriamoci di processare solo cartelle (ignora eventuali file readme, csv, ecc.)\n",
    "        if os.path.isdir(cat_path):\n",
    "            for brand in os.listdir(cat_path):\n",
    "                brand_full_path = os.path.join(cat_path, brand)\n",
    "                if os.path.isdir(brand_full_path):\n",
    "                    test_brand_list.append(brand_full_path)\n",
    "    \n",
    "    # Se non trova nessun brand, stampiamo un errore chiaro\n",
    "    if not test_brand_list:\n",
    "        print(\"❌ ERRORE: Nessun brand trovato! Controlla se il path punta alla cartella giusta.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"✅ Trovati {len(test_brand_list)} brand totali.\")\n",
    "\n",
    "    test_data_list = []\n",
    "\n",
    "    # Sampling Logic (Invariata)\n",
    "    if total_set_size is not None:\n",
    "        if len(test_brand_list) == 0: return []\n",
    "        images_per_brand = round(total_set_size / len(test_brand_list))\n",
    "\n",
    "        if images_per_brand < min_images_per_brand:\n",
    "            new_test_brand_count = round(total_set_size / min_images_per_brand)\n",
    "            test_brand_list = random.sample(test_brand_list, min(len(test_brand_list), new_test_brand_count))\n",
    "            images_per_brand = min_images_per_brand\n",
    "\n",
    "        for brand in test_brand_list:\n",
    "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
    "\n",
    "            if len(imgs) < min_images_per_brand:\n",
    "                # print(f\"images are less than {min_images_per_brand} for this brand: {brand}\")\n",
    "                pass\n",
    "\n",
    "            test_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
    "    else:\n",
    "        for brand in test_brand_list:\n",
    "            test_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
    "\n",
    "    return test_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c62222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_embeddings(model, file_list, transform, device):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Simple linear dataset of all unique test images\n",
    "    dataset = DatasetTest(file_list, transform)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images = data[\"image\"].to(device)\n",
    "            labels = data[\"label\"]\n",
    "\n",
    "            # Extract and move to CPU to save VRAM\n",
    "            embeddings = F.normalize(model(images), p=2, dim=1).cpu()\n",
    "\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_embeddings), torch.cat(all_labels)\n",
    "\n",
    "def cosine_similarity(averaged_support_embeddings, query_embeddings_tensor):\n",
    "\n",
    "    # Normalize embeddings if you want cosine similarity\n",
    "    support_emb_norm = F.normalize(averaged_support_embeddings, p=2, dim=0)       # [embedding_dim]\n",
    "    query_emb_norm = F.normalize(query_embeddings_tensor, p=2, dim=1)             # [num_queries, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sims = torch.matmul(query_emb_norm, support_emb_norm)  # [num_queries]\n",
    "    return sims\n",
    "\n",
    "def evaluate_few_shot(model, fewshot_iterator, transform, device, num_episodes=100):\n",
    "    evaluator = MetricEvaluator(device=device)\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    r_at_95p = []\n",
    "    ap_scores = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    unique_paths = list(fewshot_iterator.all_files_set)\n",
    "    embs, labels = compute_global_embeddings(model, unique_paths, transform, device)\n",
    "    j_score = evaluator.compute_discriminant_ratio(embs, labels)\n",
    "\n",
    "    # 2. Set to eval mode and disable gradient tracking\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_episodes):\n",
    "\n",
    "            task = fewshot_iterator()\n",
    "\n",
    "            if task is None:\n",
    "                print(f\"Stopped early at episode {i} because we ran out of brands.\")\n",
    "                break\n",
    "\n",
    "            support_paths = task[\"support_set\"]\n",
    "            query_paths = task[\"query_set\"]\n",
    "\n",
    "            # Build datasets and loaders\n",
    "            support_dataset = DatasetTest(support_paths, transform)\n",
    "            query_dataset = DatasetTest(query_paths, transform)\n",
    "\n",
    "            support_loader = DataLoader(support_dataset, batch_size=32)\n",
    "            query_loader = DataLoader(query_dataset, batch_size=64)\n",
    "\n",
    "            # Extract embeddings\n",
    "            support_embeddings = []\n",
    "            query_embeddings = []\n",
    "            query_labels = []\n",
    "            support_labels = []\n",
    "\n",
    "            # Compute embeddings for support set\n",
    "            for data in support_loader:\n",
    "                images = data[\"image\"].to(device)\n",
    "                support_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
    "\n",
    "                batch_labels = data[\"label\"]\n",
    "                support_labels.append(batch_labels)\n",
    "                support_brand = batch_labels[0]\n",
    "\n",
    "            support_embeddings_tensor = torch.cat(support_embeddings)\n",
    "            support_labels_tensor = torch.cat(support_labels)\n",
    "\n",
    "            # Average embeddings\n",
    "            averaged_support_embeddings = support_embeddings_tensor.mean(dim=0)\n",
    "            averaged_support_embeddings_for_ap = support_embeddings_tensor.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # Compute embeddings for query set\n",
    "            for data in query_loader:\n",
    "                images = data[\"image\"].to(device)\n",
    "                query_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
    "\n",
    "                batch_labels = data[\"label\"]\n",
    "                query_labels.append(batch_labels)\n",
    "\n",
    "            # query_embeddings and query_labels are list of tensors, this unrolls them\n",
    "            query_embeddings_tensor = torch.cat(query_embeddings)\n",
    "            query_labels_tensor = torch.cat(query_labels)\n",
    "\n",
    "            # mAP\n",
    "            ap_score_single = evaluator.compute_map(\n",
    "                query_emb=averaged_support_embeddings_for_ap,\n",
    "                gallery_emb=query_embeddings_tensor,\n",
    "                query_labels=support_labels_tensor,\n",
    "                gallery_labels=query_labels_tensor\n",
    "            )\n",
    "\n",
    "            ap_scores.append(ap_score_single)\n",
    "\n",
    "            # Compute similarity\n",
    "            sims = cosine_similarity(averaged_support_embeddings, query_embeddings_tensor)\n",
    "\n",
    "            # Ground truth: query belongs to support brand?\n",
    "            gt = (query_labels_tensor == support_brand).float()\n",
    "\n",
    "            # Predictions, does the model predict it is the same brand?\n",
    "            pred = (sims >= Config.prediciton_threashold).float().cpu()\n",
    "\n",
    "            # Accuracy\n",
    "            acc = (pred == gt).float().mean().item()\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            # Precision, Recall, F1\n",
    "            prec, rec = evaluator.compute_precision_recall(sims, gt, threshold=Config.prediciton_threashold)\n",
    "            f1 = evaluator.compute_f1_score(prec, rec)\n",
    "            r95 = evaluator.compute_recall_at_fixed_precision(sims, gt, min_precision=0.95)\n",
    "\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "            f1_scores.append(f1)\n",
    "            r_at_95p.append(r95)\n",
    "\n",
    "    # Aggregate results\n",
    "    results = {\n",
    "        \"accuracy\": sum(accuracies) / len(accuracies),\n",
    "        \"precision\": sum(precisions) / len(precisions),\n",
    "        \"recall\": sum(recalls) / len(recalls),\n",
    "        \"f1\": sum(f1_scores) / len(f1_scores),\n",
    "        \"r@95p\": sum(r_at_95p) / len(r_at_95p),\n",
    "        \"map\": sum(ap_scores) / len(ap_scores),\n",
    "        \"J\": j_score,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "class MetricEvaluator:\n",
    "    \"\"\"\n",
    "    A class to calculate evaluation metrics for Few-Shot Learning and Metric Learning.\n",
    "\n",
    "    Implements:\n",
    "    1. Discriminant Ratio (J): Optimized scalar implementation (O(d) memory).\n",
    "    2. Mean Average Precision (mAP): Ranking quality metric.\n",
    "    3. Recall at Fixed Precision (R@P): Operational metric.\n",
    "    4. Precision & Recall: Raw metrics at a specific similarity threshold.\n",
    "    5. F1 Score: Harmonic mean of Precision and Recall.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator.\n",
    "\n",
    "        Args:\n",
    "            device (str): 'cuda' or 'cpu'. If None, detects automatically.\n",
    "        \"\"\"\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.epsilon = 1e-6  # For numerical stability\n",
    "\n",
    "    def compute_discriminant_ratio(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Calculates the Discriminant Ratio (J) using the optimized Scalar approach.\n",
    "\n",
    "        Theory:\n",
    "            J = Tr(Sb) / Tr(Sw)\n",
    "            Using the Trace Trick: Tr(Sw) = Tr(St) - Tr(Sb)\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): Tensor of shape (Batch_Size, Dimension).\n",
    "            labels (torch.Tensor): Tensor of class labels.\n",
    "\n",
    "        Returns:\n",
    "            float: The Discriminant Ratio score.\n",
    "        \"\"\"\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        # 1. Global Mean Computation\n",
    "        global_mean = embeddings.mean(dim=0)\n",
    "\n",
    "        # 2. Calculate Trace of Total Scatter (St)\n",
    "        # Sum of squared Euclidean distances of all points from the global mean.\n",
    "        tr_st = torch.sum((embeddings - global_mean) ** 2)\n",
    "\n",
    "        # 3. Calculate Trace of Between-Class Scatter (Sb)\n",
    "        tr_sb = 0\n",
    "        unique_classes = torch.unique(labels)\n",
    "\n",
    "        for c in unique_classes:\n",
    "            class_mask = (labels == c)\n",
    "            class_embeddings = embeddings[class_mask]\n",
    "            n_c = class_embeddings.size(0)\n",
    "\n",
    "            if n_c > 0:\n",
    "                mu_c = class_embeddings.mean(dim=0)\n",
    "                tr_sb += n_c * torch.sum((mu_c - global_mean) ** 2)\n",
    "\n",
    "        # 4. Calculate Trace of Within-Class Scatter (Sw)\n",
    "        tr_sw = tr_st - tr_sb\n",
    "\n",
    "        # Calculate J\n",
    "        j_score = tr_sb / (tr_sw + self.epsilon)\n",
    "\n",
    "        return j_score.item()\n",
    "\n",
    "    def compute_map(self, query_emb, gallery_emb, query_labels, gallery_labels):\n",
    "        \"\"\"\n",
    "        Calculates Mean Average Precision (mAP).\n",
    "        \"\"\"\n",
    "        query_emb = query_emb.to(self.device)\n",
    "        gallery_emb = gallery_emb.to(self.device)\n",
    "        query_labels = query_labels.to(self.device)\n",
    "        gallery_labels = gallery_labels.to(self.device)\n",
    "\n",
    "        # L2 Normalize for Cosine Similarity\n",
    "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "        gallery_emb = F.normalize(gallery_emb, p=2, dim=1)\n",
    "\n",
    "        # Similarity Matrix: S = Q * G^T\n",
    "        similarity_matrix = torch.matmul(query_emb, gallery_emb.T)\n",
    "\n",
    "        num_queries = query_labels.size(0)\n",
    "        average_precisions = []\n",
    "\n",
    "        for i in range(num_queries):\n",
    "            scores = similarity_matrix[i]\n",
    "            target_label = query_labels[i]\n",
    "\n",
    "            # Ranking\n",
    "            sorted_indices = torch.argsort(scores, descending=True)\n",
    "            sorted_gallery_labels = gallery_labels[sorted_indices]\n",
    "\n",
    "            # Relevance Mask\n",
    "            relevance_mask = (sorted_gallery_labels == target_label).float()\n",
    "\n",
    "            total_relevant = relevance_mask.sum()\n",
    "            if total_relevant == 0:\n",
    "                average_precisions.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Cumulative Precision\n",
    "            cumsum = torch.cumsum(relevance_mask, dim=0)\n",
    "            ranks = torch.arange(1, len(relevance_mask) + 1).to(self.device)\n",
    "            precisions = cumsum / ranks\n",
    "\n",
    "            # Average Precision (AP)\n",
    "            ap = (precisions * relevance_mask).sum() / total_relevant\n",
    "            average_precisions.append(ap.item())\n",
    "\n",
    "        if not average_precisions:\n",
    "            return 0.0\n",
    "        return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    def compute_precision_recall(self, similarity_scores, is_match, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Calculates raw Precision and Recall at a specific similarity threshold.\n",
    "\n",
    "        Definitions:\n",
    "            Precision = TP / (TP + FP)\n",
    "            Recall    = TP / (TP + FN)\n",
    "\n",
    "        Args:\n",
    "            similarity_scores (torch.Tensor): 1D tensor of scores (0.0 to 1.0).\n",
    "            is_match (torch.Tensor): 1D binary tensor (Ground Truth).\n",
    "            threshold (float): Cutoff for deciding if a retrieval is Positive.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (precision, recall)\n",
    "        \"\"\"\n",
    "        similarity_scores = similarity_scores.to(self.device)\n",
    "        is_match = is_match.to(self.device)\n",
    "\n",
    "        # Binarize predictions: 1 if score >= threshold (Positive), else 0 (Negative)\n",
    "        predicted_positive = (similarity_scores >= threshold).float()\n",
    "\n",
    "        # True Positives (TP): Predicted Positive AND Actually Match\n",
    "        tp = (predicted_positive * is_match).sum()\n",
    "\n",
    "        # False Positives (FP): Predicted Positive BUT Actually Non-Match\n",
    "        fp = (predicted_positive * (1 - is_match)).sum()\n",
    "\n",
    "        # False Negatives (FN): Predicted Negative BUT Actually Match\n",
    "        # (We invert the prediction mask to find negatives)\n",
    "        fn = ((1 - predicted_positive) * is_match).sum()\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        return precision.item(), recall.item()\n",
    "\n",
    "    def compute_recall_at_fixed_precision(self, similarity_scores, is_match, min_precision=0.95):\n",
    "        \"\"\"\n",
    "        Calculates Recall at a Fixed Precision (R@P).\n",
    "        Finds the lowest threshold where Precision >= min_precision.\n",
    "        \"\"\"\n",
    "        similarity_scores = similarity_scores.to(self.device)\n",
    "        is_match = is_match.to(self.device)\n",
    "\n",
    "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "        sorted_matches = is_match[sorted_indices]\n",
    "\n",
    "        tps = torch.cumsum(sorted_matches, dim=0)\n",
    "        total_retrieved = torch.arange(1, len(sorted_matches) + 1).to(self.device)\n",
    "\n",
    "        precisions = tps / total_retrieved\n",
    "\n",
    "        # Find indices where Precision satisfies the constraint\n",
    "        valid_indices = torch.where(precisions >= min_precision)[0]\n",
    "\n",
    "        if len(valid_indices) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        cutoff_index = valid_indices[-1]\n",
    "\n",
    "        # Recall = TP_at_cutoff / Total_Relevant_In_Dataset\n",
    "        total_relevant_in_dataset = is_match.sum()\n",
    "\n",
    "        if total_relevant_in_dataset == 0:\n",
    "            return 0.0\n",
    "\n",
    "        recall = tps[cutoff_index] / total_relevant_in_dataset\n",
    "\n",
    "        return recall.item()\n",
    "\n",
    "    def compute_f1_score(self, precision, recall):\n",
    "        \"\"\"\n",
    "        Calculates F1 Score (Harmonic Mean).\n",
    "        \"\"\"\n",
    "        if (precision + recall) == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "\n",
    "class FewShotIterator:\n",
    "    def __init__(self, file_list, n_shot):\n",
    "        \"\"\"\n",
    "        Initializes the iterator class.\n",
    "        It prepares the global testset and creates a cyclic iterator over the valid brands.\n",
    "        \"\"\"\n",
    "        self.n_shot = n_shot\n",
    "\n",
    "        # 1. Validation: Check if input list is empty\n",
    "        if not file_list:\n",
    "            raise ValueError(\"The test file list is empty.\")\n",
    "\n",
    "        #    (Dataset - SupportSet)  is significantly faster with sets (O(1)) compared to lists.\n",
    "        self.all_files_set = set(file_list)\n",
    "\n",
    "        # 3. Organize data by Brand\n",
    "        #    We create a dictionary mapping: { 'BrandName': [list_of_image_paths] }\n",
    "        self.brands_map = {}\n",
    "\n",
    "        for file_path in file_list:\n",
    "            # Extract brand name assuming structure: .../Category/Brand/Image.jpg\n",
    "            brand_name = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "            if brand_name not in self.brands_map:\n",
    "                self.brands_map[brand_name] = []\n",
    "            self.brands_map[brand_name].append(file_path)\n",
    "\n",
    "        self.valid_brands_list = list(self.brands_map.keys())\n",
    "\n",
    "        if not self.valid_brands_list:\n",
    "            raise ValueError(f\"No brand found with more than {n_shot} images.\")\n",
    "\n",
    "        #    'itertools.cycle' creates an infinite loop over the valid brands list.\n",
    "        self.brand_iterator = cycle(self.valid_brands_list)\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Executed when the class instance is called.\n",
    "        Logic:\n",
    "        1. Pick next brand (Sequential).\n",
    "        2. Pick Support Set (Random 5 images from that brand).\n",
    "        3. Pick Query Set (EVERYTHING else in the testset).\n",
    "        \"\"\"\n",
    "        # A. Get the next brand sequentially from the cycle\n",
    "        try:\n",
    "            selected_brand_name = next(self.brand_iterator)\n",
    "        except StopIteration:\n",
    "            # Gracefully signal that we are done\n",
    "            print(\"Iterator finished: All brands have been processed.\")\n",
    "            return None\n",
    "\n",
    "        # B. Retrieve all images specific to this chosen brand\n",
    "        images_of_current_brand = self.brands_map[selected_brand_name]\n",
    "\n",
    "        # Select a random number between 1 to 5 which is the number of images of the support brand guaranteed in the query set\n",
    "        num_query_guarantee_if_available = random.randint(1, 5)\n",
    "\n",
    "        # C. Create SUPPORT SET\n",
    "        #    Select 'n_shot' unique images randomly from the current brand.\n",
    "        support_set_list = random.sample(images_of_current_brand, self.n_shot)\n",
    "        support_set_set = set(support_set_list)\n",
    "\n",
    "        # D. Create QUERY SET (Global Subtraction)\n",
    "        #    Requirement: The Query Set contains 50 images of which at least 1 is from the support brand\n",
    "        #    Step 1: initialize the Query list\n",
    "        query_set_list = []\n",
    "\n",
    "        #    Step 2: Sample randomly the images to guarantee in the query set\n",
    "        remaining_brand_images = list(set(images_of_current_brand) - support_set_set)\n",
    "        guaranteed_images_in_query = random.sample(remaining_brand_images, min(num_query_guarantee_if_available, len(remaining_brand_images)))\n",
    "\n",
    "        if (len(remaining_brand_images) == 0):\n",
    "            print(f\"for the brand {selected_brand_name} {len(remaining_brand_images)} images where put in the query set\")\n",
    "\n",
    "        #    Step 3: Sample the Negative Queries (Distractors from OTHER brands)\n",
    "        # We subtract ALL images of the current brand to ensure zero accidental matches\n",
    "        remaining_images_in_query = list(self.all_files_set - set(images_of_current_brand) - set(guaranteed_images_in_query))\n",
    "\n",
    "        total_query_size = 50\n",
    "        num_remaining_query = total_query_size - min(num_query_guarantee_if_available, len(remaining_brand_images))\n",
    "        query_remaining = random.sample(remaining_images_in_query,  min(len(remaining_images_in_query),num_remaining_query))\n",
    "\n",
    "        #    Step 4: Combine and Shuffle\n",
    "        query_set_list = guaranteed_images_in_query + query_remaining\n",
    "        random.shuffle(query_set_list)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"brand_name\": selected_brand_name,\n",
    "            \"support_set\": support_set_list,\n",
    "            \"query_set\": query_set_list\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed5c9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaricamento ResNet50 originale (ImageNet Weights)...\n",
      "Preparazione dati per il testing...\n",
      "Scansione cartelle in: c:\\Users\\Lenovo\\Desktop\\ProgettoFinale1\\PY_script\\LogoDet-3K\n",
      "✅ Trovati 3000 brand totali.\n",
      "Avvio calcolo Baseline su 1000 episodi...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrica.capitalize()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 4. Esecuzione del Loop di Testing\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Il loop 'evaluate_few_shot' riceverà vettori da 2048 dimensioni.\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Funzionerà correttamente perché usa cosine_similarity che è agnostica alla dimensione.\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvvio calcolo Baseline su \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mConfig.num_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m episodi...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m results = \u001b[43mevaluate_few_shot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfewshot_iterator\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_episodes\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 5. Stampa dei Risultati\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Risultati Baseline (ResNet50 \u001b[39m\u001b[33m'\u001b[39m\u001b[33mOff-the-shelf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m) ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mevaluate_few_shot\u001b[39m\u001b[34m(model, fewshot_iterator, transform, device, num_episodes)\u001b[39m\n\u001b[32m     42\u001b[39m torch.cuda.empty_cache()\n\u001b[32m     44\u001b[39m unique_paths = \u001b[38;5;28mlist\u001b[39m(fewshot_iterator.all_files_set)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m embs, labels = \u001b[43mcompute_global_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m j_score = evaluator.compute_discriminant_ratio(embs, labels)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 2. Set to eval mode and disable gradient tracking\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcompute_global_embeddings\u001b[39m\u001b[34m(model, file_list, transform, device)\u001b[39m\n\u001b[32m      8\u001b[39m loader = DataLoader(dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mDatasetTest.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mDatasetTest.load_image\u001b[39m\u001b[34m(self, image_path)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_path):\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m     17\u001b[39m         img = \u001b[38;5;28mself\u001b[39m.transform(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\PIL\\Image.py:3493\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3491\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3492\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3493\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3494\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_baseline_resnet50(device):\n",
    "    \"\"\"\n",
    "    Scarica la ResNet50 originale pre-addestrata su ImageNet.\n",
    "    Sostituisce il layer fully connected (fc) con Identity per restituire\n",
    "    i feature embeddings (dimensione 2048) invece delle classi.\n",
    "    \"\"\"\n",
    "    print(\"Scaricamento ResNet50 originale (ImageNet Weights)...\")\n",
    "    \n",
    "    # 1. Carica i pesi di default (ImageNet)\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    \n",
    "    # 2. Rimuovi la testa di classificazione (1000 classi)\n",
    "    # Sostituendola con Identity, l'output del modello sarà l'output \n",
    "    # del layer precedente (avgpool), ovvero un vettore di dimensione 2048.\n",
    "    model.fc = nn.Identity()\n",
    "    \n",
    "    # 3. Configurazione per inferenza\n",
    "    model.to(device)\n",
    "    model.eval() # Imposta batchnorm e dropout in modalità valutazione\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # 1. Configurazione Dispositivo e Trasformazioni\n",
    "    # Usa le stesse trasformazioni standard di ImageNet usate nel training/testing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    device = torch.device(Config.device if hasattr(Config, 'device') else 'cuda')\n",
    "\n",
    "    # 2. Istanzia il modello Baseline\n",
    "    model = get_baseline_resnet50(device)\n",
    "\n",
    "    # 3. Preparazione Dati\n",
    "    # Recupera i path e crea l'iteratore usando le tue funzioni esistenti\n",
    "    print(\"Preparazione dati per il testing...\")\n",
    "    test_paths = getTestPaths(Config.dataset_root) \n",
    "    iterator = FewShotIterator(test_paths, n_shot=Config.n_shot)\n",
    "\n",
    "    # 4. Esecuzione del Loop di Testing\n",
    "    # Il loop 'evaluate_few_shot' riceverà vettori da 2048 dimensioni.\n",
    "    # Funzionerà correttamente perché usa cosine_similarity che è agnostica alla dimensione.\n",
    "    print(f\"Avvio calcolo Baseline su {Config.num_episodes} episodi...\")\n",
    "    \n",
    "    results = evaluate_few_shot(\n",
    "        model=model, \n",
    "        fewshot_iterator=iterator, \n",
    "        transform=transform, \n",
    "        device=device, \n",
    "        num_episodes=Config.num_episodes\n",
    "    )\n",
    "\n",
    "    # 5. Stampa dei Risultati\n",
    "    print(\"\\n=== Risultati Baseline (ResNet50 'Off-the-shelf') ===\")\n",
    "    for metrica, valore in results.items():\n",
    "        print(f\"{metrica.capitalize():<15}: {valore:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
