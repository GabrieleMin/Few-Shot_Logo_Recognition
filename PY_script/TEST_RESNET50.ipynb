{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e522aa3f",
   "metadata": {},
   "source": [
    "# Few-Shot Logo Recognition Project: ResNet50 Baseline Evaluation\n",
    "\n",
    "### 1. Abstract and Objective\n",
    "This notebook aims to establish a **baseline** for the *Few-Shot Logo Recognition* task using the **LogoDet-3K** dataset.\n",
    "Before proceeding with fine-tuning or modifying the network architecture, it is crucial to measure the performance of a standard pre-trained model. The results obtained in this notebook will serve as a benchmark to evaluate the effectiveness of future optimizations.\n",
    "\n",
    "### 2. Methodology\n",
    "The approach used in this phase is **\"Off-the-shelf Feature Extraction\"**:\n",
    "* **Model:** A **ResNet50** pre-trained on *ImageNet* is used.\n",
    "* **Feature Extraction:** The final classification layer (Fully Connected) is removed and replaced with an identity function. Instead of predicting the 1000 ImageNet classes, the model returns the feature vector (embedding) of dimension **2048**.\n",
    "* **Inference:** Classification is performed via **Cosine Similarity**. The distance between the *query* image embedding (to be classified) and the *support* image embedding (known brand example) is calculated.\n",
    "* **Protocol:** Testing follows an *episodic* approach (N-Way, K-Shot) simulated over 1000 episodes to ensure statistical robustness.\n",
    "\n",
    "### 3. Notebook Structure\n",
    "The code is organized into the following logical sections:\n",
    "\n",
    "1.  **Configuration and Setup:** Definition of global parameters (seed, path, device) and environment setup.\n",
    "2.  **Dataset Management:**\n",
    "    * `DatasetTest`: PyTorch class for loading images.\n",
    "    * `FewShotIterator`: Logic for creating episodes (Support Set vs Query Set).\n",
    "3.  **Evaluation Metrics:** Implementation of the `MetricEvaluator` class for calculating Accuracy, mAP, F1-Score, Precision, Recall, and Discriminant Ratio (J).\n",
    "4.  **Baseline Model Definition:** `get_baseline_resnet50` function for loading the model and modifying the head.\n",
    "5.  **Evaluation Loop:** Execution of the test and aggregation of final results.\n",
    "\n",
    "---\n",
    "**Dataset:** LogoDet-3K | **Model:** ResNet50 (Frozen) | **Main Metric:** Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfec627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione file indice CSV da: ./LogoDet-3K...\n",
      "✅ File CSV generato con successo: LogoDet-3K/brand_to_index.csv (3000 brand mappati)\n",
      "✅ Dataset trovato correttemente in: c:\\Users\\Lenovo\\Desktop\\ProgettoFinale1\\PY_script\\LogoDet-3K\n",
      "✅ CSV index trovato.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "from itertools import cycle\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "#from google.colab import drive\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "class Config:\n",
    "    # 1. SETUP\n",
    "    project_name = \"FewShot_Evaluation\"\n",
    "    seed = 42\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 2. DATASET PATH\n",
    "    #dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
    "    dataset_root = \"./LogoDet-3K\"\n",
    "    \n",
    "    csv_index_path = \"LogoDet-3K/brand_to_index.csv\"\n",
    "    # 3. MODEL PARAMETERS\n",
    "    embedding_dim = 128\n",
    "    pretrained = True\n",
    "    freeze_layers = 5\n",
    "    trained_model_path = \"\"\n",
    "\n",
    "    # 4. EVALUATION SETTINGS\n",
    "    prediciton_threashold = 0.5\n",
    "    n_shot = 1\n",
    "    num_episodes = 1000\n",
    "\n",
    "torch.manual_seed(Config.seed)\n",
    "random.seed(Config.seed)\n",
    "\n",
    "#def setup_dataset(zip_path, extract_to):\n",
    "#    \"\"\"\n",
    "#    Mounts Google Drive and extracts the dataset if not already present.\n",
    "#    \"\"\"\n",
    "#    # 1. Mount Google Drive\n",
    "#    if not os.path.exists('/content/drive'):\n",
    "#        drive.mount('/content/drive')\n",
    "#\n",
    "#    # 2. Check if the folder already exists\n",
    "#    if os.path.exists(extract_to):\n",
    "#        print(f\"Dataset folder '{extract_to}' already exists. Skipping extraction.\")\n",
    "#    else:\n",
    "#        print(f\"Extracting dataset from {zip_path}...\")\n",
    "#        if os.path.exists(zip_path):\n",
    "#            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#                zip_ref.extractall(extract_to)\n",
    "#            print(\"Extraction complete.\")\n",
    "#        else:\n",
    "#            print(f\"ERROR: Zip file not found at {zip_path}. Check your path.\")\n",
    "def setup_dataset():\n",
    "    \"\"\"\n",
    "    Verifica semplicemente se la cartella del dataset esiste in locale.\n",
    "    Non scarica né decomprime nulla.\n",
    "    \"\"\"\n",
    "    # Verifica esistenza dataset\n",
    "    if os.path.exists(Config.dataset_root):\n",
    "        print(f\" Dataset trovato correttemente in: {os.path.abspath(Config.dataset_root)}\")\n",
    "    else:\n",
    "        print(f\" ERRORE: La cartella '{Config.dataset_root}' non è stata trovata.\")\n",
    "        print(\"Assicurati che il nome della cartella in Config.dataset_root corrisponda esattamente a quella sul tuo PC.\")\n",
    "        \n",
    "    # Verifica esistenza CSV (opzionale ma utile)\n",
    "    # Se il csv è dentro la cartella del dataset, aggiusta il path in Config\n",
    "    if os.path.exists(Config.csv_index_path):\n",
    "        print(f\" CSV index trovato.\")\n",
    "    else:\n",
    "        print(f\" ATTENZIONE: File CSV '{Config.csv_index_path}' non trovato. Controlla il percorso.\")\n",
    "\n",
    "# Esegui il setup (che ora è solo un controllo)\n",
    "def generate_brand_index_csv(root_dir, csv_output_path):\n",
    "    \"\"\"\n",
    "    Scansiona le cartelle dei brand e genera automaticamente il file CSV\n",
    "    brand_to_index.csv necessario per il mapping (Brand -> ID Numerico).\n",
    "    \"\"\"\n",
    "    print(f\"Generazione file indice CSV da: {root_dir}...\")\n",
    "    \n",
    "    brands = set()\n",
    "    \n",
    "    # Scansiona la struttura: root -> Categoria -> Brand\n",
    "    if os.path.exists(root_dir):\n",
    "        for category in os.listdir(root_dir):\n",
    "            cat_path = os.path.join(root_dir, category)\n",
    "            if os.path.isdir(cat_path):\n",
    "                for brand in os.listdir(cat_path):\n",
    "                    brand_path = os.path.join(cat_path, brand)\n",
    "                    if os.path.isdir(brand_path):\n",
    "                        brands.add(brand)\n",
    "    \n",
    "    if not brands:\n",
    "        print(\"❌ ERRORE: Nessun brand trovato per generare il CSV!\")\n",
    "        return\n",
    "\n",
    "    # Ordina alfabeticamente per avere indici consistenti\n",
    "    sorted_brands = sorted(list(brands))\n",
    "    \n",
    "    # Crea il DataFrame e salva\n",
    "    df = pd.DataFrame({\n",
    "        'brand': sorted_brands,\n",
    "        'index': range(len(sorted_brands))\n",
    "    })\n",
    "    \n",
    "    df.to_csv(csv_output_path, index=False)\n",
    "    print(f\"✅ File CSV generato con successo: {csv_output_path} ({len(brands)} brand mappati)\")\n",
    "\n",
    "generate_brand_index_csv(Config.dataset_root, Config.csv_index_path)\n",
    "setup_dataset()\n",
    "\n",
    "#setup_dataset(\"/content/drive/MyDrive/LogoDet-3K-divided.zip\", \"/content/LogoDet-3K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class DatasetTest(Dataset):\n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "\n",
    "    # Load label string to index mapping\n",
    "        df = pd.read_csv(Config.csv_index_path)\n",
    "        self.label_to_id = {row['brand']: int(row['index']) for _, row in df.iterrows()}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def load_image(self, image_path):\n",
    "        img = Image.open(image_path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        image_label = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "        label_idx = self.label_to_id[image_label]\n",
    "\n",
    "        return {\"image\": img, \"label\": label_idx}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.load_image(self.file_list[idx])\n",
    "\n",
    "\n",
    "def getTestPaths(root_dir, total_set_size=None, min_images_per_brand=2):\n",
    "    # MODIFICA: Puntiamo direttamente alla root, senza cercare la sottocartella 'test'\n",
    "    test_path = root_dir \n",
    "    test_brand_list = []\n",
    "\n",
    "    # Collect brand folders\n",
    "    if not os.path.exists(test_path):\n",
    "        print(f\"Warning: {test_path} not found.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Scansione cartelle in: {os.path.abspath(test_path)}\")\n",
    "\n",
    "    for category in os.listdir(test_path):\n",
    "        cat_path = os.path.join(test_path, category)\n",
    "        # Assicuriamoci di processare solo cartelle (ignora eventuali file readme, csv, ecc.)\n",
    "        if os.path.isdir(cat_path):\n",
    "            for brand in os.listdir(cat_path):\n",
    "                brand_full_path = os.path.join(cat_path, brand)\n",
    "                if os.path.isdir(brand_full_path):\n",
    "                    test_brand_list.append(brand_full_path)\n",
    "    \n",
    "    # Se non trova nessun brand, stampiamo un errore chiaro\n",
    "    if not test_brand_list:\n",
    "        print(\"ERRORE: Nessun brand trovato! Controlla se il path punta alla cartella giusta.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Trovati {len(test_brand_list)} brand totali.\")\n",
    "\n",
    "    test_data_list = []\n",
    "\n",
    "    # Sampling Logic (Invariata)\n",
    "    if total_set_size is not None:\n",
    "        if len(test_brand_list) == 0: return []\n",
    "        images_per_brand = round(total_set_size / len(test_brand_list))\n",
    "\n",
    "        if images_per_brand < min_images_per_brand:\n",
    "            new_test_brand_count = round(total_set_size / min_images_per_brand)\n",
    "            test_brand_list = random.sample(test_brand_list, min(len(test_brand_list), new_test_brand_count))\n",
    "            images_per_brand = min_images_per_brand\n",
    "\n",
    "        for brand in test_brand_list:\n",
    "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
    "\n",
    "            if len(imgs) < min_images_per_brand:\n",
    "                # print(f\"images are less than {min_images_per_brand} for this brand: {brand}\")\n",
    "                pass\n",
    "\n",
    "            test_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
    "    else:\n",
    "        for brand in test_brand_list:\n",
    "            test_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
    "\n",
    "    return test_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c62222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_embeddings(model, file_list, transform, device):\n",
    "    \"\"\"\n",
    "    Estrae le feature da tutte le immagini della lista passata.\n",
    "    Versione con PRINT DI DEBUG per monitorare l'avanzamento.\n",
    "    \"\"\"\n",
    "    total_files = len(file_list)\n",
    "    \n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Simple linear dataset of all unique test images\n",
    "    dataset = DatasetTest(file_list, transform)\n",
    "    \n",
    "    # Batch size 64 è un buon compromesso per la tua RAM\n",
    "    batch_size = 64\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    num_batches = len(loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader):\n",
    "            # Stampa lo stato di avanzamento sulla stessa riga\n",
    "            # Usa end=\"\\r\" per sovrascrivere la riga e creare un effetto \"caricamento\"\n",
    "            print(f\"  > [Batch {i+1}/{num_batches}] Elaborazione batch in corso...\", end=\"\\r\")\n",
    "            \n",
    "            images = data[\"image\"].to(device)\n",
    "            labels = data[\"label\"]\n",
    "\n",
    "            # Extract and move to CPU to save VRAM\n",
    "            # Qui avviene il calcolo pesante della ResNet\n",
    "            embeddings = F.normalize(model(images), p=2, dim=1).cpu()\n",
    "\n",
    "            all_embeddings.append(embeddings)\n",
    "            all_labels.append(labels)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    final_embeddings = torch.cat(all_embeddings)\n",
    "    final_labels = torch.cat(all_labels)\n",
    "    \n",
    "    print(f\"[END] 'compute_global_embeddings' completato. Shape finale: {final_embeddings.shape}\")\n",
    "    return final_embeddings, final_labels\n",
    "\n",
    "def cosine_similarity(averaged_support_embeddings, query_embeddings_tensor):\n",
    "\n",
    "    # Normalize embeddings if you want cosine similarity\n",
    "    support_emb_norm = F.normalize(averaged_support_embeddings, p=2, dim=0)       # [embedding_dim]\n",
    "    query_emb_norm = F.normalize(query_embeddings_tensor, p=2, dim=1)             # [num_queries, embedding_dim]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    sims = torch.matmul(query_emb_norm, support_emb_norm)  # [num_queries]\n",
    "    return sims\n",
    "\n",
    "def evaluate_few_shot(model, fewshot_iterator, transform, device, num_episodes=100):\n",
    "    \n",
    "    evaluator = MetricEvaluator(device=device)\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    r_at_95p = []\n",
    "    ap_scores = []\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    unique_paths = list(fewshot_iterator.all_files_set)\n",
    "    \n",
    "    print(\"[HEAVY TASK] Inizio compute_global_embeddings\")\n",
    "    \n",
    "    # Qui è dove probabilmente perdeva tempo\n",
    "    embs, labels = compute_global_embeddings(model, unique_paths, transform, device)\n",
    "    j_score = evaluator.compute_discriminant_ratio(embs, labels)\n",
    "    print(\"[HEAVY TASK] fine compute_global_embeddings\")\n",
    "\n",
    "    # -----------------------------------\n",
    "\n",
    "    # 2. Set to eval mode and disable gradient tracking\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_episodes):\n",
    "            # Print di stato per ogni episodio\n",
    "            print(f\"  > [Episodio {i+1}/{num_episodes}] Generazione task...\", end=\"\\r\")\n",
    "\n",
    "            task = fewshot_iterator()\n",
    "\n",
    "            if task is None:\n",
    "                print(f\"\\n[STOP] Fermato anticipatamente all'episodio {i} (finiti i brand).\")\n",
    "                break\n",
    "\n",
    "            support_paths = task[\"support_set\"]\n",
    "            query_paths = task[\"query_set\"]\n",
    "            \n",
    "\n",
    "            # Build datasets and loaders\n",
    "            support_dataset = DatasetTest(support_paths, transform)\n",
    "            query_dataset = DatasetTest(query_paths, transform)\n",
    "\n",
    "            support_loader = DataLoader(support_dataset, batch_size=32)\n",
    "            query_loader = DataLoader(query_dataset, batch_size=64)\n",
    "\n",
    "            # Extract embeddings\n",
    "            support_embeddings = []\n",
    "            query_embeddings = []\n",
    "            query_labels = []\n",
    "            support_labels = []\n",
    "\n",
    "            # Compute embeddings for support set\n",
    "            for data in support_loader:\n",
    "                images = data[\"image\"].to(device)\n",
    "                support_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
    "\n",
    "                batch_labels = data[\"label\"]\n",
    "                support_labels.append(batch_labels)\n",
    "                support_brand = batch_labels[0]\n",
    "\n",
    "            support_embeddings_tensor = torch.cat(support_embeddings)\n",
    "            support_labels_tensor = torch.cat(support_labels)\n",
    "\n",
    "            # Average embeddings\n",
    "            averaged_support_embeddings = support_embeddings_tensor.mean(dim=0)\n",
    "            averaged_support_embeddings_for_ap = support_embeddings_tensor.mean(dim=0, keepdim=True)\n",
    "\n",
    "            # Compute embeddings for query set\n",
    "            for data in query_loader:\n",
    "                images = data[\"image\"].to(device)\n",
    "                query_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
    "\n",
    "                batch_labels = data[\"label\"]\n",
    "                query_labels.append(batch_labels)\n",
    "\n",
    "            # query_embeddings and query_labels are list of tensors, this unrolls them\n",
    "            query_embeddings_tensor = torch.cat(query_embeddings)\n",
    "            query_labels_tensor = torch.cat(query_labels)\n",
    "\n",
    "            # mAP\n",
    "            ap_score_single = evaluator.compute_map(\n",
    "                query_emb=averaged_support_embeddings_for_ap,\n",
    "                gallery_emb=query_embeddings_tensor,\n",
    "                query_labels=support_labels_tensor,\n",
    "                gallery_labels=query_labels_tensor\n",
    "            )\n",
    "\n",
    "            ap_scores.append(ap_score_single)\n",
    "\n",
    "            # Compute similarity\n",
    "            sims = cosine_similarity(averaged_support_embeddings, query_embeddings_tensor)\n",
    "\n",
    "            # Ground truth: query belongs to support brand?\n",
    "            gt = (query_labels_tensor == support_brand).float()\n",
    "\n",
    "            # Predictions, does the model predict it is the same brand?\n",
    "            pred = (sims >= Config.prediciton_threashold).float().cpu()\n",
    "\n",
    "            # Accuracy\n",
    "            acc = (pred == gt).float().mean().item()\n",
    "            accuracies.append(acc)\n",
    "\n",
    "            # Precision, Recall, F1\n",
    "            prec, rec = evaluator.compute_precision_recall(sims, gt, threshold=Config.prediciton_threashold)\n",
    "            f1 = evaluator.compute_f1_score(prec, rec)\n",
    "            r95 = evaluator.compute_recall_at_fixed_precision(sims, gt, min_precision=0.95)\n",
    "\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "            f1_scores.append(f1)\n",
    "            r_at_95p.append(r95)\n",
    "            \n",
    "            # Feedback visivo ogni 10 episodi per non intasare troppo, ma vedere che avanza\n",
    "            if (i + 1) % 10 == 0:\n",
    "                 print(f\"  > [Episodio {i+1}/{num_episodes}] Completato. (Acc parziale: {acc:.2f})\")\n",
    "\n",
    "    # Aggregate results\n",
    "    results = {\n",
    "        \"accuracy\": sum(accuracies) / len(accuracies),\n",
    "        \"precision\": sum(precisions) / len(precisions),\n",
    "        \"recall\": sum(recalls) / len(recalls),\n",
    "        \"f1\": sum(f1_scores) / len(f1_scores),\n",
    "        \"r@95p\": sum(r_at_95p) / len(r_at_95p),\n",
    "        \"map\": sum(ap_scores) / len(ap_scores),\n",
    "        \"J\": j_score,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "class MetricEvaluator:\n",
    "    \"\"\"\n",
    "    A class to calculate evaluation metrics for Few-Shot Learning and Metric Learning.\n",
    "\n",
    "    Implements:\n",
    "    1. Discriminant Ratio (J): Optimized scalar implementation (O(d) memory).\n",
    "    2. Mean Average Precision (mAP): Ranking quality metric.\n",
    "    3. Recall at Fixed Precision (R@P): Operational metric.\n",
    "    4. Precision & Recall: Raw metrics at a specific similarity threshold.\n",
    "    5. F1 Score: Harmonic mean of Precision and Recall.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device=None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator.\n",
    "\n",
    "        Args:\n",
    "            device (str): 'cuda' or 'cpu'. If None, detects automatically.\n",
    "        \"\"\"\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.epsilon = 1e-6  # For numerical stability\n",
    "\n",
    "    def compute_discriminant_ratio(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Calculates the Discriminant Ratio (J) using the optimized Scalar approach.\n",
    "\n",
    "        Theory:\n",
    "            J = Tr(Sb) / Tr(Sw)\n",
    "            Using the Trace Trick: Tr(Sw) = Tr(St) - Tr(Sb)\n",
    "\n",
    "        Args:\n",
    "            embeddings (torch.Tensor): Tensor of shape (Batch_Size, Dimension).\n",
    "            labels (torch.Tensor): Tensor of class labels.\n",
    "\n",
    "        Returns:\n",
    "            float: The Discriminant Ratio score.\n",
    "        \"\"\"\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        # 1. Global Mean Computation\n",
    "        global_mean = embeddings.mean(dim=0)\n",
    "\n",
    "        # 2. Calculate Trace of Total Scatter (St)\n",
    "        # Sum of squared Euclidean distances of all points from the global mean.\n",
    "        tr_st = torch.sum((embeddings - global_mean) ** 2)\n",
    "\n",
    "        # 3. Calculate Trace of Between-Class Scatter (Sb)\n",
    "        tr_sb = 0\n",
    "        unique_classes = torch.unique(labels)\n",
    "\n",
    "        for c in unique_classes:\n",
    "            class_mask = (labels == c)\n",
    "            class_embeddings = embeddings[class_mask]\n",
    "            n_c = class_embeddings.size(0)\n",
    "\n",
    "            if n_c > 0:\n",
    "                mu_c = class_embeddings.mean(dim=0)\n",
    "                tr_sb += n_c * torch.sum((mu_c - global_mean) ** 2)\n",
    "\n",
    "        # 4. Calculate Trace of Within-Class Scatter (Sw)\n",
    "        tr_sw = tr_st - tr_sb\n",
    "\n",
    "        # Calculate J\n",
    "        j_score = tr_sb / (tr_sw + self.epsilon)\n",
    "\n",
    "        return j_score.item()\n",
    "\n",
    "    def compute_map(self, query_emb, gallery_emb, query_labels, gallery_labels):\n",
    "        \"\"\"\n",
    "        Calculates Mean Average Precision (mAP).\n",
    "        \"\"\"\n",
    "        query_emb = query_emb.to(self.device)\n",
    "        gallery_emb = gallery_emb.to(self.device)\n",
    "        query_labels = query_labels.to(self.device)\n",
    "        gallery_labels = gallery_labels.to(self.device)\n",
    "\n",
    "        # L2 Normalize for Cosine Similarity\n",
    "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
    "        gallery_emb = F.normalize(gallery_emb, p=2, dim=1)\n",
    "\n",
    "        # Similarity Matrix: S = Q * G^T\n",
    "        similarity_matrix = torch.matmul(query_emb, gallery_emb.T)\n",
    "\n",
    "        num_queries = query_labels.size(0)\n",
    "        average_precisions = []\n",
    "\n",
    "        for i in range(num_queries):\n",
    "            scores = similarity_matrix[i]\n",
    "            target_label = query_labels[i]\n",
    "\n",
    "            # Ranking\n",
    "            sorted_indices = torch.argsort(scores, descending=True)\n",
    "            sorted_gallery_labels = gallery_labels[sorted_indices]\n",
    "\n",
    "            # Relevance Mask\n",
    "            relevance_mask = (sorted_gallery_labels == target_label).float()\n",
    "\n",
    "            total_relevant = relevance_mask.sum()\n",
    "            if total_relevant == 0:\n",
    "                average_precisions.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Cumulative Precision\n",
    "            cumsum = torch.cumsum(relevance_mask, dim=0)\n",
    "            ranks = torch.arange(1, len(relevance_mask) + 1).to(self.device)\n",
    "            precisions = cumsum / ranks\n",
    "\n",
    "            # Average Precision (AP)\n",
    "            ap = (precisions * relevance_mask).sum() / total_relevant\n",
    "            average_precisions.append(ap.item())\n",
    "\n",
    "        if not average_precisions:\n",
    "            return 0.0\n",
    "        return sum(average_precisions) / len(average_precisions)\n",
    "\n",
    "    def compute_precision_recall(self, similarity_scores, is_match, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Calculates raw Precision and Recall at a specific similarity threshold.\n",
    "\n",
    "        Definitions:\n",
    "            Precision = TP / (TP + FP)\n",
    "            Recall    = TP / (TP + FN)\n",
    "\n",
    "        Args:\n",
    "            similarity_scores (torch.Tensor): 1D tensor of scores (0.0 to 1.0).\n",
    "            is_match (torch.Tensor): 1D binary tensor (Ground Truth).\n",
    "            threshold (float): Cutoff for deciding if a retrieval is Positive.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (precision, recall)\n",
    "        \"\"\"\n",
    "        similarity_scores = similarity_scores.to(self.device)\n",
    "        is_match = is_match.to(self.device)\n",
    "\n",
    "        # Binarize predictions: 1 if score >= threshold (Positive), else 0 (Negative)\n",
    "        predicted_positive = (similarity_scores >= threshold).float()\n",
    "\n",
    "        # True Positives (TP): Predicted Positive AND Actually Match\n",
    "        tp = (predicted_positive * is_match).sum()\n",
    "\n",
    "        # False Positives (FP): Predicted Positive BUT Actually Non-Match\n",
    "        fp = (predicted_positive * (1 - is_match)).sum()\n",
    "\n",
    "        # False Negatives (FN): Predicted Negative BUT Actually Match\n",
    "        # (We invert the prediction mask to find negatives)\n",
    "        fn = ((1 - predicted_positive) * is_match).sum()\n",
    "\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "\n",
    "        return precision.item(), recall.item()\n",
    "\n",
    "    def compute_recall_at_fixed_precision(self, similarity_scores, is_match, min_precision=0.95):\n",
    "        \"\"\"\n",
    "        Calculates Recall at a Fixed Precision (R@P).\n",
    "        Finds the lowest threshold where Precision >= min_precision.\n",
    "        \"\"\"\n",
    "        similarity_scores = similarity_scores.to(self.device)\n",
    "        is_match = is_match.to(self.device)\n",
    "\n",
    "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
    "        sorted_matches = is_match[sorted_indices]\n",
    "\n",
    "        tps = torch.cumsum(sorted_matches, dim=0)\n",
    "        total_retrieved = torch.arange(1, len(sorted_matches) + 1).to(self.device)\n",
    "\n",
    "        precisions = tps / total_retrieved\n",
    "\n",
    "        # Find indices where Precision satisfies the constraint\n",
    "        valid_indices = torch.where(precisions >= min_precision)[0]\n",
    "\n",
    "        if len(valid_indices) == 0:\n",
    "            return 0.0\n",
    "\n",
    "        cutoff_index = valid_indices[-1]\n",
    "\n",
    "        # Recall = TP_at_cutoff / Total_Relevant_In_Dataset\n",
    "        total_relevant_in_dataset = is_match.sum()\n",
    "\n",
    "        if total_relevant_in_dataset == 0:\n",
    "            return 0.0\n",
    "\n",
    "        recall = tps[cutoff_index] / total_relevant_in_dataset\n",
    "\n",
    "        return recall.item()\n",
    "\n",
    "    def compute_f1_score(self, precision, recall):\n",
    "        \"\"\"\n",
    "        Calculates F1 Score (Harmonic Mean).\n",
    "        \"\"\"\n",
    "        if (precision + recall) == 0:\n",
    "            return 0.0\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "\n",
    "class FewShotIterator:\n",
    "    def __init__(self, file_list, n_shot):\n",
    "        \"\"\"\n",
    "        Initializes the iterator class.\n",
    "        It prepares the global testset and creates a cyclic iterator over the valid brands.\n",
    "        \"\"\"\n",
    "        self.n_shot = n_shot\n",
    "\n",
    "        # 1. Validation: Check if input list is empty\n",
    "        if not file_list:\n",
    "            raise ValueError(\"The test file list is empty.\")\n",
    "\n",
    "        #    (Dataset - SupportSet)  is significantly faster with sets (O(1)) compared to lists.\n",
    "        self.all_files_set = set(file_list)\n",
    "\n",
    "        # 3. Organize data by Brand\n",
    "        #    We create a dictionary mapping: { 'BrandName': [list_of_image_paths] }\n",
    "        self.brands_map = {}\n",
    "\n",
    "        for file_path in file_list:\n",
    "            # Extract brand name assuming structure: .../Category/Brand/Image.jpg\n",
    "            brand_name = os.path.basename(os.path.dirname(file_path))\n",
    "\n",
    "            if brand_name not in self.brands_map:\n",
    "                self.brands_map[brand_name] = []\n",
    "            self.brands_map[brand_name].append(file_path)\n",
    "\n",
    "        self.valid_brands_list = list(self.brands_map.keys())\n",
    "\n",
    "        if not self.valid_brands_list:\n",
    "            raise ValueError(f\"No brand found with more than {n_shot} images.\")\n",
    "\n",
    "        #    'itertools.cycle' creates an infinite loop over the valid brands list.\n",
    "        self.brand_iterator = cycle(self.valid_brands_list)\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Executed when the class instance is called.\n",
    "        Logic:\n",
    "        1. Pick next brand (Sequential).\n",
    "        2. Pick Support Set (Random 5 images from that brand).\n",
    "        3. Pick Query Set (EVERYTHING else in the testset).\n",
    "        \"\"\"\n",
    "        # A. Get the next brand sequentially from the cycle\n",
    "        try:\n",
    "            selected_brand_name = next(self.brand_iterator)\n",
    "        except StopIteration:\n",
    "            # Gracefully signal that we are done\n",
    "            print(\"Iterator finished: All brands have been processed.\")\n",
    "            return None\n",
    "\n",
    "        # B. Retrieve all images specific to this chosen brand\n",
    "        images_of_current_brand = self.brands_map[selected_brand_name]\n",
    "\n",
    "        # Select a random number between 1 to 5 which is the number of images of the support brand guaranteed in the query set\n",
    "        num_query_guarantee_if_available = random.randint(1, 5)\n",
    "\n",
    "        # C. Create SUPPORT SET\n",
    "        #    Select 'n_shot' unique images randomly from the current brand.\n",
    "        support_set_list = random.sample(images_of_current_brand, self.n_shot)\n",
    "        support_set_set = set(support_set_list)\n",
    "\n",
    "        # D. Create QUERY SET (Global Subtraction)\n",
    "        #    Requirement: The Query Set contains 50 images of which at least 1 is from the support brand\n",
    "        #    Step 1: initialize the Query list\n",
    "        query_set_list = []\n",
    "\n",
    "        #    Step 2: Sample randomly the images to guarantee in the query set\n",
    "        remaining_brand_images = list(set(images_of_current_brand) - support_set_set)\n",
    "        guaranteed_images_in_query = random.sample(remaining_brand_images, min(num_query_guarantee_if_available, len(remaining_brand_images)))\n",
    "\n",
    "        if (len(remaining_brand_images) == 0):\n",
    "            print(f\"for the brand {selected_brand_name} {len(remaining_brand_images)} images where put in the query set\")\n",
    "\n",
    "        #    Step 3: Sample the Negative Queries (Distractors from OTHER brands)\n",
    "        # We subtract ALL images of the current brand to ensure zero accidental matches\n",
    "        remaining_images_in_query = list(self.all_files_set - set(images_of_current_brand) - set(guaranteed_images_in_query))\n",
    "\n",
    "        total_query_size = 50\n",
    "        num_remaining_query = total_query_size - min(num_query_guarantee_if_available, len(remaining_brand_images))\n",
    "        query_remaining = random.sample(remaining_images_in_query,  min(len(remaining_images_in_query),num_remaining_query))\n",
    "\n",
    "        #    Step 4: Combine and Shuffle\n",
    "        query_set_list = guaranteed_images_in_query + query_remaining\n",
    "        random.shuffle(query_set_list)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"brand_name\": selected_brand_name,\n",
    "            \"support_set\": support_set_list,\n",
    "            \"query_set\": query_set_list\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed5c9439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scansione cartelle in: c:\\Users\\Lenovo\\Desktop\\ProgettoFinale1\\PY_script\\LogoDet-3K\n",
      "✅ Trovati 3000 brand totali.\n",
      "[HEAVY TASK] Inizio compute_global_embeddings\n",
      "\n",
      "[START] Inizio 'compute_global_embeddings' su 158654 immagini.\n",
      "[INFO] Creazione Dataset e DataLoader lineare...\n",
      "[INFO] DataLoader pronto. Totale batch da processare: 2479\n",
      "[LOOP] Inizio estrazione feature (questo processo usa CPU/GPU intensamente)...\n",
      "  > [Batch 2479/2479] Elaborazione batch in corso...\n",
      "[INFO] Loop batch terminato. Concatenazione dei risultati...\n",
      "[END] 'compute_global_embeddings' completato. Shape finale: torch.Size([158654, 2048])\n",
      "[HEAVY TASK] fine compute_global_embeddings\n",
      "  > [Episodio 1/10] Generazione task...\n",
      "    [DEBUG Ep.1] Support size: 1, Query size: 50\n",
      "  > [Episodio 10/10] Completato. (Acc parziale: 0.98)\n",
      "Accuracy       : 0.9160\n",
      "Precision      : 0.1542\n",
      "Recall         : 0.1950\n",
      "F1             : 0.1624\n",
      "R@95p          : 0.1550\n",
      "Map            : 0.2852\n",
      "J              : 0.3716\n"
     ]
    }
   ],
   "source": [
    "def get_baseline_resnet50(device):\n",
    "    \"\"\"\n",
    "    Scarica la ResNet50 originale pre-addestrata su ImageNet.\n",
    "    Sostituisce il layer fully connected (fc) con Identity per restituire\n",
    "    i feature embeddings (dimensione 2048) invece delle classi.\n",
    "    \"\"\"\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    model = models.resnet50(weights=weights)\n",
    "    model.fc = nn.Identity()\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "\n",
    "    # 1. Configurazione Dispositivo e Trasformazioni\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    device_name = Config.device if hasattr(Config, 'device') else 'cuda'\n",
    "    device = torch.device(device_name)\n",
    "\n",
    "    # 2. Istanzia il modello Baseline\n",
    "    # Chiama la funzione con i print aggiunti sopra\n",
    "    model = get_baseline_resnet50(device)\n",
    "\n",
    "    dataset_root = Config.dataset_root\n",
    "    \n",
    "    test_paths = getTestPaths(dataset_root) \n",
    "    iterator = FewShotIterator(test_paths, n_shot=Config.n_shot)\n",
    "    \n",
    "    # Nota: i print interni al loop dipendono da come è scritta evaluate_few_shot, \n",
    "    # ma qui vediamo quando entra e quando esce.\n",
    "    results = evaluate_few_shot(\n",
    "        model=model, \n",
    "        fewshot_iterator=iterator, \n",
    "        transform=transform, \n",
    "        device=device, \n",
    "        num_episodes=Config.num_episodes\n",
    "    )\n",
    "\n",
    "    # 5. Stampa dei Risultati\n",
    "    for metrica, valore in results.items():\n",
    "        print(f\"{metrica.capitalize():<15}: {valore:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
