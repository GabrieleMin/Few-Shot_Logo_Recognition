{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2PxXj8JgvT7"
      },
      "source": [
        "# Few-Shot Evaluation Notebook\n",
        "\n",
        "This notebook implements the evaluation logic for a few-shot learning model using a ResNet50 backbone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bib25CWFgvT8"
      },
      "source": [
        "### Imports and configuration settings\n",
        "\n",
        "**Key operations:**\n",
        "1. The libraries needed are imported\n",
        "2. **Config**: Config contains all the configurations necessary to run the script\n",
        "3. Seeding to obtain a repeatable experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3CGki6RgvT9",
        "outputId": "a03d0ad4-2540-44d6-d226-9816bc0c3b86"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "from itertools import cycle\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "class Config:\n",
        "    # 1. SETUP\n",
        "    project_name = \"FewShot_Evaluation\"\n",
        "    seed = 42\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 2. DATASET PATH\n",
        "    dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
        "    csv_index_path = \"LogoDet-3K/brand_to_index.csv\"\n",
        "\n",
        "    # 3. MODEL PARAMETERS\n",
        "    embedding_dim = 128\n",
        "    pretrained = True\n",
        "    freeze_layers = 5\n",
        "    trained_model_path = \"\"\n",
        "\n",
        "    # 4. EVALUATION SETTINGS\n",
        "    prediciton_threashold = 0.5\n",
        "    n_shot = 1\n",
        "    num_episodes = 1000\n",
        "\n",
        "torch.manual_seed(Config.seed)\n",
        "random.seed(Config.seed)\n",
        "\n",
        "def setup_dataset(zip_path, extract_to):\n",
        "    \"\"\"\n",
        "    Mounts Google Drive and extracts the dataset if not already present.\n",
        "    \"\"\"\n",
        "    # 1. Mount Google Drive\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # 2. Check if the folder already exists\n",
        "    if os.path.exists(extract_to):\n",
        "        print(f\"Dataset folder '{extract_to}' already exists. Skipping extraction.\")\n",
        "    else:\n",
        "        print(f\"Extracting dataset from {zip_path}...\")\n",
        "        if os.path.exists(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "            print(\"Extraction complete.\")\n",
        "        else:\n",
        "            print(f\"ERROR: Zip file not found at {zip_path}. Check your path.\")\n",
        "\n",
        "setup_dataset(\"/content/drive/MyDrive/LogoDet-3K-divided.zip\", \"/content/LogoDet-3K\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74w2UW_bgvT-"
      },
      "source": [
        "### Dataset and Model Architecture\n",
        "\n",
        "**Key operations:**\n",
        "1. **DatasetTest**: Specialized loader for test images that parses XML files to retrieve label indices.\n",
        "2. **LogoResNet50**: A modified ResNet50 architecture that replaces the final classifier with a layer generating feature embeddings.\n",
        "3. **load_model**: A function that given the path of a saved model loads it and returns it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls0Bk17QgvT-"
      },
      "outputs": [],
      "source": [
        "class DatasetTest(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    # Load label string to index mapping\n",
        "        df = pd.read_csv(Config.csv_index_path)\n",
        "        self.label_to_id = {row['brand']: int(row['index']) for _, row in df.iterrows()}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        img = Image.open(image_path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        image_label = os.path.basename(os.path.dirname(image_path))\n",
        "\n",
        "        label_idx = self.label_to_id[image_label]\n",
        "\n",
        "        return {\"image\": img, \"label\": label_idx}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.load_image(self.file_list[idx])\n",
        "\n",
        "class LogoResNet50(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True, num_of_freeze_layer=5, activation_fn=None):\n",
        "        super(LogoResNet50, self).__init__()\n",
        "\n",
        "        # 1. Load Pre-trained Weights\n",
        "        # Initialize the model with weights pretrained on ImageNet for transfer learning\n",
        "        if pretrained:\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "            self.model = models.resnet50(weights=weights)\n",
        "        else:\n",
        "            self.model = models.resnet50(weights=None)\n",
        "\n",
        "        # 2. Modify the Head (Fully Connected Layer)\n",
        "        # We need to produce feature embeddings instead of class probabilities\n",
        "        input_features_fc = self.model.fc.in_features # Typically 2048 for ResNet50\n",
        "\n",
        "        head_layers = []\n",
        "        # Project features to the desired embedding dimension (e.g., 128)\n",
        "        head_layers.append(nn.Linear(input_features_fc, embedding_dim))\n",
        "\n",
        "        # Add an optional activation function if provided\n",
        "        if activation_fn is not None:\n",
        "            head_layers.append(activation_fn)\n",
        "\n",
        "        # Replace the original classifier with our custom embedding head\n",
        "        self.model.fc = nn.Sequential(*head_layers)\n",
        "\n",
        "        # 3. Freezing Management\n",
        "        # Define the blocks here to access them in the freeze method.\n",
        "        # This structure allows progressive freezing/unfreezing strategies\n",
        "        self.blocks = [\n",
        "            ['conv1', 'bn1'],   # Level 1\n",
        "            ['layer1'],         # Level 2\n",
        "            ['layer2'],         # Level 3\n",
        "            ['layer3'],         # Level 4\n",
        "            ['layer4'],         # Level 5: Entire backbone frozen\n",
        "        ]\n",
        "\n",
        "        # Apply the initial freezing configuration\n",
        "        self.freeze_numer_of_layer(num_of_freeze_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def freeze_numer_of_layer(self, num_of_freeze_layer):\n",
        "        \"\"\"\n",
        "        Manages layer freezing for transfer learning strategies.\n",
        "\n",
        "        Args:\n",
        "            num_of_freeze_layer (int):\n",
        "              0   -> All layers unlocked (Full Fine-Tuning)\n",
        "              1-5 -> Progressively freezes the backbone layers from shallow to deep\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: RESET. Unfreeze everything (requires_grad = True).\n",
        "        # This ensures we start from a clean state before applying new constraints.\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # If num is 0, exit immediately (Full Fine-Tuning mode)\n",
        "        if num_of_freeze_layer == 0:\n",
        "            print(\"Configuration: Full Fine-Tuning (All layers are trainable)\")\n",
        "            return\n",
        "\n",
        "        # Safety check to avoid index out of bounds\n",
        "        limit = min(num_of_freeze_layer, len(self.blocks))\n",
        "\n",
        "        frozen_list = []\n",
        "\n",
        "        # STEP 2: Progressively freeze the requested blocks\n",
        "        for i in range(limit):\n",
        "            current_blocks = self.blocks[i]\n",
        "            for block_name in current_blocks:\n",
        "                # Retrieve the layer by name\n",
        "                layer = getattr(self.model, block_name)\n",
        "\n",
        "                # Freeze parameters for this specific block\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                frozen_list.append(block_name)\n",
        "\n",
        "        print(f\"Freezing Level {limit}. Frozen blocks: {frozen_list}\")\n",
        "\n",
        "def load_model(model_path, device):\n",
        "    model = LogoResNet50(embedding_dim=Config.embedding_dim, pretrained=Config.pretrained, num_of_freeze_layer=Config.freeze_layers)\n",
        "    # state = torch.load(model_path)\n",
        "    # model.load_state_dict(state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XARVVcgIgvT-"
      },
      "source": [
        "### Evaluation Utilities\n",
        "\n",
        "**Key operations:**\n",
        "1. **MetricEvaluator**: Calculates standard metrics like Discriminant Ration, mAP, Precision, Recall, and F1 Score.\n",
        "2. **FewShotIterator**: Manages the sampling of N-Shot tasks from the test dataset.\n",
        "3. **getTestPaths**: A function that returns a set of paths from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5wdZW04gvT-"
      },
      "outputs": [],
      "source": [
        "class MetricEvaluator:\n",
        "    \"\"\"\n",
        "    A class to calculate evaluation metrics for Few-Shot Learning and Metric Learning.\n",
        "\n",
        "    Implements:\n",
        "    1. Discriminant Ratio (J): Optimized scalar implementation (O(d) memory).\n",
        "    2. Mean Average Precision (mAP): Ranking quality metric.\n",
        "    3. Recall at Fixed Precision (R@P): Operational metric.\n",
        "    4. Precision & Recall: Raw metrics at a specific similarity threshold.\n",
        "    5. F1 Score: Harmonic mean of Precision and Recall.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        \"\"\"\n",
        "        Initialize the evaluator.\n",
        "\n",
        "        Args:\n",
        "            device (str): 'cuda' or 'cpu'. If None, detects automatically.\n",
        "        \"\"\"\n",
        "        if device:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.epsilon = 1e-6  # For numerical stability\n",
        "\n",
        "    def compute_discriminant_ratio(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Calculates the Discriminant Ratio (J) using the optimized Scalar approach.\n",
        "\n",
        "        Theory:\n",
        "            J = Tr(Sb) / Tr(Sw)\n",
        "            Using the Trace Trick: Tr(Sw) = Tr(St) - Tr(Sb)\n",
        "\n",
        "        Args:\n",
        "            embeddings (torch.Tensor): Tensor of shape (Batch_Size, Dimension).\n",
        "            labels (torch.Tensor): Tensor of class labels.\n",
        "\n",
        "        Returns:\n",
        "            float: The Discriminant Ratio score.\n",
        "        \"\"\"\n",
        "        embeddings = embeddings.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # 1. Global Mean Computation\n",
        "        global_mean = embeddings.mean(dim=0)\n",
        "\n",
        "        # 2. Calculate Trace of Total Scatter (St)\n",
        "        # Sum of squared Euclidean distances of all points from the global mean.\n",
        "        tr_st = torch.sum((embeddings - global_mean) ** 2)\n",
        "\n",
        "        # 3. Calculate Trace of Between-Class Scatter (Sb)\n",
        "        tr_sb = 0\n",
        "        unique_classes = torch.unique(labels)\n",
        "\n",
        "        for c in unique_classes:\n",
        "            class_mask = (labels == c)\n",
        "            class_embeddings = embeddings[class_mask]\n",
        "            n_c = class_embeddings.size(0)\n",
        "\n",
        "            if n_c > 0:\n",
        "                mu_c = class_embeddings.mean(dim=0)\n",
        "                tr_sb += n_c * torch.sum((mu_c - global_mean) ** 2)\n",
        "\n",
        "        # 4. Calculate Trace of Within-Class Scatter (Sw)\n",
        "        tr_sw = tr_st - tr_sb\n",
        "\n",
        "        # Calculate J\n",
        "        j_score = tr_sb / (tr_sw + self.epsilon)\n",
        "\n",
        "        return j_score.item()\n",
        "\n",
        "    def compute_map(self, query_emb, gallery_emb, query_labels, gallery_labels):\n",
        "        \"\"\"\n",
        "        Calculates Mean Average Precision (mAP).\n",
        "        \"\"\"\n",
        "        query_emb = query_emb.to(self.device)\n",
        "        gallery_emb = gallery_emb.to(self.device)\n",
        "        query_labels = query_labels.to(self.device)\n",
        "        gallery_labels = gallery_labels.to(self.device)\n",
        "\n",
        "     \n",
        "        distance_matrix = torch.cdist(query_emb, gallery_emb, p=2)\n",
        "\n",
        "\n",
        "        num_queries = query_labels.size(0)\n",
        "        average_precisions = []\n",
        "\n",
        "        for i in range(num_queries):\n",
        "            scores = distance_matrix[i]\n",
        "            target_label = query_labels[i]\n",
        "\n",
        "            # Ranking\n",
        "            sorted_indices = torch.argsort(scores, descending=False)\n",
        "            sorted_gallery_labels = gallery_labels[sorted_indices]\n",
        "\n",
        "            # Relevance Mask\n",
        "            relevance_mask = (sorted_gallery_labels == target_label).float()\n",
        "\n",
        "            total_relevant = relevance_mask.sum()\n",
        "            if total_relevant == 0:\n",
        "                average_precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # Cumulative Precision\n",
        "            cumsum = torch.cumsum(relevance_mask, dim=0)\n",
        "            ranks = torch.arange(1, len(relevance_mask) + 1).to(self.device)\n",
        "            precisions = cumsum / ranks\n",
        "\n",
        "            # Average Precision (AP)\n",
        "            ap = (precisions * relevance_mask).sum() / total_relevant\n",
        "            average_precisions.append(ap.item())\n",
        "\n",
        "        if not average_precisions:\n",
        "            return 0.0\n",
        "        return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "    def compute_precision_recall(self, distance_scores, is_match, threshold=Config.prediciton_threashold):\n",
        "        \"\"\"\n",
        "        Calculates raw Precision and Recall at a specific similarity threshold.\n",
        "\n",
        "        Definitions:\n",
        "            Precision = TP / (TP + FP)\n",
        "            Recall    = TP / (TP + FN)\n",
        "\n",
        "        Args:\n",
        "            distance_scores: (torch.Tensor): 1D tensor of scores.\n",
        "            is_match (torch.Tensor): 1D binary tensor (Ground Truth).\n",
        "            threshold (float): Cutoff for deciding if a retrieval is Positive.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (precision, recall)\n",
        "        \"\"\"\n",
        "        distance_scores = distance_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        # Binarize predictions: 1 if score <= threshold (Positive), else 0 (Negative)\n",
        "        predicted_positive = (distance_scores <= threshold).float()\n",
        "\n",
        "        # True Positives (TP): Predicted Positive AND Actually Match\n",
        "        tp = (predicted_positive * is_match).sum()\n",
        "\n",
        "        # False Positives (FP): Predicted Positive BUT Actually Non-Match\n",
        "        fp = (predicted_positive * (1 - is_match)).sum()\n",
        "\n",
        "        # False Negatives (FN): Predicted Negative BUT Actually Match\n",
        "        # (We invert the prediction mask to find negatives)\n",
        "        fn = ((1 - predicted_positive) * is_match).sum()\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        return precision.item(), recall.item()\n",
        "\n",
        "    def compute_recall_at_fixed_precision(self, distance_scores, is_match, min_precision=0.95):\n",
        "        \"\"\"\n",
        "        Calculates Recall at a Fixed Precision (R@P).\n",
        "        Finds the lowest threshold where Precision >= min_precision.\n",
        "        \"\"\"\n",
        "        distance_scores = distance_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        sorted_indices = torch.argsort(distance_scores, descending=False)\n",
        "        sorted_matches = is_match[sorted_indices]\n",
        "\n",
        "        tps = torch.cumsum(sorted_matches, dim=0)\n",
        "        total_retrieved = torch.arange(1, len(sorted_matches) + 1).to(self.device)\n",
        "\n",
        "        precisions = tps / total_retrieved\n",
        "\n",
        "        # Find indices where Precision satisfies the constraint\n",
        "        valid_indices = torch.where(precisions >= min_precision)[0]\n",
        "\n",
        "        if len(valid_indices) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cutoff_index = valid_indices[-1]\n",
        "\n",
        "        # Recall = TP_at_cutoff / Total_Relevant_In_Dataset\n",
        "        total_relevant_in_dataset = is_match.sum()\n",
        "\n",
        "        if total_relevant_in_dataset == 0:\n",
        "            return 0.0\n",
        "\n",
        "        recall = tps[cutoff_index] / total_relevant_in_dataset\n",
        "\n",
        "        return recall.item()\n",
        "\n",
        "    def compute_f1_score(self, precision, recall):\n",
        "        \"\"\"\n",
        "        Calculates F1 Score (Harmonic Mean).\n",
        "        \"\"\"\n",
        "        if (precision + recall) == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "class FewShotIterator:\n",
        "    def __init__(self, file_list, n_shot):\n",
        "        \"\"\"\n",
        "        Initializes the iterator class.\n",
        "        It prepares the global testset and creates a cyclic iterator over the valid brands.\n",
        "        \"\"\"\n",
        "        self.n_shot = n_shot\n",
        "\n",
        "        # 1. Validation: Check if input list is empty\n",
        "        if not file_list:\n",
        "            raise ValueError(\"The test file list is empty.\")\n",
        "\n",
        "        #    (Dataset - SupportSet)  is significantly faster with sets (O(1)) compared to lists.\n",
        "        self.all_files_set = set(file_list)\n",
        "\n",
        "        # 3. Organize data by Brand\n",
        "        #    We create a dictionary mapping: { 'BrandName': [list_of_image_paths] }\n",
        "        self.brands_map = {}\n",
        "\n",
        "        for file_path in file_list:\n",
        "            # Extract brand name assuming structure: .../Category/Brand/Image.jpg\n",
        "            brand_name = os.path.basename(os.path.dirname(file_path))\n",
        "\n",
        "            if brand_name not in self.brands_map:\n",
        "                self.brands_map[brand_name] = []\n",
        "            self.brands_map[brand_name].append(file_path)\n",
        "\n",
        "        self.valid_brands_list = list(self.brands_map.keys())\n",
        "\n",
        "        if not self.valid_brands_list:\n",
        "            raise ValueError(f\"No brand found with more than {n_shot} images.\")\n",
        "\n",
        "        #    'itertools.cycle' creates an infinite loop over the valid brands list.\n",
        "        self.brand_iterator = cycle(self.valid_brands_list)\n",
        "\n",
        "    def __call__(self):\n",
        "        \"\"\"\n",
        "        Executed when the class instance is called.\n",
        "        Logic:\n",
        "        1. Pick next brand (Sequential).\n",
        "        2. Pick Support Set (Random 5 images from that brand).\n",
        "        3. Pick Query Set (EVERYTHING else in the testset).\n",
        "        \"\"\"\n",
        "        # A. Get the next brand sequentially from the cycle\n",
        "        try:\n",
        "            selected_brand_name = next(self.brand_iterator)\n",
        "        except StopIteration:\n",
        "            # Gracefully signal that we are done\n",
        "            print(\"Iterator finished: All brands have been processed.\")\n",
        "            return None\n",
        "\n",
        "        # B. Retrieve all images specific to this chosen brand\n",
        "        images_of_current_brand = self.brands_map[selected_brand_name]\n",
        "\n",
        "        # Select a random number between 1 to 5 which is the number of images of the support brand guaranteed in the query set\n",
        "        num_query_guarantee_if_available = random.randint(1, 5)\n",
        "\n",
        "        # C. Create SUPPORT SET\n",
        "        #    Select 'n_shot' unique images randomly from the current brand.\n",
        "        support_set_list = random.sample(images_of_current_brand, self.n_shot)\n",
        "        support_set_set = set(support_set_list)\n",
        "\n",
        "        # D. Create QUERY SET (Global Subtraction)\n",
        "        #    Requirement: The Query Set contains 50 images of which at least 1 is from the support brand\n",
        "        #    Step 1: initialize the Query list\n",
        "        query_set_list = []\n",
        "\n",
        "        #    Step 2: Sample randomly the images to guarantee in the query set\n",
        "        remaining_brand_images = list(set(images_of_current_brand) - support_set_set)\n",
        "        guaranteed_images_in_query = random.sample(remaining_brand_images, min(num_query_guarantee_if_available, len(remaining_brand_images)))\n",
        "\n",
        "        if (len(remaining_brand_images) == 0):\n",
        "            print(f\"for the brand {selected_brand_name} {len(remaining_brand_images)} images where put in the query set\")\n",
        "\n",
        "        #    Step 3: Sample the Negative Queries (Distractors from OTHER brands)\n",
        "        # We subtract ALL images of the current brand to ensure zero accidental matches\n",
        "        remaining_images_in_query = list(self.all_files_set - set(images_of_current_brand) - set(guaranteed_images_in_query))\n",
        "\n",
        "        total_query_size = 50\n",
        "        num_remaining_query = total_query_size - min(num_query_guarantee_if_available, len(remaining_brand_images))\n",
        "        query_remaining = random.sample(remaining_images_in_query,  min(len(remaining_images_in_query),num_remaining_query))\n",
        "\n",
        "        #    Step 4: Combine and Shuffle\n",
        "        query_set_list = guaranteed_images_in_query + query_remaining\n",
        "        random.shuffle(query_set_list)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"brand_name\": selected_brand_name,\n",
        "            \"support_set\": support_set_list,\n",
        "            \"query_set\": query_set_list\n",
        "        }\n",
        "\n",
        "def getTestPaths(root_dir, total_set_size=None, min_images_per_brand=2):\n",
        "    test_path = os.path.join(root_dir, 'test')\n",
        "    test_brand_list = []\n",
        "\n",
        "    # Collect brand folders\n",
        "    if not os.path.exists(test_path):\n",
        "        print(f\"Warning: {test_path} not found.\")\n",
        "        return []\n",
        "\n",
        "    for category in os.listdir(test_path):\n",
        "        cat_path = os.path.join(test_path, category)\n",
        "        if os.path.isdir(cat_path):\n",
        "            for brand in os.listdir(cat_path):\n",
        "                brand_full_path = os.path.join(cat_path, brand)\n",
        "                if os.path.isdir(brand_full_path):\n",
        "                    test_brand_list.append(brand_full_path)\n",
        "\n",
        "    test_data_list = []\n",
        "\n",
        "    # Sampling Logic\n",
        "    if total_set_size is not None:\n",
        "        images_per_brand = round(total_set_size / len(test_brand_list))\n",
        "\n",
        "        if images_per_brand < min_images_per_brand:\n",
        "            new_test_brand_count = round(total_set_size / min_images_per_brand)\n",
        "            test_brand_list = random.sample(test_brand_list, min(len(test_brand_list), new_test_brand_count))\n",
        "            images_per_brand = min_images_per_brand\n",
        "\n",
        "        for brand in test_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the TEST set\")\n",
        "\n",
        "            test_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "    else:\n",
        "        for brand in test_brand_list:\n",
        "            test_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "\n",
        "    return test_data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3MinJ8igvT_"
      },
      "source": [
        "### Evaluation Loop\n",
        "\n",
        "**Key operations:**\n",
        "1. **evaluate_few_shot**: Runs the evaluation episodes, computes embeddings, and calculates aggregated metrics.\n",
        "2. **Distance Calculation**: Distances are computed using F.pairwise_distance.\n",
        "3. **compute_global_embeddings**: A function used to compute the embeddings of all passed images and return the embeddings + the respective labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKX_RvrDgvT_",
        "outputId": "af4af7cf-15d6-4442-e92b-0cb824d85afd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing Level 5. Frozen blocks: ['conv1', 'bn1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
            "J score computed: 1.9644616842269897\n",
            "\n",
            "=== Evaluation Results ===\n",
            "Accuracy  : 0.8944\n",
            "Precision : 0.1313\n",
            "Recall    : 0.4800\n",
            "F1        : 0.1786\n",
            "R@95p     : 0.3300\n",
            "Map       : 0.4174\n",
            "J         : 1.9645\n"
          ]
        }
      ],
      "source": [
        "def evaluate_few_shot(model, fewshot_iterator, transform, device, num_episodes=100):\n",
        "    evaluator = MetricEvaluator(device=device)\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    r_at_95p = []\n",
        "    ap_scores = []\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    unique_paths = list(fewshot_iterator.all_files_set)\n",
        "    embs, labels = compute_global_embeddings(model, unique_paths, transform, device)\n",
        "    j_score = evaluator.compute_discriminant_ratio(embs, labels)\n",
        "\n",
        "    # 2. Set to eval mode and disable gradient tracking\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_episodes):\n",
        "\n",
        "            task = fewshot_iterator()\n",
        "\n",
        "            if task is None:\n",
        "                print(f\"Stopped early at episode {i} because we ran out of brands.\")\n",
        "                break\n",
        "\n",
        "            support_paths = task[\"support_set\"]\n",
        "            query_paths = task[\"query_set\"]\n",
        "\n",
        "            # Build datasets and loaders\n",
        "            support_dataset = DatasetTest(support_paths, transform)\n",
        "            query_dataset = DatasetTest(query_paths, transform)\n",
        "\n",
        "            support_loader = DataLoader(support_dataset, batch_size=32)\n",
        "            query_loader = DataLoader(query_dataset, batch_size=64)\n",
        "\n",
        "            # Extract embeddings\n",
        "            support_embeddings = []\n",
        "            query_embeddings = []\n",
        "            query_labels = []\n",
        "            support_labels = []\n",
        "\n",
        "            # Compute embeddings for support set\n",
        "            for data in support_loader:\n",
        "                images = data[\"image\"].to(device)\n",
        "                support_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
        "\n",
        "                batch_labels = data[\"label\"]\n",
        "                support_labels.append(batch_labels)\n",
        "                support_brand = batch_labels[0]\n",
        "\n",
        "            support_embeddings_tensor = torch.cat(support_embeddings)\n",
        "            support_labels_tensor = torch.cat(support_labels)\n",
        "\n",
        "            # Average embeddings\n",
        "            averaged_support_embeddings = support_embeddings_tensor.mean(dim=0)\n",
        "            averaged_support_embeddings_for_ap = support_embeddings_tensor.mean(dim=0, keepdim=True)\n",
        "            averaged_support_embeddings_unsqueezed = averaged_support_embeddings.unsqueeze(0)\n",
        "           \n",
        "            # Compute embeddings for query set\n",
        "            for data in query_loader:\n",
        "                images = data[\"image\"].to(device)\n",
        "                query_embeddings.append(F.normalize(model(images), p=2, dim=1))\n",
        "\n",
        "                batch_labels = data[\"label\"]\n",
        "                query_labels.append(batch_labels)\n",
        "\n",
        "            # query_embeddings and query_labels are list of tensors, this unrolls them\n",
        "            query_embeddings_tensor = torch.cat(query_embeddings)\n",
        "            query_labels_tensor = torch.cat(query_labels)\n",
        "\n",
        "            # mAP\n",
        "            ap_score_single = evaluator.compute_map(\n",
        "                query_emb=averaged_support_embeddings_unsqueezed,\n",
        "                gallery_emb=query_embeddings_tensor,\n",
        "                query_labels=support_brand.unsqueeze(0).to(device),\n",
        "                gallery_labels=query_labels_tensor.to(device)\n",
        "            )\n",
        "\n",
        "            ap_scores.append(ap_score_single)\n",
        "\n",
        "            # Pairwise Distance\n",
        "            dists = F.pairwise_distance(averaged_support_embeddings_unsqueezed, query_embeddings_tensor)\n",
        "\n",
        "            # Ground truth: query belongs to support brand?\n",
        "            gt = (query_labels_tensor == support_brand).float().to(device)\n",
        "\n",
        "            # Predictions\n",
        "            pred = (dists <= Config.prediciton_threashold).float()\n",
        "\n",
        "\n",
        "            # Accuracy\n",
        "            acc = (pred == gt).float().mean().item()\n",
        "            accuracies.append(acc)\n",
        "\n",
        "            # Precision, Recall, F1\n",
        "            neg_dists = -dists\n",
        "            neg_threshold = -Config.prediciton_threashold\n",
        "\n",
        "            prec, rec = evaluator.compute_precision_recall(neg_dists, gt, threshold=neg_threshold)\n",
        "            f1 = evaluator.compute_f1_score(prec, rec)\n",
        "            r95 = evaluator.compute_recall_at_fixed_precision(neg_dists, gt, min_precision=0.95)\n",
        "\n",
        "            precisions.append(prec)\n",
        "            recalls.append(rec)\n",
        "            f1_scores.append(f1)\n",
        "            r_at_95p.append(r95)\n",
        "\n",
        "    # Aggregate results\n",
        "    results = {\n",
        "        \"accuracy\": sum(accuracies) / len(accuracies),\n",
        "        \"precision\": sum(precisions) / len(precisions),\n",
        "        \"recall\": sum(recalls) / len(recalls),\n",
        "        \"f1\": sum(f1_scores) / len(f1_scores),\n",
        "        \"r@95p\": sum(r_at_95p) / len(r_at_95p),\n",
        "        \"map\": sum(ap_scores) / len(ap_scores),\n",
        "        \"J\": j_score,\n",
        "    }\n",
        "    return results\n",
        "\n",
        "\n",
        "def compute_global_embeddings(model, file_list, transform, device):\n",
        "    model.eval()\n",
        "    all_embeddings = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Simple linear dataset of all unique test images\n",
        "    dataset = DatasetTest(file_list, transform)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            images = data[\"image\"].to(device)\n",
        "            labels = data[\"label\"]\n",
        "\n",
        "            # Extract and move to CPU to save VRAM\n",
        "            embeddings = F.normalize(model(images), p=2, dim=1).cpu()\n",
        "\n",
        "            all_embeddings.append(embeddings)\n",
        "            all_labels.append(labels)\n",
        "\n",
        "    return torch.cat(all_embeddings), torch.cat(all_labels)\n",
        "\n",
        "def main():\n",
        "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),\n",
        "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    device = torch.device(Config.device)\n",
        "    model = load_model(Config.trained_model_path, device)\n",
        "    # model.load_state_dict(torch.load(Config.trained_model_path)) # Uncomment to load trained weights\n",
        "\n",
        "    test_paths = getTestPaths(Config.dataset_root) # Optionally add total_set_size and min_images_per_brand\n",
        "    iterator = FewShotIterator(test_paths, n_shot=Config.n_shot)\n",
        "\n",
        "    results = evaluate_few_shot(model, iterator, transform, device, Config.num_episodes)\n",
        "    print(\"\\n=== Evaluation Results ===\")\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k.capitalize():<10}: {v:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **show_visual_demo** Function \n",
        "\n",
        "This function performs a Visual Qualitative Analysis of the Deep Metric Learning model's performance. The goal is to simulate a real-world image retrieval scenario and verify if the network has successfully learned to distinguish between different brands.\n",
        "\n",
        "The workflow is as follows:\n",
        "\n",
        "1. **Triplet Sampling**: The code randomly selects three images from the Test Dataset:\n",
        "\n",
        "- Anchor (Query): A reference image of a random brand (e.g., \"Adidas\").\n",
        "\n",
        "- Positive: Another image of the same brand as the Anchor (the correct match the model should retrieve).\n",
        "\n",
        "- Negative: An image of a different brand (distractor) that the model should reject.\n",
        "\n",
        "2. **Feature Extraction (Inference)**: The three images are pre-processed and passed through the trained model. The model outputs a feature embedding (a 128-dimensional numeric vector) for each image, representing the visual characteristics of the logo. These vectors are then normalized.\n",
        "\n",
        "3. **Distance Calculation**: The Pairwise Distance (Euclidean Distance) is computed between the embeddings:\n",
        "\n",
        "Anchor vs. Positive: We expect a very low value (close to 0.0), indicating the vectors are close in the latent space.\n",
        "\n",
        "Anchor vs. Negative: We expect a higher value, indicating the model successfully pushed the features of different brands apart.\n",
        "\n",
        "4. **Result Visualization**: A plot with three side-by-side panels is generated. The title color indicates the test outcome based on the configured threshold (Config.prediciton_threashold, typically 0.5):\n",
        "\n",
        "**Center Panel (Positive Pair)**:\n",
        "\n",
        "- Green (MATCH): Similarity is above the threshold. The model correctly recognized the brand (True Positive).\n",
        "\n",
        "- Red (MISSED): Similarity is below the threshold. The model failed to recognize the same brand (False Negative).\n",
        "\n",
        "**Right Panel (Negative Pair)**:\n",
        "\n",
        "- Green (REJECTED): Similarity is below the threshold. The model correctly distinguished the different brands (True Negative).\n",
        "\n",
        "- Red (FALSE POSITIVE): Similarity is above the threshold. The model confused the different brand with the Anchor (False Positive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_embedding_demo(model, image_path, transform, device):\n",
        "    \"\"\"Extracts the embedding of a single image for the demo.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening {image_path}: {e}\")\n",
        "        return None, None\n",
        "    \n",
        "    # Prepare the tensor (add batch dimension)\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        emb = model(img_tensor)\n",
        "        emb = F.normalize(emb, p=2, dim=1) \n",
        "    \n",
        "    return emb, img\n",
        "\n",
        "def show_visual_demo():\n",
        " \n",
        "   \n",
        "    device = torch.device(Config.device)\n",
        "    \n",
        "\n",
        "    try:\n",
        "        model = load_model(Config.trained_model_path, device)\n",
        "    except NameError:\n",
        "        print(\"ERROR: You must run the cell with class definitions (LogoResNet50, Config, etc.) first.\")\n",
        "        return\n",
        "\n",
        "    # Trasformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # Retrieve test files\n",
        "    test_paths = getTestPaths(Config.dataset_root)\n",
        "    if not test_paths:\n",
        "        print(\"No images found. Check Config.dataset_root\")\n",
        "        return\n",
        "\n",
        "    # Organize by Brand\n",
        "    brands_map = {}\n",
        "    for p in test_paths:\n",
        "        b = os.path.basename(os.path.dirname(p))\n",
        "        brands_map.setdefault(b, []).append(p)\n",
        "    all_brands = list(brands_map.keys())\n",
        "\n",
        "    # Search for a valid pair (Anchor + Positive)\n",
        "    max_retries = 100\n",
        "    found = False\n",
        "    \n",
        "    for _ in range(max_retries):\n",
        "        target_brand = random.choice(all_brands)\n",
        "        if len(brands_map[target_brand]) >= 2:\n",
        "            # Found a brand with at least 2 photos\n",
        "            target_imgs = random.sample(brands_map[target_brand], 2)\n",
        "            anchor_path = target_imgs[0]\n",
        "            positive_path = target_imgs[1]\n",
        "            \n",
        "            # Search for a negative (different brand)\n",
        "            while True:\n",
        "                neg_brand = random.choice(all_brands)\n",
        "                if neg_brand != target_brand and len(brands_map[neg_brand]) > 0:\n",
        "                    negative_path = random.choice(brands_map[neg_brand])\n",
        "                    distractor_name = neg_brand\n",
        "                    break\n",
        "            found = True\n",
        "            break\n",
        "    \n",
        "    if not found:\n",
        "        print(\"Could not find a brand with enough images for the demo.\")\n",
        "        return\n",
        "\n",
        "    # Compute Embeddings and Similarity\n",
        "    emb_anchor, img_anchor = get_embedding_demo(model, anchor_path, transform, device)\n",
        "    emb_pos, img_pos = get_embedding_demo(model, positive_path, transform, device)\n",
        "    emb_neg, img_neg = get_embedding_demo(model, negative_path, transform, device)\n",
        "\n",
        "\n",
        "    dist_pos = F.pairwise_distance(emb_anchor, emb_pos).item()\n",
        "    dist_neg = F.pairwise_distance(emb_anchor, emb_neg).item()\n",
        "\n",
        "    # Draw the Plot\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    \n",
        "    # Anchor Image\n",
        "    axes[0].imshow(img_anchor)\n",
        "    axes[0].set_title(f\"ANCHOR (Query)\\nBrand: {target_brand}\", fontsize=14, color='blue', fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Positive Image\n",
        "    is_match = dist_pos <= Config.prediciton_threashold\n",
        "    color_p = 'green' if is_match else 'red'\n",
        "    label_p = \"MATCH\" if is_match else \"MISSED\"\n",
        "    \n",
        "    axes[1].imshow(img_pos)\n",
        "    axes[1].set_title(f\"POSITIVE (Stesso Brand)\\nDist: {dist_pos:.4f}\\n{label_p}\", fontsize=14, color=color_p, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Negative Image\n",
        "    is_reject = dist_neg > Config.prediciton_threashold\n",
        "    color_n = 'green' if is_reject else 'red'\n",
        "    label_n = \"REJECTED\" if is_reject else \"FALSE POSITIVE\"\n",
        "    \n",
        "    axes[2].imshow(img_neg)\n",
        "    axes[2].set_title(f\"NEGATIVE (Brand: {distractor_name})\\nDist: {dist_neg:.4f}\\n{label_n}\", fontsize=14, color=color_n, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_visual_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **3D t-SNE Visualization of the Embedding Space** \n",
        "Generates an interactive 3D t-SNE visualization of the model's embeddings.\n",
        "\n",
        "This function randomly selects a subset of classes (brands) from the test set,\n",
        "extracts their feature embeddings using the provided model, and reduces\n",
        "the dimensionality to 3 components using t-SNE.\n",
        "\n",
        "The result is plotted as an interactive 3D scatter plot where clusters\n",
        "can be rotated and inspected to assess the quality of the learned metric space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def visualize_tsne_3d(model, test_paths, num_classes=10):\n",
        "    device = torch.device(Config.device)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "   \n",
        "    brands_map = {}\n",
        "    for p in test_paths:\n",
        "        b = os.path.basename(os.path.dirname(p))\n",
        "        brands_map.setdefault(b, []).append(p)\n",
        "\n",
        "\n",
        "    available_brands = list(brands_map.keys())\n",
        "    num_to_plot = min(len(available_brands), num_classes)\n",
        "   \n",
        "    selected_brands = random.sample(available_brands, num_to_plot)\n",
        "\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "\n",
        "  \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, brand in enumerate(selected_brands):\n",
        "            img_paths = brands_map[brand][:30]\n",
        "            for p in img_paths:\n",
        "                try:\n",
        "                    img = Image.open(p).convert('RGB')\n",
        "                    t_img = transform(img).unsqueeze(0).to(device)\n",
        "                    emb = F.normalize(model(t_img), p=2, dim=1).cpu().numpy()\n",
        "\n",
        "                    embeddings.append(emb[0])\n",
        "                    labels.append(brand)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {p}: {e}\")\n",
        "                    pass\n",
        "\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "\n",
        "    tsne = TSNE(n_components=3, random_state=42, perplexity=30, n_iter=1000)\n",
        "    tsne_results = tsne.fit_transform(embeddings)\n",
        "\n",
        "\n",
        "    df_tsne = pd.DataFrame({\n",
        "        'x': tsne_results[:, 0],\n",
        "        'y': tsne_results[:, 1],\n",
        "        'z': tsne_results[:, 2],\n",
        "        'Brand': labels\n",
        "    })\n",
        "\n",
        "    fig = px.scatter_3d(\n",
        "        df_tsne, x='x', y='y', z='z',\n",
        "        color='Brand', \n",
        "        title=f\"3D t-SNE Visualization of {num_to_plot} Brands\",\n",
        "        hover_name='Brand', \n",
        "        opacity=0.7,\n",
        "        size_max=10,\n",
        "        template=\"plotly_dark\" \n",
        "    )\n",
        "\n",
        "    fig.update_traces(marker=dict(size=4, line=dict(width=0)))\n",
        "\n",
        "    fig.update_layout(margin=dict(l=0, r=0, b=0, t=40))\n",
        "    \n",
        "    fig.show()\n",
        "\n",
        "\n",
        "if 'Config' in globals() and 'load_model' in globals() and 'getTestPaths' in globals():\n",
        "    try:\n",
        "    \n",
        "        if 'model' not in globals():\n",
        "             model = load_model(Config.trained_model_path, torch.device(Config.device))\n",
        "\n",
        "        test_paths = getTestPaths(Config.dataset_root)\n",
        "        \n",
        "        if test_paths:\n",
        "            visualize_tsne_3d(model, test_paths, num_classes=10) \n",
        "        else:\n",
        "            print(\"Image not found\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error during the execution: {e}\")\n",
        "else:\n",
        "    print(\"Config, load_model o getTestPaths not defined\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
