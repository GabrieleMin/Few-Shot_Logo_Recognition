{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs6umlZq-w7J"
      },
      "source": [
        "## **Deep Metric Learning Training Pipeline**\n",
        "Overview: This notebook implements a comprehensive Deep Metric Learning pipeline specifically designed for Logo Recognition. The pipeline covers every stage of the process, including data preparation, model architecture definition, loss function implementation, and a robust training loop equipped with logging and checkpointing mechanisms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRQRkyWl_Jhs"
      },
      "source": [
        "# **Configuration System Analysis (Config)**\n",
        "It separates data paths and hyperparameters from the execution logic, ensuring that the experiment is both reproducible and portable across different environments.\n",
        "1. Environment & Path Management\n",
        "This section handles the setup of the working environment to ensure smooth execution.\n",
        "* WORK_DIR: The root directory of the project.\n",
        "* dataset_root: The source location of the image dataset.\n",
        "* checkpoints_base_dir: The destination folder where model weights (.pth files) and training logs are saved.\n",
        "* device: Automatically detects and assigns hardware acceleration (cuda for GPU or cpu).\n",
        "* seed: A fixed integer (set to 42) to enforce reproducibility in data splits and weight initialization.\n",
        "2. Model Architecture\n",
        "Defines the neural network structure without modifying the model class directly.\n",
        "* backbone: The pre-trained network architecture used as the feature extractor (set to 'resnet50').\n",
        "* embedding_dim: The dimension of the output vector (set to 128). This defines the coordinates of the geometric space where images are mapped.\n",
        "* freeze_layers: An integer determining how many initial layers are locked to preserve pre-trained features (0 indicates Full Fine-tuning).\n",
        "3. Training Strategy\n",
        "Contains the general parameters governing the learning cycle.\n",
        "* loss_type: The master switch (options: 'triplet', 'euclidean', 'cosine'). This determines which Dataset class is loaded and which Loss function is instantiated.\n",
        "* epochs: The total number of training iterations over the dataset.\n",
        "* split_ratios: The percentages used for partitioning data into Training, Validation, and Test sets.\n",
        "4. Hyperparameter Dictionary (HYPERPARAMS)\n",
        "A nested dictionary containing optimized configuration recipes tailored for each specific loss function. This allows for context-aware parameter loading.\n",
        "* Triplet / Euclidean Loss: Uses margin=1.0. These loss functions work on absolute Euclidean distances.\n",
        "* Cosine Loss: Uses margin=0.2. This works on normalized angular similarities (ranging from -1 to 1).\n",
        "* Optimizer: Uses Adam with a conservative learning rate (1e-5) to preserve the quality of the pre-trained features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RScl_tLAClpL"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJFgs8erCn39",
        "outputId": "45070212-098b-4150-f941-d599fea2a1b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset in: /content/LogoDet-3K\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "from torchsummary import summary\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/LogoDet-3K-divided.zip'\n",
        "extract_path = '/content/LogoDet-3K'\n",
        "\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    if os.path.exists(zip_path):\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_path)\n",
        "        print(f\"Dataset in: {extract_path}\")\n",
        "    else:\n",
        "        print(f\"ERROR: File {zip_path} not found\")\n",
        "\n",
        "else:\n",
        "    print(\"Dataset found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "n6eqzOQ1-RuM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Config:\n",
        "    loss_type = \"cosine\"\n",
        "\n",
        "    WORK_DIR = \"/content/drive/MyDrive/LogoDet_Experiments\"\n",
        "    dataset_root = \"/content/LogoDet-3K/LogoDet-3K-divided\"\n",
        "\n",
        "    checkpoints_base_dir = os.path.join(WORK_DIR, \"runs\")\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed = 42\n",
        "\n",
        "    # Dataset Split\n",
        "    total_size = \"full\"\n",
        "    train_split_ratio = 0.8\n",
        "    val_split_ratio = 0.2\n",
        "\n",
        "    # Training Params\n",
        "    epochs = 50\n",
        "    backbone = \"resnet50\"\n",
        "    pretrained = True\n",
        "    embedding_dim = 256\n",
        "\n",
        "    optimizer = \"SGD\"\n",
        "    SGD_momentum = 0.9\n",
        "\n",
        "    initial_freeze_layers = 4\n",
        "    final_freeze_layers = 3\n",
        "    LR_reduction = 1\n",
        "    unfreeze_layers = True\n",
        "    unfreeze_at_epoch = 15\n",
        "    weight_decay = 0\n",
        "    backbone_reduction_factor = 0.25\n",
        "\n",
        "    HYPERPARAMS = {\n",
        "        \"euclidean\": {\n",
        "            \"batch_size\": 32,\n",
        "            \"margin\": 1.0,\n",
        "            \"learning_rate\": 1e-3,\n",
        "            \"folder_name\": \"contrastive_euclidean\",\n",
        "            \"description\": \"Contrastive Loss\"\n",
        "        },\n",
        "        \"cosine\": {\n",
        "            \"batch_size\": 256,\n",
        "            \"margin\": 0.2,\n",
        "            \"learning_rate\": 1e-2,\n",
        "            \"folder_name\": \"contrastive_cosine\",\n",
        "            \"description\": \"Contrastive Loss\"\n",
        "        },\n",
        "        \"triplet\": {\n",
        "            \"batch_size\": 256,\n",
        "            \"margin\": 1.0,\n",
        "            \"learning_rate\": 1e-1,\n",
        "            \"folder_name\": \"triplet_loss\",\n",
        "            \"description\": \"Triplet Loss\"\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N8nRlT2_5eb"
      },
      "source": [
        "**Data Loading and Processing logic**\n",
        "\n",
        "It defines how images are loaded, paired, or grouped into triplets to train a neural network using PyTorch.\n",
        "\n",
        "Key components:\n",
        "\n",
        "1. DatasetTriplet Class\n",
        "\n",
        "      This class is designed for training with Triplet Loss.\n",
        "\n",
        "      Goal: To provide the model with three images at once:\n",
        "\n",
        "      Anchor: The reference image.\n",
        "\n",
        "      Positive: A different image of the same brand.\n",
        "\n",
        "      Negative: An image of a different brand.\n",
        "\n",
        "      Mechanism:\n",
        "\n",
        "      It parses the file path to extract the label (Brand Name).\n",
        "\n",
        "      It maintains a dictionary (label_to_indices) to know which images belong to which brand.\n",
        "\n",
        "      In __getitem__, it randomly samples a Positive and a Negative to form a valid triplet.\n",
        "\n",
        "      Safety Features: It includes fallback logic (e.g., if a brand has only one image, the Positive becomes the Anchor itself; it retries up to 50 times to find a valid Negative).\n",
        "\n",
        "2. DatasetContrastive Class\n",
        "\n",
        "      This class is designed for training with Contrastive Loss (Siamese Networks).\n",
        "\n",
        "      Goal: To provide the model with a Pair of images and a binary label.\n",
        "\n",
        "      Mechanism:\n",
        "\n",
        "      It selects a first image (img1).\n",
        "\n",
        "      It flips a coin (50% chance) to decide if the second image (img2) should be the Same Brand (Positive pair) or a Different Brand (Negative pair).\n",
        "\n",
        "      Output: Returns (img1, img2, label), where label is 1 for similar and 0 for different (or vice versa depending on loss implementation).\n",
        "\n",
        "3. getTrainValPaths Function\n",
        "\n",
        "      This utility splits the dataset ensuring brand disjointness. Unlike standard random splits, it groups images by brand first,\n",
        "      \n",
        "      then assigns entire brands to either the Training \n",
        "\n",
        "      or Validation set. This prevents \"data leakage\" and simulates a realistic Few-Shot scenario where the model must recognize \n",
        "      \n",
        "      similarity between logos it has never seen before \n",
        "\n",
        "      during validation.\n",
        "\n",
        "4. Technical Details\n",
        "\n",
        "      Reproducibility: It sets a fixed SEED = 101 for random and torch to ensure that data splits and pairings are consistent every time you run the code.\n",
        "\n",
        "      Cross-Platform Compatibility: It uses .replace('\\\\', '/') to ensure file paths work correctly on both Windows and Linux/Colab.\n",
        "\n",
        "      Robustness: Both dataset classes have try-except blocks in load_image to handle corrupted images or path errors gracefully (returning a black image instead of crashing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rcE9mUlCA3Nd"
      },
      "outputs": [],
      "source": [
        "\n",
        "SEED = 101\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "class DatasetTriplet(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "        self.label_to_indices = defaultdict(list)\n",
        "        for idx, img_path in enumerate(self.file_list):\n",
        "            label = img_path.replace('\\\\', '/').split('/')[-2]\n",
        "            self.label_to_indices[label].append(idx)\n",
        "\n",
        "\n",
        "        self.labels = list(self.label_to_indices.keys())\n",
        "\n",
        "        print(f\"[DatasetTriplet] Unique brands found: {len(self.labels)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img_transformed = self.transform(img)\n",
        "        else:\n",
        "            img_transformed = transforms.ToTensor()(img)\n",
        "\n",
        "\n",
        "        return img_transformed\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. Anchor\n",
        "        anchor_img_path = self.file_list[idx]\n",
        "        anchor_label = anchor_img_path.replace('\\\\', '/').split('/')[-2]\n",
        "        anchor = self.load_image(anchor_img_path)\n",
        "\n",
        "        # 2. Positive\n",
        "        positive_indices = [i for i in self.label_to_indices[anchor_label] if i != idx]\n",
        "        if len(positive_indices) > 0:\n",
        "            positive_idx = random.choice(positive_indices)\n",
        "            pos_path = self.file_list[positive_idx]\n",
        "        else:\n",
        "            pos_path = anchor_img_path\n",
        "        positive = self.load_image(pos_path)\n",
        "\n",
        "        # 3. Negative\n",
        "        while True:\n",
        "            neg_label = random.choice(self.labels)\n",
        "            if neg_label != anchor_label:\n",
        "                break\n",
        "        negative_indices = [i for i in self.label_to_indices[neg_label] if i != idx]\n",
        "        negative_idx = random.choice(negative_indices)\n",
        "        neg_path = self.file_list[negative_idx]\n",
        "        negative = self.load_image(neg_path)\n",
        "\n",
        "        return anchor, positive, negative\n",
        "\n",
        "\n",
        "class DatasetContrastive(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.label_to_indices = defaultdict(list)\n",
        "        for idx, img_path in enumerate(self.file_list):\n",
        "            label = img_path.replace('\\\\', '/').split('/')[-2]\n",
        "            self.label_to_indices[label].append(idx)\n",
        "\n",
        "\n",
        "        self.labels = list(self.label_to_indices.keys())\n",
        "\n",
        "    def __len__(self): return len(self.file_list)\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        else: img = transforms.ToTensor()(img)\n",
        "        return {\"image\": img}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        label = img_path.replace('\\\\', '/').split('/')[-2]\n",
        "        img1 = self.load_image(img_path)\n",
        "\n",
        "        is_positive_pair = random.choice([0, 1])\n",
        "\n",
        "        if len(self.labels) < 2:\n",
        "            is_positive_pair = 1\n",
        "\n",
        "        if is_positive_pair:\n",
        "            pos_indices = [i for i in self.label_to_indices[label] if i != idx]\n",
        "            if len(pos_indices) == 0:\n",
        "                print(\"ERROR LOADING A POSITIVE MATCH FOR THE LOADED IMAGE\")\n",
        "                exit()\n",
        "            else:\n",
        "                idx2 = random.choice(pos_indices)\n",
        "                path2 = self.file_list[idx2]\n",
        "        else:\n",
        "            neg_label = random.choice([l for l in self.label_to_indices.keys() if l != label])\n",
        "            idx2 = random.choice(self.label_to_indices[neg_label])\n",
        "            path2 = self.file_list[idx2]\n",
        "\n",
        "        img2 = self.load_image(path2)\n",
        "        return img1, img2, torch.tensor(is_positive_pair, dtype=torch.float32)\n",
        "\n",
        "# ==========================================\n",
        "# UTILS\n",
        "# ==========================================\n",
        "class DatasetTest(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "    def __len__(self): return len(self.file_list)\n",
        "    def load_image(self, image_path):\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        if self.transform: img = self.transform(img)\n",
        "        return {\"image\": img, \"label\": -1}\n",
        "    def __getitem__(self, idx): return self.load_image(self.file_list[idx])\n",
        "\n",
        "def getTrainValPaths(root_dir, val_split, total_set_size=None, min_images_per_brand=2):\n",
        "    train_val_path = os.path.join(root_dir, 'train_val')\n",
        "    train_val_brands = []\n",
        "\n",
        "    # Collect brand folders\n",
        "    if not os.path.exists(train_val_path):\n",
        "        print(f\"Warning: {train_val_path} not found.\")\n",
        "        return [], []\n",
        "\n",
        "    for category in os.listdir(train_val_path):\n",
        "        cat_path = os.path.join(train_val_path, category)\n",
        "        if os.path.isdir(cat_path):\n",
        "            for brand in os.listdir(cat_path):\n",
        "                brand_full_path = os.path.join(cat_path, brand)\n",
        "                if os.path.isdir(brand_full_path):\n",
        "                    train_val_brands.append(brand_full_path)\n",
        "\n",
        "    # Split brands into Train and Val\n",
        "    val_size = int(len(train_val_brands) * val_split)\n",
        "    train_size = len(train_val_brands) - val_size\n",
        "    generator = torch.Generator().manual_seed(Config.seed)\n",
        "    train_subset, val_subset = random_split(train_val_brands, [train_size, val_size], generator=generator)\n",
        "\n",
        "    train_brand_list = [train_val_brands[i] for i in train_subset.indices]\n",
        "    val_brand_list = [train_val_brands[i] for i in val_subset.indices]\n",
        "\n",
        "    train_data_list = []\n",
        "    val_data_list = []\n",
        "\n",
        "    # Sampling Logic\n",
        "    if total_set_size is not None:\n",
        "        images_per_brand = round(total_set_size / len(train_val_brands))\n",
        "\n",
        "        if images_per_brand < min_images_per_brand:\n",
        "            print(f\"Not enough images per brand ({images_per_brand}), downscaling brand sets to ensure {min_images_per_brand} images/brand.\")\n",
        "\n",
        "            # Calculate how many brands we can actually afford\n",
        "            new_total_brand_count = round(total_set_size / min_images_per_brand)\n",
        "            new_val_size = round(new_total_brand_count * val_split)\n",
        "            new_train_size = new_total_brand_count - new_val_size\n",
        "\n",
        "            train_brand_list = random.sample(train_brand_list, min(len(train_brand_list), new_train_size))\n",
        "            val_brand_list = random.sample(val_brand_list, min(len(val_brand_list), new_val_size))\n",
        "            images_per_brand = min_images_per_brand\n",
        "\n",
        "        for brand in train_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the TRAIN set\")\n",
        "\n",
        "            train_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "\n",
        "        for brand in val_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the VALIDATION set\")\n",
        "\n",
        "            val_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "    else:\n",
        "        for brand in train_brand_list:\n",
        "            train_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "        for brand in val_brand_list:\n",
        "            val_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "\n",
        "    return train_data_list, val_data_list\n",
        "\n",
        "def show_contrastive_with_bboxes(img1, img2): pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSFjRdVp_sTK"
      },
      "source": [
        "ResNet50-based architecture modified for Metric Learning (generating embeddings).\n",
        "\n",
        "Key operations:\n",
        "\n",
        "1.\tBackbone Initialization: Loads a standard ResNet50 (optionally with ImageNet weights).\n",
        "2.\tHead Replacement: Swaps the original 1000-class classifier with a linear projection layer to output embeddings of size embedding_dim.\n",
        "3.\tProgressive Freezing: Implements a custom freeze_numer_of_layer method to selectively freeze backbone blocks (from shallow 'conv1' to deep 'layer4') for controlled fine-tuning.\n",
        "4.  Differential Learning Rates (build_optimizer): Splits the network into distinct parameter groups to apply varying learning rates. The pre-trained backbone receives a reduced learning rate (scaled down by backbone_reduction_factor) to preserve generic visual features, while the newly initialized embedding head receives the full base learning rate to adapt quickly. It dynamically supports both SGD and Adam optimizers based on configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oCUWRNlW_0Cn"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LogoResNet50(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True, num_of_freeze_layer=5, activation_fn=None):\n",
        "        super(LogoResNet50, self).__init__()\n",
        "\n",
        "        # 1. Load Pre-trained Weights\n",
        "        # Initialize the model with weights pretrained on ImageNet for transfer learning\n",
        "        if pretrained:\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "            self.model = models.resnet50(weights=weights)\n",
        "        else:\n",
        "            self.model = models.resnet50(weights=None)\n",
        "\n",
        "        # 2. Modify the Head (Fully Connected Layer)\n",
        "        # We need to produce feature embeddings instead of class probabilities\n",
        "        input_features_fc = self.model.fc.in_features # Typically 2048 for ResNet50\n",
        "\n",
        "        head_layers = []\n",
        "        # Project features to the desired embedding dimension (e.g., 128)\n",
        "        head_layers.append(nn.Linear(input_features_fc, 1024))\n",
        "        head_layers.append(nn.ReLU())\n",
        "        head_layers.append(nn.Linear(1024, embedding_dim))\n",
        "\n",
        "        # Add an optional activation function if provided\n",
        "        if activation_fn is not None:\n",
        "            head_layers.append(activation_fn)\n",
        "\n",
        "        # Replace the original classifier with our custom embedding head\n",
        "        self.model.fc = nn.Sequential(*head_layers)\n",
        "\n",
        "        # 3. Freezing Management\n",
        "        # Define the blocks here to access them in the freeze method.\n",
        "        # This structure allows progressive freezing/unfreezing strategies\n",
        "        self.blocks = [\n",
        "            ['conv1', 'bn1'],   # Level 1\n",
        "            ['layer1'],         # Level 2\n",
        "            ['layer2'],         # Level 3\n",
        "            ['layer3'],         # Level 4\n",
        "            ['layer4'],         # Level 5: Entire backbone frozen\n",
        "        ]\n",
        "\n",
        "        # Apply the initial freezing configuration\n",
        "        self.freeze_numer_of_layer(num_of_freeze_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def freeze_numer_of_layer(self, num_of_freeze_layer):\n",
        "        \"\"\"\n",
        "        Manages layer freezing for transfer learning strategies.\n",
        "\n",
        "        Args:\n",
        "            num_of_freeze_layer (int):\n",
        "              0   -> All layers unlocked (Full Fine-Tuning)\n",
        "              1-5 -> Progressively freezes the backbone layers from shallow to deep\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: RESET. Unfreeze everything (requires_grad = True).\n",
        "        # This ensures we start from a clean state before applying new constraints.\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # If num is 0, exit immediately (Full Fine-Tuning mode)\n",
        "        if num_of_freeze_layer == 0:\n",
        "            print(\"Configuration: Full Fine-Tuning (All layers are trainable)\")\n",
        "            return\n",
        "\n",
        "        # Safety check to avoid index out of bounds\n",
        "        limit = min(num_of_freeze_layer, len(self.blocks))\n",
        "\n",
        "        frozen_list = []\n",
        "\n",
        "        # STEP 2: Progressively freeze the requested blocks\n",
        "        for i in range(limit):\n",
        "            current_blocks = self.blocks[i]\n",
        "            for block_name in current_blocks:\n",
        "                # Retrieve the layer by name\n",
        "                layer = getattr(self.model, block_name)\n",
        "\n",
        "                # Freeze parameters for this specific block\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                frozen_list.append(block_name)\n",
        "\n",
        "        print(f\"Freezing Level {limit}. Frozen blocks: {frozen_list}\")\n",
        "\n",
        "\n",
        "def build_optimizer(model, base_lr):\n",
        "    backbone_params = []\n",
        "    head_params = []\n",
        "\n",
        "    for name, param in model.model.named_parameters():\n",
        "        if not param.requires_grad:\n",
        "            continue\n",
        "        if name.startswith(\"fc.\"):\n",
        "            head_params.append(param)\n",
        "        else:\n",
        "            backbone_params.append(param)\n",
        "\n",
        "    param_groups = [\n",
        "        {\"params\": backbone_params, \"lr\": base_lr * Config.backbone_reduction_factor},\n",
        "        {\"params\": head_params,     \"lr\": base_lr},\n",
        "    ]\n",
        "\n",
        "    weight_decay = Config.weight_decay\n",
        "\n",
        "    if Config.optimizer == \"Adam\":\n",
        "        optimizer = optim.Adam(\n",
        "            param_groups,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "    else:\n",
        "        optimizer = optim.SGD(\n",
        "            param_groups,\n",
        "            momentum=Config.SGD_momentum,\n",
        "            weight_decay=weight_decay\n",
        "        )\n",
        "\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRUsiukq_jwj"
      },
      "source": [
        "**Loss Function** definitions used to train the LogoResNet50 model. These classes define the mathematical rules that teach the neural network how to distinguish between similar and dissimilar logo images in a vector space.\n",
        "Here is a breakdown of the three classes defined in the script:\n",
        "1. ContrastiveLossEuclidean\n",
        "This class implements the classic Contrastive Loss based on Euclidean distance.\n",
        "* Function: It creates a \"Siamese\" objective. It pulls pairs of images belonging to the same class (Label 1) closer together and pushes pairs from different classes (Label 0) apart.\n",
        "* Mechanism:\n",
        "    * If images are Similar: It minimizes the squared Euclidean distance between their embeddings.\n",
        "    * If images are Different: It penalizes the model if the distance is smaller than the defined margin (default 2.0).\n",
        "2. ContrastiveLossCosine\n",
        "This class implements Cosine Embedding Loss.\n",
        "* Function: Instead of measuring the straight-line distance (Euclidean), this measures the angle between the two feature vectors. This is often more effective for high-dimensional spaces where the magnitude of the vector matters less than its direction.\n",
        "* Input Requirements: It expects labels where 1 represents similar pairs and -1 represents dissimilar pairs.\n",
        "3. TripletLoss\n",
        "This class implements the standard Triplet Margin Loss.\n",
        "* Function: It uses a three-part input structure rather than pairs:\n",
        "    * Anchor: The reference image.\n",
        "    * Positive: An image of the same class as the Anchor.\n",
        "    * Negative: An image of a different class.\n",
        "* Goal: It forces the distance between the Anchor and the Positive to be smaller than the distance between the Anchor and the Negative by at least the specified margin (default 1.0). This is generally considered more robust than Contrastive Loss for ranking tasks like logo retrieval.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jaCoeoAn_c-D"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLossEuclidean(nn.Module):\n",
        "    def __init__(self, margin=2.0):\n",
        "        super(ContrastiveLossEuclidean, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        # Label 1 = Similar | Label 0 = Dissimilar\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim=True)\n",
        "        loss_contrastive = torch.mean(\n",
        "            (label) * torch.pow(euclidean_distance, 2) +\n",
        "            (1-label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n",
        "        )\n",
        "        return loss_contrastive\n",
        "\n",
        "class ContrastiveLossCosine(nn.Module):\n",
        "    def __init__(self, margin=0.2):\n",
        "        super(ContrastiveLossCosine, self).__init__()\n",
        "        self.loss_fn = nn.CosineEmbeddingLoss(margin=margin)\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        return self.loss_fn(output1, output2, label)\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Triplet Margin Loss Standard.\n",
        "    Input: Anchor, Positive, Negative.\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.loss_fn = nn.TripletMarginLoss(margin=margin, p=2)\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        return self.loss_fn(anchor, positive, negative)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gzUkvO6A6CC"
      },
      "source": [
        "# **Training script**\n",
        "It coordinates all core components—configuration, dataset handling, model architecture, and loss functions—to execute the full training workflow.\n",
        "The script:\n",
        "* Sets up the execution environment and imports all required custom and PyTorch modules.\n",
        "* Loads training settings from a configuration class and dynamically selects hyperparameters based on the chosen loss function (Triplet, Euclidean, or Cosine).\n",
        "* Splits the dataset into training and validation sets and applies data augmentation for better generalization.\n",
        "* Initializes a ResNet-50–based embedding model and the corresponding metric learning loss.\n",
        "* Runs the training and validation loop for multiple epochs, performing forward passes, loss computation, backpropagation, and optimization.\n",
        "* Saves model checkpoints, logs training history to a CSV file, and generates loss curve plots to monitor convergence (Loss Curves, F1 Score Curve, Threshold Adaptation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8TKhcyC_FsE",
        "outputId": "59f22ddd-f73e-4626-d546-1b435307fb0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "starting execution\n",
            "\n",
            "--------------------------------------------------\n",
            "STARTING TRAINING: COSINE\n",
            "Batch Size: 256 | Margin: 0.2 | LR: 0.01\n",
            "Output Folder: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine\n",
            "--------------------------------------------------\n",
            "\n",
            "Loading dataset paths...\n",
            "train_files length: 108171\n",
            "val_files length: 27528\n",
            "Using DatasetContrastive (Pairs)\n",
            "Initializing LogoResNet50...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 243MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freezing Level 4. Frozen blocks: ['conv1', 'bn1', 'layer1', 'layer2', 'layer3']\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
            "             ReLU-81          [-1, 256, 28, 28]               0\n",
            "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
            "             ReLU-84          [-1, 256, 14, 14]               0\n",
            "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
            "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
            "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-89         [-1, 1024, 14, 14]               0\n",
            "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
            "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
            "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
            "             ReLU-93          [-1, 256, 14, 14]               0\n",
            "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
            "             ReLU-96          [-1, 256, 14, 14]               0\n",
            "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
            "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
            "             ReLU-99         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
            "            ReLU-103          [-1, 256, 14, 14]               0\n",
            "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
            "            ReLU-106          [-1, 256, 14, 14]               0\n",
            "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-109         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
            "            ReLU-113          [-1, 256, 14, 14]               0\n",
            "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
            "            ReLU-116          [-1, 256, 14, 14]               0\n",
            "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-119         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
            "            ReLU-123          [-1, 256, 14, 14]               0\n",
            "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
            "            ReLU-126          [-1, 256, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-143          [-1, 512, 14, 14]               0\n",
            "          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-146            [-1, 512, 7, 7]               0\n",
            "          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-151           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-152           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-155            [-1, 512, 7, 7]               0\n",
            "          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-158            [-1, 512, 7, 7]               0\n",
            "          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-161           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-162           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-165            [-1, 512, 7, 7]               0\n",
            "          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-168            [-1, 512, 7, 7]               0\n",
            "          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-171           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-172           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n",
            "          Linear-174                 [-1, 1024]       2,098,176\n",
            "            ReLU-175                 [-1, 1024]               0\n",
            "          Linear-176                  [-1, 256]         262,400\n",
            "          ResNet-177                  [-1, 256]               0\n",
            "================================================================\n",
            "Total params: 25,868,608\n",
            "Trainable params: 17,325,312\n",
            "Non-trainable params: 8,543,296\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 286.57\n",
            "Params size (MB): 98.68\n",
            "Estimated Total Size (MB): 385.83\n",
            "----------------------------------------------------------------\n",
            "Existing CSV found, loading historical data for charts\n",
            "Resuming from epoch 43\n",
            "Freezing Level 3. Frozen blocks: ['conv1', 'bn1', 'layer1', 'layer2']\n",
            "Loading weights from: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_42.pth\n",
            "Weights loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43/50: 100%|██████████| 423/423 [10:38<00:00,  1.51s/it, loss=0.1580]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43 DONE. Train Loss: 0.1870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 43: Loss = 0.2014 | F1 Score = 0.8103\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_43.pth\n",
            "CSV updated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44/50: 100%|██████████| 423/423 [10:36<00:00,  1.51s/it, loss=0.1640]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44 DONE. Train Loss: 0.1864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 44: Loss = 0.2045 | F1 Score = 0.8079\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_44.pth\n",
            "CSV updated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45/50: 100%|██████████| 423/423 [10:35<00:00,  1.50s/it, loss=0.1972]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45 DONE. Train Loss: 0.1840\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 45: Loss = 0.2025 | F1 Score = 0.8096\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_45.pth\n",
            "CSV updated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46/50: 100%|██████████| 423/423 [10:35<00:00,  1.50s/it, loss=0.1790]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46 DONE. Train Loss: 0.1833\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 46: Loss = 0.2046 | F1 Score = 0.8047\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_46.pth\n",
            "CSV updated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47/50: 100%|██████████| 423/423 [10:36<00:00,  1.51s/it, loss=0.1474]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47 DONE. Train Loss: 0.1813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 47: Loss = 0.2024 | F1 Score = 0.8054\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_47.pth\n",
            "CSV updated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48/50: 100%|██████████| 423/423 [10:36<00:00,  1.51s/it, loss=0.1822]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48 DONE. Train Loss: 0.1813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 48: Loss = 0.1997 | F1 Score = 0.8117\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_48.pth\n",
            "CSV updated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49/50: 100%|██████████| 423/423 [10:36<00:00,  1.51s/it, loss=0.2018]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49 DONE. Train Loss: 0.1794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 49: Loss = 0.2039 | F1 Score = 0.8049\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_49.pth\n",
            "CSV updated.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50/50: 100%|██████████| 423/423 [10:36<00:00,  1.51s/it, loss=0.1566]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50 DONE. Train Loss: 0.1773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VALIDATION Epoch 50: Loss = 0.2040 | F1 Score = 0.8082\n",
            "------------------------------\n",
            "Checkpoint saved: /content/drive/MyDrive/LogoDet_Experiments/runs/contrastive_cosine/model_epoch_50.pth\n",
            "CSV updated.\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F  \n",
        "def save_plots(model, optimizer, train_losses, val_losses, val_f1_scores, val_thresholds, output_dir, train_transform, title_suffix=\"\", params=None, unfreeze_epoch=None):\n",
        "    fig = plt.figure(figsize=(20, 8))\n",
        "    gs = fig.add_gridspec(2, 3, height_ratios=[1, 1])\n",
        "\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    ax3 = fig.add_subplot(gs[0, 2])\n",
        "\n",
        "    # --- Ax1: Loss ---\n",
        "    ax1.set_title(f\"Loss Curve {title_suffix}\")\n",
        "    ax1.plot(train_losses, label=\"Train Loss\", color=\"blue\")\n",
        "    ax1.plot(val_losses, label=\"Val Loss\", color=\"red\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Ax2: F1 Score ---\n",
        "    ax2.set_title(f\"F1 Score Curve {title_suffix}\")\n",
        "    ax2.plot(val_f1_scores, label=\"Val F1 Score\", color=\"green\")\n",
        "    ax2.set_xlabel(\"Epochs\")\n",
        "    ax2.set_ylabel(\"F1 Score\")\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    # --- Ax3: Threshold Evolution ---\n",
        "    ax3.set_title(f\"Threshold Evolution {title_suffix}\")\n",
        "    ax3.plot(val_thresholds, label=\"Best Threshold\", color=\"purple\", linestyle='-')\n",
        "    ax3.set_xlabel(\"Epochs\")\n",
        "    ax3.set_ylabel(\"Distance Value\")\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "\n",
        "    if params:\n",
        "\n",
        "        freeze_text = f\"Freeze: {Config.initial_freeze_layers} (Fixed)\"\n",
        "\n",
        "        lr_text = f\"LR={params['learning_rate']}\"\n",
        "\n",
        "\n",
        "        if Config.unfreeze_layers and unfreeze_epoch and len(train_losses) > unfreeze_epoch:\n",
        "             freeze_text = f\"Freeze: {Config.initial_freeze_layers} -> {Config.final_freeze_layers} (at ep {unfreeze_epoch})\"\n",
        "\n",
        "\n",
        "             current_lr = params['learning_rate']\n",
        "             prev_lr = current_lr * (1/Config.LR_reduction)\n",
        "             lr_text = f\"LR: {prev_lr:.1e} -> {current_lr:.1e}\"\n",
        "\n",
        "        info_text = (f\"Config: Batch={params['batch_size']} | \"\n",
        "                     f\"With Normalization on output | \"\n",
        "                     f\"With lower backbone LR by a factor of {Config.backbone_reduction_factor} | \"\n",
        "                     f\"Optimizer= {Config.optimizer} | \"\n",
        "                     f\"weight decay: {Config.weight_decay} | \"\n",
        "                     f\"Margin={params['margin']} | \"\n",
        "                     f\"{lr_text} | \\n\"\n",
        "                     f\"total dataset size: {Config.total_size} | \\n\"\n",
        "                     f\"train transformations: {train_transform} | \\n\"\n",
        "                     f\"Emb={model.model.fc}\\n\"\n",
        "                     f\"{freeze_text}\\n\"\n",
        "                     f\"LR decrease factoir: {Config.LR_reduction}\")\n",
        "\n",
        "        ax_text = fig.add_subplot(gs[1, :])\n",
        "        ax_text.axis('off')\n",
        "\n",
        "        ax_text.text(0.5, 0.95, info_text, ha=\"center\", va=\"top\", fontsize=10,\n",
        "                     wrap=True, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":10})\n",
        "\n",
        "    # Add unfreeze lines to all plots\n",
        "    for ax in [ax1, ax2, ax3]:\n",
        "        if Config.unfreeze_layers and unfreeze_epoch and len(train_losses) > unfreeze_epoch:\n",
        "            ax.axvline(x=unfreeze_epoch, color='gray', linestyle='--', alpha=0.8, label='Unfreeze')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(output_dir, \"training_plot.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "\n",
        "def calculate_f1(tp, tn, fp, fn):\n",
        "    denominator = (2 * tp) + fp + fn\n",
        "    if denominator == 0:\n",
        "        return 0.0\n",
        "\n",
        "    f1_score = (2 * tp) / denominator\n",
        "\n",
        "    return f1_score\n",
        "\n",
        "# =========================================================================\n",
        "# MAIN\n",
        "# =========================================================================\n",
        "def main():\n",
        "    print(\"starting execution\")\n",
        "    loss_type = Config.loss_type\n",
        "\n",
        "    if hasattr(Config, 'HYPERPARAMS'):\n",
        "        params = Config.HYPERPARAMS[loss_type]\n",
        "        BATCH_SIZE = params['batch_size']\n",
        "        MARGIN = params['margin']\n",
        "        LR = params['learning_rate']\n",
        "        SAVE_FOLDER = params['folder_name']\n",
        "    else:\n",
        "        BATCH_SIZE = 32\n",
        "        MARGIN = 1.0\n",
        "        LR = 1e-4\n",
        "        SAVE_FOLDER = f\"{loss_type}_run\"\n",
        "\n",
        "    device = torch.device(Config.device)\n",
        "    save_dir = os.path.join(Config.checkpoints_base_dir, SAVE_FOLDER)\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    print(f\"\\n{'-'*50}\")\n",
        "    print(f\"STARTING TRAINING: {loss_type.upper()}\")\n",
        "    print(f\"Batch Size: {BATCH_SIZE} | Margin: {MARGIN} | LR: {LR}\")\n",
        "    print(f\"Output Folder: {save_dir}\")\n",
        "    print(f\"{'-'*50}\\n\")\n",
        "\n",
        "    # --- DATASET ---\n",
        "    print(\"Loading dataset paths...\")\n",
        "\n",
        "    train_files, val_files = getTrainValPaths(\n",
        "        Config.dataset_root,\n",
        "        val_split=Config.val_split_ratio,\n",
        "        min_images_per_brand=5\n",
        "    )\n",
        "    print(f\"train_files length: {len(train_files)}\")\n",
        "    print(f\"val_files length: {len(val_files)}\")\n",
        "\n",
        "    # Transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomPerspective(p=0.3),\n",
        "        transforms.RandomHorizontalFlip(p=0.3),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    if loss_type == \"triplet\":\n",
        "        print(\"Using DatasetTriplet\")\n",
        "        train_dataset = DatasetTriplet(train_files, transform=train_transform)\n",
        "        val_dataset = DatasetTriplet(val_files, transform=val_transform)\n",
        "    else:\n",
        "        print(\"Using DatasetContrastive (Pairs)\")\n",
        "        train_dataset = DatasetContrastive(train_files, transform=train_transform)\n",
        "        val_dataset = DatasetContrastive(val_files, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=True, persistent_workers=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "    # --- MODEL & LOSS ---\n",
        "    print(\"Initializing LogoResNet50...\")\n",
        "    model = LogoResNet50(embedding_dim=Config.embedding_dim, pretrained=Config.pretrained, num_of_freeze_layer=Config.initial_freeze_layers)\n",
        "    model = model.to(device)\n",
        "    summary(model, input_size=(3, 224, 224))\n",
        "\n",
        "    if loss_type == \"euclidean\":\n",
        "        criterion = ContrastiveLossEuclidean(margin=MARGIN)\n",
        "    elif loss_type == \"cosine\":\n",
        "        criterion = ContrastiveLossCosine(margin=MARGIN)\n",
        "    elif loss_type == \"triplet\":\n",
        "        criterion = TripletLoss(margin=MARGIN)\n",
        "    else:\n",
        "        raise ValueError(f\"Loss type {loss_type} not supported\")\n",
        "\n",
        "    optimizer = build_optimizer(model, LR)\n",
        "\n",
        "    # --- SETUP CSV E HISTORY ---\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_f1_scores = []\n",
        "    val_thresholds = []\n",
        "    csv_path = os.path.join(save_dir, \"training_history.csv\")\n",
        "    # F1 values\n",
        "    val_tp = 0\n",
        "    val_tn = 0\n",
        "    val_fp = 0\n",
        "    val_fn = 0\n",
        "\n",
        "    start_epoch = 0\n",
        "    if os.path.exists(csv_path):\n",
        "        print(\"Existing CSV found, loading historical data for charts\")\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            train_losses = df['Train Loss'].tolist()\n",
        "            val_losses = df['Val Loss'].tolist()\n",
        "            if 'Val F1 Score' in df.columns:\n",
        "                 val_f1_scores = df['Val F1 Score'].tolist()\n",
        "            if 'Val threshold' in df.columns:\n",
        "                 val_thresholds = df['Val threshold'].tolist()\n",
        "            start_epoch = len(train_losses)\n",
        "            print(f\"Resuming from epoch {start_epoch + 1}\")\n",
        "            if (Config.unfreeze_layers and start_epoch > Config.unfreeze_at_epoch):\n",
        "                model.freeze_numer_of_layer(Config.final_freeze_layers)\n",
        "                LR = LR * Config.LR_reduction\n",
        "                optimizer = build_optimizer(model, LR)\n",
        "        except:\n",
        "            print(\"Error reading CSV\")\n",
        "    else:\n",
        "        with open(csv_path, mode='w', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([\"Epoch\", \"Train Loss\", \"Val Loss\", \"Val F1 Score\", \"Val threshold\"])\n",
        "\n",
        "    # Resume (if the file for the current epoch exists)\n",
        "    resume_ckpt = os.path.join(save_dir, f\"model_epoch_{start_epoch}.pth\")\n",
        "    if os.path.exists(resume_ckpt) and start_epoch > 0:\n",
        "        print(f\"Loading weights from: {resume_ckpt}\")\n",
        "        model.load_state_dict(torch.load(resume_ckpt, map_location=device))\n",
        "        print(\"Weights loaded\")\n",
        "\n",
        "    try: from tqdm import tqdm\n",
        "    except ImportError: tqdm = lambda iterator, desc=\"\": iterator\n",
        "\n",
        "    # --- TRAINING LOOP ---\n",
        "    for epoch in range(start_epoch, Config.epochs):\n",
        "        if Config.unfreeze_layers and Config.unfreeze_at_epoch > 0 and epoch == Config.unfreeze_at_epoch:\n",
        "                print(f\"\\nUNFREEZING LAYERS AT EPOCH {epoch}\")\n",
        "\n",
        "\n",
        "                model.freeze_numer_of_layer(Config.final_freeze_layers)\n",
        "                summary(model, input_size=(3, 224, 224))\n",
        "\n",
        "\n",
        "                LR = LR * Config.LR_reduction\n",
        "                print(f\"Learning Rate reduced to {LR}\")\n",
        "\n",
        "\n",
        "                optimizer = build_optimizer(model, LR)\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.epochs}\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if loss_type == \"triplet\":\n",
        "                anc, pos, neg = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "                emb_a = F.normalize(model(anc), p=2, dim=1)\n",
        "                emb_p = F.normalize(model(pos), p=2, dim=1)\n",
        "                emb_n = F.normalize(model(neg), p=2, dim=1)\n",
        "\n",
        "                loss = criterion(emb_a, emb_p, emb_n)\n",
        "            else:\n",
        "                img1 = batch[0]['image'].to(device)\n",
        "                img2 = batch[1]['image'].to(device)\n",
        "                label = batch[2].to(device)\n",
        "\n",
        "                # out1 = model(img1)\n",
        "                # out2 = model(img2)\n",
        "                out1 = F.normalize(model(img1), p=2, dim=1)\n",
        "                out2 = F.normalize(model(img2), p=2, dim=1)\n",
        "\n",
        "\n",
        "                if loss_type == \"euclidean\":\n",
        "                    target = label.float()\n",
        "                else:\n",
        "                    target = label.float()\n",
        "                    target[target == 0] = -1\n",
        "\n",
        "                loss = criterion(out1, out2, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if hasattr(pbar, \"set_postfix\"): pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1} DONE. Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # --- VALIDATION ---\n",
        "        model.eval()\n",
        "        val_loss_acc = 0.0\n",
        "        val_tp, val_tn, val_fp, val_fn = 0, 0, 0, 0\n",
        "        threshold = MARGIN\n",
        "\n",
        "        all_distances = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                if loss_type == \"triplet\":\n",
        "                    anc, pos, neg = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "                    emb_a = F.normalize(model(anc), p=2, dim=1)\n",
        "                    emb_p = F.normalize(model(pos), p=2, dim=1)\n",
        "                    emb_n = F.normalize(model(neg), p=2, dim=1)\n",
        "                    loss = criterion(emb_a, emb_p, emb_n)\n",
        "\n",
        "                    dist_pos = F.pairwise_distance(emb_a, emb_p).cpu().tolist()\n",
        "                    dist_neg = F.pairwise_distance(emb_a, emb_n).cpu().tolist()\n",
        "\n",
        "                    # Accumulate for optimal threshold search\n",
        "                    all_distances.extend(dist_pos)\n",
        "                    all_labels.extend([1] * len(dist_pos))\n",
        "                    all_distances.extend(dist_neg)\n",
        "                    all_labels.extend([0] * len(dist_neg))\n",
        "                else:\n",
        "                    img1, img2, label = batch[0]['image'].to(device), batch[1]['image'].to(device), batch[2].to(device)\n",
        "                    # out1 = model(img1)\n",
        "                    # out2 = model(img2)\n",
        "                    out1 = F.normalize(model(img1), p=2, dim=1)\n",
        "                    out2 = F.normalize(model(img2), p=2, dim=1)\n",
        "\n",
        "                    target = label.float().clone()\n",
        "                    if loss_type != \"euclidean\":\n",
        "                        target[target == 0] = -1\n",
        "                    loss = criterion(out1, out2, target)\n",
        "                    if loss_type == \"cosine\":\n",
        "                        dist = (1 - F.cosine_similarity(out1, out2)).cpu().tolist()\n",
        "                    else:\n",
        "                        dist = (F.pairwise_distance(out1, out2)).cpu().tolist()\n",
        "\n",
        "                    all_distances.extend(dist)\n",
        "                    all_labels.extend(label.cpu().tolist())\n",
        "\n",
        "                val_loss_acc += loss.item()\n",
        "\n",
        "        # --- FINAL F1 CALCULATION ---\n",
        "        best_threshold = None\n",
        "        import numpy as np\n",
        "        all_distances = np.array(all_distances)\n",
        "        all_labels = np.array(all_labels)\n",
        "\n",
        "        best_f1 = 0.0\n",
        "        best_threshold = 0.0\n",
        "        # Try 100 thresholds between the min and max distance observed\n",
        "        threshold_candidates = np.linspace(all_distances.min(), all_distances.max(), 100)\n",
        "\n",
        "        for t in threshold_candidates:\n",
        "            preds = (all_distances < t)\n",
        "            tp = int(np.sum((preds == 1) & (all_labels == 1)))\n",
        "            fp = int(np.sum((preds == 1) & (all_labels == 0)))\n",
        "            fn = int(np.sum((preds == 0) & (all_labels == 1)))\n",
        "            tn = 0 # TN not used by your calculate_f1 denominator\n",
        "\n",
        "            current_f1 = calculate_f1(tp, tn, fp, fn)\n",
        "            if current_f1 > best_f1:\n",
        "                best_f1 = current_f1\n",
        "                best_threshold = t\n",
        "        epoch_f1 = best_f1\n",
        "\n",
        "        avg_val_loss = val_loss_acc / len(val_loader)\n",
        "        if best_threshold:\n",
        "            threshold = best_threshold\n",
        "\n",
        "        val_thresholds.append(threshold)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_f1_scores.append(epoch_f1)\n",
        "        print(f\"VALIDATION Epoch {epoch+1}: Loss = {avg_val_loss:.4f} | F1 Score = {epoch_f1:.4f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # SAVING\n",
        "        ckpt_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "        print(f\"Checkpoint saved: {ckpt_path}\")\n",
        "\n",
        "        # UPDATING CSV E PLOT\n",
        "        with open(csv_path, mode='a', newline='') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow([epoch+1, avg_loss, avg_val_loss,epoch_f1, threshold])\n",
        "        print(\"CSV updated.\")\n",
        "        current_params = {\n",
        "                'batch_size': BATCH_SIZE,\n",
        "                'margin': MARGIN,\n",
        "                'learning_rate': LR\n",
        "            }\n",
        "        save_plots(model, optimizer, train_losses, val_losses, val_f1_scores, val_thresholds, save_dir, title_suffix=f\"({loss_type})\", params=current_params,\n",
        "              unfreeze_epoch=Config.unfreeze_at_epoch, train_transform = train_transform)\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
