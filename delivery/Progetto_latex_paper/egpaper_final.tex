\newcommand{\myimgwidth}{0.6\linewidth}
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{float}
\usepackage{caption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{4321}
\begin{document}

%%%%%%%%% TITLE
\title{Few-Shot Logo Recognition}

\author{Oggero Paolo\\
s342937\\
address:\\
{\tt\small TODO: insert email address}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Cancemi Alessia\\
s347156\\
address\\
{\tt\small TODO: insert email address}
\and
Mincigrucci Gabriele\\
s358987\\
address\\
{\tt\small TODO: insert email address}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Logo recognition in open-world scenarios is hampered by the long-tailed
   nature of the data. This work presents a Few-Shot Learning pipeline based
   on LogoDet-3K and Deep Metric Learning. Using a ResNet-50 optimized with
   Triplet and Contrastive Loss, we map logos into an embedding space.
   Through a progressive freezing strategy, the model learns generalized representations
   that allows the retrieval of brands never seen during training.
\end{abstract}



%%%%%%%%% BODY TEXT
\section{Introduction}

Accurate logo recognition has become a fundamental pillar of media analysis and copyright protection. In uncontrolled contexts, however, computer vision systems must contend with market dynamics: new brands emerge every day with unique visual identities that must be instantly identified.
Classical classification paradigms suffer from two structural limitations: the need for enormous datasets for each class and the inability to recognize categories not included in the training set. Few-Shot Learning emerges as a necessary solution, shifting the focus from "mnemonic recognition" to "morphological comparison."
In this work, we address this challenge by implementing a Deep Metric Learning system.

\begin{figure}[!ht]
   \centering

   \includegraphics[width=\linewidth]{imm/1immintro.pdf}
   
   \caption{Sample of images from LogoDet-3K}
\end{figure}

Instead of training the model to assign a label, we train it to generate embeddings in a compact latent space. Using the LogoDet-3K dataset, we developed a pipeline that extracts deep features using a ResNet-50 and projects them into a metric space where the distance reflects the similarity of the logos. This approach not only better manages data sparsity, but also allows the system to operate on unseen classes without the need for retraining.
The work introduces an embedding-based architecture, developed using a linear projection head that enables a standard CNN to extract metric features. This structure allows for an in-depth comparative analysis of losses, evaluating how Triplet and Contrastive Loss (in the Euclidean and Cosine variants) influence the topology of the latent space.
To support training, a dynamic transfer learning management system based on progressive layer unlocking is implemented, ensuring an optimal balance between model stability and learning speed, while also allowing for deeper specialization of the extracted features. The work concludes with an Open-Set Evaluation conducted on brands never encountered during the training phase, using retrieval metrics such as mAP to validate the system's actual generalization capability.


\section{Data}
For this project, we used the LogoDet-3K dataset\cite{wang2022logodet}, which currently represents one of the largest and most complex benchmarks for logo recognition. This work is based on the use of the LogoDet-3K dataset, publicly accessible via the repository: \url{ https://github.com/Wangjing1551/LogoDet-3K-Dataset}.

\subsection{Dataset Description}
LogoDet-3K comprises 3,000 logo categories, with approximately 200,000 manually
 annotated objects distributed across 158,652 images. The dataset is hierarchically 
 organized into nine supercategories (Food, Clothes, Necessities, Others, Electronics, 
 Transportation, Leisure, Sports, and Medical). The dataset is inherently long-tailed, we can see that in the figure below \ref{fig:2}, 
 some classes have a very large number of samples, while others (especially in the Medical 
 or Sports categories) have very few instances. This distribution faithfully reflects the 
 challenges of real-world scenarios. In \ref{fig:2} the classes are grouped by macrocategory along the x-axis, and the number of images per class is shown on the y-axis. The red dots indicate the class with the most images for each macrocategory. We can see a significant imbalance between macrocategories, both in terms of the number of classes per macrocategory and the number of images per class.

 \begin{figure}[!ht]
   \centering

   \includegraphics[width=\linewidth]{imm/logodet3k_with_red_dots.pdf}
   
   \caption{Distribution of the LogoDet-3K dataset by macro-category}
   \label {fig:2}
\end{figure}

\subsection{Dataset Preprocessing and Partitioning}

The preparation pipeline, aimed at optimizing dataset integrity and loading efficiency, was divided into three phases.
\textbf{Brand Consolidation}: Automated cleanup was performed to merge duplicate folders (e.g., alpinestart-1 and alpinestars-2 contained the same information) and normalize labels in XML annotations.
\textbf{Static Indexing}: To accelerate data loading, a unique brand-to-index mapping was generated offline, injected directly into the XML files, and saved in a CSV lookup file for quick reference.
\textbf{Brand-Wise Split}: The dataset was split offline, isolating 10\% of the brands for the test set, randomly selected from each category to guarantee the same data distribution. This separation at the class level (rather than at the individual image level) ensures rigorous open-set evaluation, testing the model on categories never encountered during training.
At runtime, the system manages an online split of the remaining data using the \texttt{getTrainValPaths} function. This allows you to dynamically split the brands between Training and Validation (70/30 split for routine tests and 80/20 for final runs), ensuring that validation always occurs on classes not used for weight updating.
we share our pre-processed dataset at: \url{https://drive.google.com/file/d/1yFiHK_r6ae8Xs8uls8F-ccXHPpqVyh74/view?usp=sharing}.

\subsection{Sampling and Data Loading Strategies}
To meet the requirements of the implemented loss functions, two specific Dataset classes were created:
\textbf{DatasetTriplet}: For each reference image (Anchor), randomly selects one example from the same brand (Positive) and one from a different brand (Negative).
\textbf{DatasetContrastive}: Generates image pairs with a 50\% probability of belonging to the same brand (label 1) or different brands (label 0).

\subsection{Preprocessing and Data Augmentation}
The original images have heterogeneous resolutions. In the loading module, the data is normalized and transformed to improve the model's robustness. This process involves \textbf{Resize and Normalization:} All images are resized to $224 \times 224$ pixels and normalized using the ImageNet mean and standard deviation, ensuring compatibility with the pre-trained weights of ResNet-50. 
Subsequently, \textbf{Augmentation} is applied during training, transformations, including \texttt{RandomResizedCrop}, \texttt{RandomHorizontalFlip}, and \texttt{ColorJitter}. These techniques simulate the distortions typical of real-world logos (light variations, angled shots, etc.).


\section{Methods}
This section describes the system architecture and the optimization methodologies adopted to transform the logo recognition problem into a Deep Metric Learning task.

\subsection{Model Architecture: LogoResNet50}
The implemented architecture is based on a \emph{ResNet-50 backbone}, chosen for its optimal balance between computational depth and feature extraction capability. The original model, pre-trained on ImageNet, has been modified to adapt to the \textbf{metric learning paradigm} through a custom \textbf{Embedding Head}. This module consists of a high-dimensional FC layer, followed by \emph{1D Batch Normalization} and \emph{Dropout}, ending with a second FC layer for the final latent projection. To ensure the effectiveness of the loss functions, the embeddings undergo \textbf{Spatial Normalization} using the Euclidean norm. This step forces the network to optimize the \textbf{semantic direction} of the logos, ensuring that spatial proximity reflects true \textbf{morphological similarity} rather than scale artifacts.

\subsection{Loss Functions}
To optimize the topology of the embedding space, three different training objectives were implemented and compared.
The first implemented method is \emph{Triplet Margin Loss}: This is the primary retrieval strategy. The function works on triplets (Anchor, Positive, Negative) and minimizes the distance between Anchor and Positive, while simultaneously maximizing the distance between Anchor and Negative with a configurable margin ($\alpha= 1.0 $).
Alternatively, the model uses \emph{Contrastive Loss (Euclidean)}: Optimized for image pairs, it penalizes the distance between similar pairs and forces dissimilar pairs to a distance greater than the set margin.    
Finally, the third objective is \emph{Contrastive Loss (Cosine)}: A variant of the previous one that uses cosine similarity instead of Euclidean distance. This metric is particularly effective in high-dimensional spaces because it focuses on the orientation of the vectors rather than their magnitude.

\subsection{Training and Transfer Learning Strategy}
Training is managed by a dynamic configuration system (Config) that allows for reproducible experiments. The main phases include.
The process initiates with \emph{Progressive Freezing} To preserve pre-learned knowledge about ImageNet, the model starts training with frozen backbone convolutional blocks (freeze\_layers).
This phase is followed by \emph{Dynamic Unfreezing}: upon reaching a predetermined epoch we tested unfreezeing some layers, allowing for fine-tuning of logo-specific spatial features. To handle the sudden increase in trainable parameters we experimented with a learning rate reduction, allowing for more stable and precise fine-tuning.
Finally, regarding \emph{Optimization}: Both \textbf{SGD} and \textbf{Adam} were evaluated, adopting in later runs a differentiated learning rate strategy for both, where the learning rate for the backbone parameters is reduced by a config factor compared to that for the head parameters. This allows us to refine the specific morphological knowledge of the logos without compromising the patterns pre-trained on ImageNet.


\subsection{Evaluation Metrics}
The main metrics calculated in the validation loop are:
\begin{itemize}
   \item \textbf{F1-Score}: Dynamically calculated during training to monitor the balance between Precision and Recall.
   \item \textbf{Mean Average Precision (mAP)}: Used to measure retrieval quality, i.e., how effectively the model ranks correct logos at the top of search results.
   \item \textbf{Precision and Recall}: Calculated to analyze the model's behavior with respect to false positives and false negatives. Precision indicates how reliable the positive predictions (logo matches) are, while recall measures the system's ability to retrieve all correct instances.
   \item \textbf{Recall at fixed Precision (R@95P)}: It measures the percentage of correct matches recovered when the system detects a high accuracy (95\%), that is, when you want to minimize false positives.
    \item \textbf{discriminant ratio (J)}: It is calculated as the ratio between the average distance of negative pairs and the average distance of positive pairs. A higher J indicates a better separation between classes in the embedding space, which is crucial for effective retrieval.
\end{itemize}


\section{experiments}
The experimental analysis consisted of a series of tests aimed at identifying the optimal combination of hyperparameters, following different transfer learning strategies.
For clarity, the ranges of the hyperparameters used are shown in \ref{tab:hyper_ranges}.

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \caption{Range of hyperparameters (Minimum - Maximum) observed across all experimental configurations.}
    \label{tab:hyper_ranges}
    \begin{tabularx}{\linewidth}{|X|c|c|}
        \hline
        \textbf{Hyperparameter} & \textbf{Min Value} & \textbf{Max Value} \\
        \hline
        Batch Size & 32 & 256 \\
        \hline
        Learning Rate (Initial) & $1.0 \times 10^{-4}$ & $1.0 \times 10^{-1}$ \\
        \hline
        Margin & 0.2 & 1.0 \\
        \hline
        Weight Decay & 0 & $1.0 \times 10^{-4}$ \\
        \hline
        Backbone LR Reduction Factor & 0.03 & 0.5 \\
        \hline
        Embedding Output Dim. & 128 & 256 \\
        \hline
    \end{tabularx}
\end{table}

\subsection{Configuring Training and Testing}

For training and evaluating the system, a pipeline has been defined that separates feature learning from their evaluation in unseen scenarios.
\textbf{Training (Deep Metric Learning)}: All experiments were conducted using a ResNet-50 pre-trained on ImageNet and adapted to the metric learning paradigm via an embedding head.
Training was performed using a progressive unfreezing strategy.
\textbf{Testing (Few-Shot Evaluation)}: Testing is not based on simple class accuracy, but rather on episodic evaluation (N-way K-shot).
The system is tested on brands never seen in the training set. For each episode, a few example images (support set) are provided, along with a query to be classified using cosine similarity in the latent space. This ensures that the model is effectively "measuring similarity" and not memorizing labels.

\subsection{Hyperparameter Optimization Strategy}
The training phase followed an iterative optimization strategy to achieve the best hyper parameters configuration. The model was evaluated based on its ability to minimize the Triplet Loss while maximizing the Validation F1 Score at an optimal distance threshold.



\subsection{Triplet Loss Analysis}
The evolution of testing on \textbf{Triplet Margin Loss} provided critical insights into the impact of gradient stability and geometric constraints on the embedding space quality.\vspace{1em}

\textbf{Analysis of Model Iterations and Improvements:} Preliminary experiments conducted on a 10,000-image subset revealed several technical challenges that shaped the final training:
\begin{itemize}
    \item \textbf{Data Augmentation:} To mitigate early overfitting (Figure \ref{fig:41}), a pipeline including perspective shifts, rotations, and color jitter was implemented. This forced the ResNet-50 backbone to move beyond simple pixel matching and learn \textbf{affine-invariant features} (Figure \ref{fig:42}).
    \item \textbf{Optimization and Unfreezing:} Transitioning from Adam to \textbf{SGD} yielded more stable convergence. Furthermore, "multi-stage unfreezing" tests showed that aggressive backbone releases, where different parts of the backbone were unfrozen at different epochs, induced gradient "shocks" (Figure \ref{fig:43}). This led to the adoption of a more conservative approach with a single unfreezing only of the later stage of the ResNet backbone.
    \item \textbf{L2 Normalization and the "Scaling Problem":} Initial runs without L2 normalization revealed a fundamental vulnerability: the model minimized loss by globally expanding distances (a trivial scaling strategy). This caused a persistent upward trajectory in the distance threshold as it can be seen in Figures from (Figure \ref{fig:41}) to (Figure \ref{fig:43}). Introducing \textbf{L2 normalization} and a hidden linear layer forced the model to map logos onto a unit hypersphere, favouring threshold convergence and ensuring a more robust latent space.
\end{itemize}
\textbf{Optimal Model Results (Figure \ref{fig:44}):} The final configuration demonstrated the robustness required for Open-Set scenarios:
\begin{itemize}
    \item \textbf{Loss and F1-Score Curves:} Constant convergence was observed, with Validation Loss stabilizing around 0.45. The \textbf{F1-score reached a peak of 0.775}; the sharp improvement at epoch 15 validates the progressive unfreezing protocol combined with a learning rate reduction (factor 0.2).
    \item \textbf{Threshold Stability:} Unlike "unconstrained" models, the optimal distance threshold converged asymptotically toward around \textbf{1.18}. This stability confirms that the model transitioned from a scaling strategy to a structured latent space with reliable boundaries, essential for identifying unseen categories.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\myimgwidth]{imm/img41.pdf}
    \caption{Early overfitting trend observed during preliminary experiments without augmentation.
    Training setup: dataset size 10k samples, batch size 32, Adam optimizer (lr $=10^{-3}$),
    margin $=1.0$, embedding MLP $2048 \rightarrow 128$.}
    \label{fig:41}
\end{figure}

% \begin{table}[t] % Prova [b!] per metterla a fondo colonna, o [t!] per alto
%     \centering
%     \small % Font leggermente ridotto
%     \caption{Configuration hyperparameters related to the results in Figure \ref{fig:41}.}
%     \label{tab:config_1}
    
%     % Regola lo spazio tra le righe per compattare
%     \renewcommand{\arraystretch}{1.1} 
    
%     % Se hai il pacchetto booktabs usa \toprule, \midrule, \bottomrule
%     % Altrimenti cambia con \hline
%     \begin{tabularx}{\linewidth}{|l|X|} % 'l' per la prima colonna, 'X' per la seconda
%         \hline % \toprule
%         \textbf{Hyperparameter} & \textbf{Value} \\
%         \hline % \midrule
%         Dataset Size & 10,000 \\
%         \hline
%         Batch Size & 32 \\
%         \hline
%         Optimizer & Adam \\
%         \hline
%         Weight Decay & None \\
%         \hline
%         Margin & 1.0 \\
%         \hline
%         Learning Rate & 0.001 \\
%         \hline
%         Backbone LR Red. & None \\
%         \hline
%         Output Norm. & No \\
%         \hline
%         Embedding Layer & MLP ($2048 \to 128$) \\
%         \hline
%         Freeze Strat. & None \\
%         \hline
%         Data Aug. & None \\
%         \hline % \bottomrule
%     \end{tabularx}
% \end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\myimgwidth]{imm/img42.pdf}
    \caption{shifts, rotations, and color jitter implementation}
    \label{fig:42}
\end{figure}

\begin{figure}[H]
   \centering
   \includegraphics[width=\myimgwidth]{imm/img43.pdf}
   \caption{backbone unfreezing "shock"}
   \label{fig:43}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\myimgwidth]{imm/img44.pdf}
    \caption{Optimal Model Results}
    \label{fig:44}
\end{figure}
%Tabella dell'immagine numero 4.4 (commentata per evitare problemi di spazio e posizionamento in CVPR)

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \caption{Configuration hyperparameters related to the results in Figure \ref{fig:44}.}
    \label{tab:config_img44}
    \begin{tabularx}{\linewidth}{|l|X|}
        \hline
        \textbf{hyperparameter} & \textbf{Value} \\
        \hline
        Dataset Size & Full \\
        \hline
        Batch Size & 256 \\
        \hline
        Optimizer & SGD (Momentum $= 0.9$) \\
        \hline
        Weight Decay & $0.0001$ \\
        \hline
        Margin & 1.0 \\
        \hline
        Learning Rate (Head) & $1.0 \times 10^{-1} \to 2.0 \times 10^{-2}$ (Decay Factor: 0.2) \\
        \hline
        Backbone LR Factor & $0.03$ (compared to the Head's LR) \\
        \hline
        Embedding Layer & MLP ($2048 \to 256 \to \text{BatchNorm1d} \to \text{ReLU} \to \text{Dropout}(p=0.3) \to 128$) \\
        \hline
        Freeze Strategy & $4 \to 3$ (Sblocco all'epoca 15) \\
        \hline
        Data Augmentation & Resize ($224 \times 224$), RandomPerspective ($p=0.3$), RandomHorizontalFlip ($p=0.3$), RandomRotation ($\pm 15^\circ$), RandomGrayscale ($p=0.1$), ColorJitter, Normalize \\
        \hline
    \end{tabularx}
\end{table}


\subsection{Contrastive Loss Analysis (Cosine)}

While the Euclidean-distance alternative showed less promising convergence and couldnâ€™t be explored further due to time and Colab compute limits, experiments with the Cosine Similarity variant reveal a distinct learning dynamic, emphasizing refinement of the angular margin between embeddings.

\textbf{Analysis of Model Iterations and Improvements:} The tuning of the hyper parameters for the contrastive loss was done taking into account the results obtained from the triplet runs:
\begin{itemize}
    \item \textbf{Optimizer Selection:} Initial tests on a 10,000 sample subset identified SGD as the more stable choice. Adam caused volatility in the distance threshold during early trials, whereas SGD (Figure \ref{fig:431}) shows a steady F1-score increase and a consistent distance threshold, suggesting momentum-based SGD better preserves the rigid geometric margin needed for this metric learning task.
    \item \textbf{Architectural Refinements:} Utilizing the refinements developed during the Triplet Loss phase, specifically the inclusion of a hidden linear layer and \textbf{L2 normalization}, the contrastive runs focused on optimizing the relationship between positive and negative pairs. The L2 normalization is particularly critical here, as Cosine Similarity inherently operates on the unit hypersphere, ensuring that the model optimizes the angle between vectors rather than their magnitude.
    \item \textbf{Full-Scale Optimization:} The final scale-up to the full dataset utilized a batch size of 256 to better fit the Colab environment and a 0.25 backbone learning rate factor. Releasing part of the backbone weights after a certain epoch allowed the embedding head to stabilize before the final fine-tuning phase.
\end{itemize}
\textbf{Optimal Model Results (Figure \ref{fig:433}):} The combination of \textbf{Contrastive Loss} and the previously optimized hyperparameters successfully established a reliable global metric:
\begin{itemize}
    \item \textbf{F1-Score and Convergence:} As visible in the graphs for the optima model, the \textbf{validation F1-score peaked at 0.811}, outperforming the triplet-based baseline. This suggests that the Contrastive approach provides a more efficient learning for the model under the selected hyperparameter configuration. By directly optimizing the similarity between pairs, the model achieved better convergence compared to the triplet-based strategy.
    \item \textbf{Threshold Stability:} The \textbf{distance threshold} successfully stabilized at approximately \textbf{0.60}. This steady convergence proves that the model achieved a structured latent space where the "intra-class" similarity and "inter-class" distance are well-defined.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=\myimgwidth]{imm/img431.pdf}
    \caption{SGD baseline}
    \label{fig:431}
\end{figure}

% \begin{table}[H]
%     \centering
%     \renewcommand{\arraystretch}{1.2}
%     \caption{Configuration hyperparameters related to the results in Figure \ref{fig:431}.}
%     \label{tab:config_img431}
%     \begin{tabularx}{\linewidth}{|l|X|}
%         \hline
%         \textbf{hyperparameter} & \textbf{Value} \\
%         \hline
%         Dataset Size & 10000 \\
%         \hline
%         Batch Size & 64 \\
%         \hline
%         Optimizer & SGD \\
%         \hline
%         Weight Decay & 0 \\
%         \hline
%         Margin & 0.2 \\
%         \hline
%         Learning Rate & $1.0 \times 10^{-2}$ (Costante, Factor: 1) \\
%         \hline
%         Backbone LR & Factor $0.5$ (rispetto al LR Head) \\
%         \hline
%         Output Norm. & True \\
%         \hline
%         Embedding & MLP ($2048 \to 1024 \to \text{ReLU} \to 256$) \\
%         \hline
%         Freeze Strat. & $4 \to 3$ (Sblocco all'epoca 5) \\
%         \hline
%         Data Aug. & Resize ($224 \times 224$), RandomPerspective ($p=0.3$), RandomHorizontalFlip ($p=0.3$), RandomRotation ($\pm 15^\circ$), RandomGrayscale ($p=0.1$), ColorJitter, Normalize \\
%         \hline
%     \end{tabularx}
% \end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\myimgwidth]{imm/img433.pdf}
    \caption{Optimal Model Results}
    \label{fig:433}
\end{figure}
%this last experimental setup \ref{fig:433} utilized the full dataset with a batch size of 256, employing an SGD optimizer with zero weight decay and a margin of 0.2.
%A constant learning rate of $1.0 \times 10^{-2}$ was maintained, while the backbone learning rate was scaled by a factor of 0.25. The architecture featured an MLP embedding head ($2048 \rightarrow 1024 \rightarrow \text{ReLU} \rightarrow 256$) with output normalization enabled. Additionally, the training process involved a freeze strategy that unlocked specific layers at epoch 15, alongside a robust data augmentation pipeline that included resizing to $224 \times 224$, random perspective, horizontal flipping, rotation, grayscale adjustments, color jitter, and normalization.

\begin{table}[H]
   \centering
   \renewcommand{\arraystretch}{1.2}
   \caption{Configuration hyperparameters related to the results in Figure \ref{fig:433}.}
   \label{tab:config_433}
   \begin{tabularx}{\linewidth}{|l|X|}
       \hline
       \textbf{hyperparameter} & \textbf{Value} \\
       \hline
       Dataset Size & Full \\
       \hline
       Batch Size & 256 \\
       \hline
       Optimizer & SGD \\
       \hline
       Weight Decay & 0 \\
       \hline
       Margin & 0.2 \\
       \hline
       Learning Rate & $1.0 \times 10^{-2}$ (Costante, Factor: 1) \\
       \hline
       Backbone LR & Factor $0.25$ \\
       \hline
       Output Norm. & True \\
       \hline
       Embedding & MLP ($2048 \to 1024 \to \text{ReLU} \to 256$) \\
       \hline
       Freeze Strat. & $4 \to 3$ (Sblocco all'epoca 15) \\
       \hline
       Data Aug. & Resize ($224 \times 224$), RandomPerspective ($p=0.3$), RandomHorizontalFlip ($p=0.3$), RandomRotation ($\pm 15^\circ$), RandomGrayscale ($p=0.1$), ColorJitter, Normalize \\
       \hline
   \end{tabularx}
\end{table}

\subsection{Comparison between Baseline and Proposed Models}

The following table presents the results obtained comparing the proposed models with the baseline across different support set sizes ($n$). 

\begin{table}[H]
\begin{center}


1-Shot Evaluation ($n=1$) \\[1ex]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|}
\hline
Metric & Triplet (Thr: 0.52) & Contrastive (Thr: 0.52) & Baseline (Thr: 0.90) \\
\hline
Accuracy  & 0.9214 & 0.9227 & 0.9185 \\
Precision & 0.3254 & 0.3453 & 0.3035 \\
Recall    & 0.4525 & 0.4732 & 0.3295 \\
F1-Score  & 0.3392 & 0.3523 & 0.2651 \\
R@95p     & 0.3035 & 0.3486 & 0.3114 \\
mAP       & 0.5083 & 0.5512 & 0.4703 \\
J         & 1.9311 & 1.9868 & 0.3898 \\
\hline
\end{tabular}%
}

\vspace{0.6cm}


5-Shot Evaluation ($n=5$) \\[1ex]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{|l|c|c|c|}
\hline
Metric & Triplet (Thr: 0.50) & Contrastive (Thr: 0.56) & Baseline (Thr: 0.75) \\
\hline
Accuracy  & 0.9298 & 0.9209 & 0.9420 \\
Precision & 0.3774 & 0.4065 & 0.4072 \\
Recall    & 0.5223 & 0.6441 & 0.3493 \\
F1-Score  & 0.3940 & 0.4458 & 0.3350 \\
R@95p     & 0.3696 & 0.4481 & 0.4506 \\
mAP       & 0.5881 & 0.6519 & 0.6015 \\
J         & 1.9311 & 1.9868 & 0.3898 \\
\hline
\end{tabular}%
}

\end{center}
\caption{Comparison between Baseline and Proposed Models for 1-Shot and 5-Shot evaluation.}
\label{tab:final_results}
\end{table}

\textbf{Comparison with Baseline:} While the ResNet-50 baseline achieves high Accuracy, it struggles with retrieval-specific metrics. Our proposed models demonstrate a substantial leap in \textbf{Recall}, \textbf{mAP} and the \textbf{discriminant ratio (J)} indicating that the Metric Learning approach is more effective at clustering similar logos than standard classification. The most striking difference is the \textbf{discriminant ratio (J)}, which rises from $0.3898$ in the baseline to nearly $2.0$, reflecting a more optimized and expansive latent space.

\textbf{Few-Shot Support ($n=5$):} All models exhibit a performance gain as $n$ increases. By averaging the embeddings of five support samples, the system creates a stable brand prototype, filtering out "contextual noise" (unrelated background elements). This is most visible in the Contrastive model's Recall, which jumps from $0.4732$ to $0.6441$.

\textbf{Comparison between Models Performance:} The Contrastive Cosine approach yields superior results compared to the Triplet model. This disparity is likely because the Triplet task is inherently more difficult to optimize; without the implementation of hard negative mining to select challenging samples, the Triplet loss struggles to refine decision boundaries as effectively as the Contrastive loss.

\textbf{Threshold Optimization and Class Imbalance:} 
The optimal similarity thresholds ($\tau$) were determined by maximizing the F1-score across a range of distance candidates. A notable discrepancy exists between the thresholds identified during validation ($\tau \approx 1.175$ for Triplet; $\tau \approx 0.59$ for Contrastive) and those utilized in the final evaluation ($\tau \approx 0.50$--$0.56$). This shift is primarily driven by the transition from a balanced validation environment to a retrieval-based testing scenario. While validation optimizes for a 1:1 positive-to-negative ratio, the testing phase reflects a realistic distribution where negative distractors vastly outnumber positive matches. In this imbalanced setting, maintaining a high F1-score requires a more stringent threshold to prevent the massive pool of negatives from generating enough False Positives to overwhelm the metric.\vspace{1em}

\textbf{Analysis of Metric Variations and Model Impact:}

\begin{itemize}

\item \textbf{Classification Metrics (Accuracy, Precision, Recall, F1-Score):}
The Baseline achieves the highest Accuracy, particularly at $n=5$, while both metric-learning models show a slight decrease, indicating a small trade-off in overall classification correctness. While the precision is not significantly impacted, the most significant improvement is observed in Recall, which increases substantially in both Triplet and Contrastive models, with the latter reaching 0.6441 at $n=5$ compared to 0.3493 for the Baseline, as a result, the F1-Score improves consistently. Overall, these variations indicate that metric learning enhances retrieval effectiveness despite a marginal reduction in Accuracy.

\item \textbf{R@95p:}
The Contrastive model improves R@95p at $n=1$ and remains very close to the Baseline at $n=5$, while the Triplet model shows slightly lower values. This suggests that the Contrastive loss better preserves high-confidence retrieval performance, maintaining strong recall under strict precision constraints.

\item \textbf{mAP:}
mAP improves noticeably with metric learning, particularly for the Contrastive model, which achieves the best overall performance at both $n=1$ and $n=5$. Since mAP reflects ranking quality across thresholds, this confirms that the learned embeddings produce a more consistent and discriminative similarity ordering.

\item \textbf{discriminant ratio (J)}
The J metric increases dramatically from 0.3898 in the Baseline to nearly 2.0 in both proposed models. This substantial rise indicates a significantly more structured and separable latent space, demonstrating improved intra-class compactness and inter-class dispersion regardless of support size.

\end{itemize} 

\subsection{Qualitative Analysis of Retrieval Behavior}

The Contrastive Loss-based architecture proves to be the most effective of the project, achieving a mAP of \textbf{65.2\%}, demonstrating an abstraction capability that goes beyond simple pixel-matching. 
Cases like \emph{Maille} (Figure \ref{fig:5}) highlight how the network extracts invariant features, correctly associating logos on complex backgrounds (e.g., cans on a black background). 
However, the addition of massive graphic elements, as in the case of \emph{Captain Morgan} (Figure \ref{fig:6}), can push the embedding beyond the tolerance threshold, masking the brand's identity and leading to false negatives.

%---------------------------------------------------------------------------------------------------
%Maille
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{imm/img5.pdf}
    \caption{Qualitative analysis of retrieval behavior (Anchor, Positive, Negative) With a complex background.}
    \label{fig:5}
\end{figure}
%Captain Morgan
\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{imm/img6.pdf}
    \caption{Qualitative analysis of retrieval behavior (Anchor, Positive, Negative) with added graphic elements.}
    \label{fig:6}
\end{figure}

%---------------------------------------------------------------------------------------------------

\subsection{Qualitative Embedding Analysis}

Visualization of the latent space using t-SNE qualitatively confirms the quantitative results. The points corresponding to the different brands form distinct and relatively compact clusters, with a clear separation between different categories.
This behavior suggests that the model does not simply memorize surface patterns, but learns morphological features shared between logos of the same brand.

\begin{figure}[h]
    \centering
    \includegraphics[width=\myimgwidth]{imm/img45.pdf}
    \caption{3D t-SNE Visualization of 10 Brands.}
    \label{fig:45}   
\end{figure}

\subsection{Future Developments}

A key limitation of this work was hardware availability. Early experiments were conducted locally on a reduced dataset due to computational constraints, and although final training runs used Google Colab A100 instances, broader access to dedicated high-performance hardware would have enabled more extensive experimentation and improved performance.

Future improvements could focus on refining the sampling strategy. The current Triplet Loss approach could benefit from \textit{Hard Negative Mining}, encouraging the model to learn finer discriminative features by selecting more challenging positive and negative samples. Additionally, alternative losses, such as Proxy-Anchor Loss, potentially leading to improved performance.

Another challenge was processing uncropped images, which introduced significant contextual noise. Incorporating an object detection stage (e.g., Faster R-CNN) to isolate logos could allow the backbone to focus on brand-specific morphology and improve accuracy. Additionally, exploring other kinds or architectures such as Transformers may further enhance class separation and overall performance.

\section{Conclusion}
This project served as a comprehensive bridge between the theoretical frameworks discussed in class and the practical challenges of modern computer vision. Throughout the development process, we faced significant constraints that mirrored real-world AI development, such as hardware constrain, difficulty of the task and complex hyper parameters tuning.
We put into practice several strategies explored during the course, including data augmentation, dropout, batch normalization and differential learning rates, to mitigate the constant risk of overfitting.
The experience of training on uncropped images also provided a valuable lesson in the impact of contextual noise. We learned that without a localized region of interest, the model is forced to interpret the entire scene, which adds a layer of complexity. 
Ultimately, we are satisfied with having successfully applied these advanced techniques to a real-world scenario. This project allowed us to move beyond the theory of metric learning to build a compact and functional system, proving that even with limited resources, it is possible to translate classroom concepts into a functioning retrieval pipeline.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
