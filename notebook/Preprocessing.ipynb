{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "UNZIP from Drive to /content/LogoDet-3k"
      ],
      "metadata": {
        "id": "rjIqoqS7brwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def unzip_dataset():\n",
        "    # 1. Mount Google Drive (if not already mounted)\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # 2. Configuration\n",
        "    # Source path (on Google Drive)\n",
        "    source_path = \"/content/drive/MyDrive/Colab Notebooks/LogoDet-3K.zip\"\n",
        "\n",
        "    # Destination path (Local VM - Ephemeral storage)\n",
        "    # Using local storage is significantly faster for training than reading from Drive\n",
        "    dest_path = \"/content/LogoDet-3K\"\n",
        "\n",
        "    # 3. Verification\n",
        "    if not os.path.exists(source_path):\n",
        "        raise FileNotFoundError(f\"The file was not found at: {source_path}\")\n",
        "\n",
        "    # 4. Extraction Routine\n",
        "    print(f\"Extracting '{source_path}' to '{dest_path}'...\")\n",
        "\n",
        "    # Create destination directory if it doesn't exist\n",
        "    os.makedirs(dest_path, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(source_path, 'r') as zip_ref:\n",
        "            # extractall handles the directory tree creation automatically\n",
        "            zip_ref.extractall(dest_path)\n",
        "        print(\" Extraction complete successfully.\")\n",
        "        print(f\"Files are located at: {dest_path}\")\n",
        "\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\" Error: The file is a corrupted zip archive.\")\n",
        "    except Exception as e:\n",
        "        print(f\" An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unzip_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ahG1V6cZf6k",
        "outputId": "62a9d79a-e596-46d7-bcb7-31a6a23179d6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting '/content/drive/MyDrive/Colab Notebooks/LogoDet-3K.zip' to '/content/LogoDet-3K'...\n",
            "âœ… Extraction complete successfully.\n",
            "Files are located at: /content/LogoDet-3K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete Duplicates"
      ],
      "metadata": {
        "id": "t3pKoL0nb6b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "def clean_duplicate_brand_folders(root_dir):\n",
        "    \"\"\"\n",
        "    Keep only the first duplicate brand folder, rename it to the base brand name,\n",
        "    and delete the rest. Fix XML labels in the kept folder to remove suffixes like -1, -2, etc.\n",
        "    \"\"\"\n",
        "    for category in os.listdir(root_dir):\n",
        "        category_path = os.path.join(root_dir, category)\n",
        "        if not os.path.isdir(category_path):\n",
        "            continue\n",
        "\n",
        "        # Find brand folders with '-number' suffix\n",
        "        brand_folders = [b for b in os.listdir(category_path) if os.path.isdir(os.path.join(category_path, b))]\n",
        "        base_name_to_folders = {}\n",
        "\n",
        "        for b in brand_folders:\n",
        "            match = re.match(r\"^(.*?)-\\d+$\", b)\n",
        "            if match:\n",
        "                base_name = match.group(1)\n",
        "                if base_name not in base_name_to_folders:\n",
        "                    base_name_to_folders[base_name] = []\n",
        "                base_name_to_folders[base_name].append(b)\n",
        "\n",
        "        # Process duplicates\n",
        "        for base_name, folders in base_name_to_folders.items():\n",
        "            if len(folders) < 1:\n",
        "                continue  # no duplicates\n",
        "\n",
        "            folders.sort()  # keep the first folder alphabetically\n",
        "            folder_to_keep = folders[0]\n",
        "            folders_to_delete = folders[1:]\n",
        "\n",
        "            kept_folder_path = os.path.join(category_path, folder_to_keep)\n",
        "            new_folder_path = os.path.join(category_path, base_name)\n",
        "\n",
        "            print(f\"Keeping folder: {folder_to_keep}, renaming to: {base_name}, deleting duplicates: {folders_to_delete}\")\n",
        "\n",
        "            # Fix XML labels in the kept folder\n",
        "            for xml_file in os.listdir(kept_folder_path):\n",
        "                if xml_file.endswith(\".xml\"):\n",
        "                    xml_path = os.path.join(kept_folder_path, xml_file)\n",
        "                    try:\n",
        "                        tree = ET.parse(xml_path)\n",
        "                        root = tree.getroot()\n",
        "                        for obj in root.findall(\"object\"):\n",
        "                            name_elem = obj.find(\"name\")\n",
        "                            # Remove any -number suffix\n",
        "                            name_elem.text = base_name\n",
        "                        tree.write(xml_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error fixing XML {xml_path}: {e}\")\n",
        "\n",
        "            # Rename the kept folder to the base brand name\n",
        "            if kept_folder_path != new_folder_path:\n",
        "                try:\n",
        "                    os.rename(kept_folder_path, new_folder_path)\n",
        "                    kept_folder_path = new_folder_path\n",
        "                    print(f\"Renamed folder: {folder_to_keep} -> {base_name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error renaming folder {kept_folder_path} -> {new_folder_path}: {e}\")\n",
        "\n",
        "            # Delete the other duplicate folders\n",
        "            for folder in folders_to_delete:\n",
        "                folder_path = os.path.join(category_path, folder)\n",
        "                try:\n",
        "                    shutil.rmtree(folder_path)\n",
        "                    print(f\"Deleted folder: {folder_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error deleting folder {folder_path}: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_dir = \"LogoDet-3K/LogoDet-3K\"  # replace with your dataset root\n",
        "    clean_duplicate_brand_folders(dataset_dir)\n",
        "    print(\"Duplicate brand folders cleaned, renamed, and XML labels fixed.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0b-ORonXtx7",
        "outputId": "00490ce7-3e1b-4a35-ab26-044f90ba9ad2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate brand folders cleaned, renamed, and XML labels fixed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creazione PAN"
      ],
      "metadata": {
        "id": "AZUdAlSwRu8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "CsNfX4K9RnVo"
      },
      "outputs": [],
      "source": [
        "########################################################################################################################################\n",
        "# ALCUNI BRANDS SEMBRANO ESSERE DUPLICATI EX LogoDet-3K\\LogoDet-3K\\Leisure\\stein world-1 E LogoDet-3K\\LogoDet-3K\\Leisure\\stein world-2 #\n",
        "########################################################################################################################################\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from collections import defaultdict\n",
        "\n",
        "SEED = 101\n",
        "\n",
        "\n",
        "\n",
        " # DATASET that takes the list of files for the dataset,\n",
        " # builds a dictionary with label -> image paths (label is taken from the image path, not from the xml because it is faster)\n",
        " # when an item from the dataset is requested it returns a triplet (anchor, positive, negative)\n",
        "\n",
        " # each sample has the structure: sample = {\"image\": img_transformed, \"labels\": labels_list, \"bbs\": bb_list}\n",
        "\n",
        "class DatasetTriplet(torch.utils.data.Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "      self.file_list = file_list\n",
        "      self.transform = transform\n",
        "\n",
        "      self.label_to_indices = defaultdict(list)\n",
        "      for idx, img_path in enumerate(self.file_list):\n",
        "        # Extract label from path: LogoDet-3K\\LogoDet-3K\\Clothes\\panerai\\21.jpg\n",
        "        # Label is the second-to-last part of the path\n",
        "        label = img_path.replace('\\\\', '/').split('/')[-2]\n",
        "        self.label_to_indices[label].append(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength =len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        xml_path = image_path.replace(\".jpg\", \".xml\")\n",
        "        img = Image.open(image_path)\n",
        "        orig_w, orig_h = img.size\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        labels_list = []\n",
        "        bb_list = []\n",
        "\n",
        "        try:\n",
        "          tree = ET.parse(xml_path)\n",
        "          root = tree.getroot()\n",
        "        except Exception as e:\n",
        "          raise Exception(f\"Failed to parse XML file: {xml_path} | Error: {e}\")\n",
        "\n",
        "        objects = root.findall(\"object\")\n",
        "\n",
        "        for obj in objects:\n",
        "          label = obj.find(\"name\").text\n",
        "          bbox = obj.find(\"bndbox\")\n",
        "          xmin = int(bbox.find(\"xmin\").text)\n",
        "          ymin = int(bbox.find(\"ymin\").text)\n",
        "          xmax = int(bbox.find(\"xmax\").text)\n",
        "          ymax = int(bbox.find(\"ymax\").text)\n",
        "\n",
        "          # Scale bounding boxes to match the resized image\n",
        "          new_w, new_h = img_transformed.shape[2], img_transformed.shape[1]\n",
        "          x_scale = new_w / orig_w\n",
        "          y_scale = new_h / orig_h\n",
        "\n",
        "          bbox_scaled = {\n",
        "              \"xmin\": int(xmin * x_scale),\n",
        "              \"ymin\": int(ymin * y_scale),\n",
        "              \"xmax\": int(xmax * x_scale),\n",
        "              \"ymax\": int(ymax * y_scale)\n",
        "          }\n",
        "\n",
        "        labels_list.append(label)\n",
        "        bb_list.append(bbox_scaled)\n",
        "\n",
        "\n",
        "        return {\"image\": img_transformed, \"labels\": labels_list, \"bbs\": bb_list}\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        anchor_img_path =self.file_list[idx]\n",
        "        anchor_label = anchor_img_path.replace('\\\\', '/').split('/')[-2]\n",
        "\n",
        "        anchor = self.load_image(anchor_img_path)\n",
        "\n",
        "        # get Positive\n",
        "        positive_indices = [i for i in self.label_to_indices[anchor_label] if i != idx]\n",
        "        positive_idx = random.choice(positive_indices)\n",
        "        positive_img_path = self.file_list[positive_idx]\n",
        "        positive_label = positive_img_path.replace('\\\\', '/').split('/')[-2]\n",
        "\n",
        "        positive = self.load_image(positive_img_path)\n",
        "\n",
        "        # get Negative\n",
        "        negative_label = random.choice([l for l in self.label_to_indices.keys() if l != anchor_label])\n",
        "        negative_idx = random.choice(self.label_to_indices[negative_label])\n",
        "        negative_img_path = self.file_list[negative_idx]\n",
        "\n",
        "        negative = self.load_image(negative_img_path)\n",
        "\n",
        "\n",
        "\n",
        "        return anchor, positive, negative\n",
        "\n",
        "\n",
        "# Returns a list of paths divided into train, validation and test.\n",
        "# the directory dir is scanned to find all brands which are then split into train, validation, test so that validation and test have proportion val_split, test_split\n",
        "# if total_set_size is provided only that number of images are loaded so that for each brand at least min_images_per_brand images are present\n",
        "\n",
        "def getPathsSetsByBrand(dir, val_split, test_split, total_set_size=None, min_images_per_brand=2):\n",
        "    category_list = []\n",
        "    brand_list = []\n",
        "\n",
        "    for category in os.listdir(dir):\n",
        "        category_path = os.path.join(dir, category)\n",
        "        for brand in os.listdir(category_path):\n",
        "           brand_path = os.path.join(category_path, brand)\n",
        "           brand_list.append(brand_path)\n",
        "        category_list.append(category_path)\n",
        "\n",
        "    test_size = int(len(brand_list) * test_split)\n",
        "    val_size = int(len(brand_list) * val_split)\n",
        "    train_size = len(brand_list) - test_size - val_size\n",
        "\n",
        "    generator = torch.Generator().manual_seed(SEED)\n",
        "    train_subset, val_subset, test_subset = random_split(brand_list, [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "    # random_split return a datase, not a list. To get the list of strings needed to pass to the Dataset we do the following:\n",
        "    train_brand_list = [brand_list[i] for i in train_subset.indices]\n",
        "    val_brand_list   = [brand_list[i] for i in val_subset.indices]\n",
        "    test_brand_list  = [brand_list[i] for i in test_subset.indices]\n",
        "\n",
        "    train_data_list = []\n",
        "    val_data_list = []\n",
        "    test_data_list = []\n",
        "\n",
        "    if total_set_size is not None:\n",
        "      images_per_brand = round(total_set_size / (len(train_brand_list) + len(val_brand_list) + len(test_brand_list)))\n",
        "\n",
        "      print(f\"Number of brands in training set: {len(train_brand_list)}\")\n",
        "      print(f\"Number of brands in validation set: {len(val_brand_list)}\")\n",
        "      print(f\"Number of brands in test set: {len(test_brand_list)}\")\n",
        "      print(f\"images sampled per brand: {images_per_brand}\")\n",
        "\n",
        "      if images_per_brand < min_images_per_brand:\n",
        "        # downscale the number of brands per set to guarantee min_images_per_brand\n",
        "\n",
        "        print(f\"not enough images per brand, resizing sets\")\n",
        "\n",
        "        new_total_brand_size = round(total_set_size / min_images_per_brand)\n",
        "        new_val_brand_size = round(new_total_brand_size * val_split)\n",
        "        new_test_brand_size = round(new_total_brand_size * test_split)\n",
        "        new_train_brand_size = new_total_brand_size - new_val_brand_size - new_test_brand_size\n",
        "\n",
        "        train_brand_list = random.sample(train_brand_list, new_train_brand_size)\n",
        "        val_brand_list = random.sample(val_brand_list, new_val_brand_size)\n",
        "        test_brand_list = random.sample(test_brand_list, new_test_brand_size)\n",
        "\n",
        "        print(f\"Number of brands in training set: {len(train_brand_list)}\")\n",
        "        print(f\"Number of brands in validation set: {len(val_brand_list)}\")\n",
        "        print(f\"Number of brands in test set: {len(test_brand_list)}\")\n",
        "\n",
        "        images_per_brand = min_images_per_brand\n",
        "        print(f\"new images sampled per brand: {images_per_brand}\")\n",
        "\n",
        "      for brand in train_brand_list:\n",
        "        all_images = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "        if images_per_brand > len(all_images):\n",
        "          print(f\"images are less than {min_images_per_brand} for this brand: {brand}\")\n",
        "        sampled_images = random.sample(all_images, min(images_per_brand, len(all_images)))\n",
        "        train_data_list.extend(sampled_images)\n",
        "\n",
        "      for brand in val_brand_list:\n",
        "        all_images = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "        if images_per_brand > len(all_images):\n",
        "          print(f\"images are less than {min_images_per_brand} for this brand: {brand}\")\n",
        "        sampled_images = random.sample(all_images, min(images_per_brand, len(all_images)))\n",
        "        val_data_list.extend(sampled_images)\n",
        "\n",
        "      for brand in test_brand_list:\n",
        "        all_images = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "        if images_per_brand > len(all_images):\n",
        "          print(f\"images are less than {min_images_per_brand} for this brand: {brand}\")\n",
        "        sampled_images = random.sample(all_images, min(images_per_brand, len(all_images)))\n",
        "        test_data_list.extend(sampled_images)\n",
        "    else:\n",
        "      for brand in train_brand_list:\n",
        "        train_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "      for brand in val_brand_list:\n",
        "        val_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "      for brand in test_brand_list:\n",
        "        test_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "\n",
        "    return train_data_list, val_data_list, test_data_list\n",
        "\n",
        "\n",
        "\n",
        "def get_K_RandomImages(data_list, K=5):\n",
        "  if K > len(data_list):\n",
        "    raise ValueError(\"K cannot be larger than the size of data_list\")\n",
        "  return random.sample(data_list, K)\n",
        "\n",
        "def visualizeImagesFromPathList(image_list, title=\"Images\"):\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i, img_path in enumerate(image_list):\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        plt.subplot(1, len(image_list), i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(img_path.split(os.sep)[-1])  # Show filename\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(title)\n",
        "    plt.show(block=True)\n",
        "\n",
        "\n",
        "def main():\n",
        "    random.seed(SEED)\n",
        "\n",
        "\n",
        "    dir = \"LogoDet-3K/LogoDet-3K\"\n",
        "    test_split = 1/10\n",
        "    val_split = 1/10\n",
        "\n",
        "    train_data_list, val_data_list, test_data_list = getPathsSetsByBrand(dir, val_split, test_split, 1000, 15)\n",
        "\n",
        "    print(f\"images in the training set: {len(train_data_list)}\")\n",
        "    print(f\"images in the validation set: {len(val_data_list)}\")\n",
        "    print(f\"images in the test set: {len(test_data_list)}\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = DatasetTriplet(train_data_list, transform)\n",
        "    val_dataset = DatasetTriplet(val_data_list, transform)\n",
        "    test_dataset = DatasetTriplet(test_data_list, transform)\n",
        "\n",
        "    sample_idx = random.randint(0, len(train_dataset) - 1)\n",
        "    anchor, positive, negative = train_dataset[sample_idx]\n",
        "\n",
        "    def show_triplet_with_bboxes(anchor, positive, negative):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        for i, item in enumerate([anchor, positive, negative]):\n",
        "            img = item[\"image\"]\n",
        "            img = img.permute(1, 2, 0).numpy()\n",
        "            img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]  # unnormalize\n",
        "            img = img.clip(0, 1)\n",
        "\n",
        "            ax = plt.subplot(1, 3, i+1)\n",
        "            ax.imshow(img)\n",
        "            plt.title([\"Anchor\", \"Positive\", \"Negative\"][i] + \"\\n\" + \", \".join(item[\"labels\"]))\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Overlay bounding boxes\n",
        "            for bbox in item[\"bbs\"]:\n",
        "                # Scale bounding box to resized image (224x224)\n",
        "                x_scale = 224 / img.shape[1]\n",
        "                y_scale = 224 / img.shape[0]\n",
        "                rect = patches.Rectangle(\n",
        "                    (bbox[\"xmin\"] * x_scale, bbox[\"ymin\"] * y_scale),\n",
        "                    (bbox[\"xmax\"] - bbox[\"xmin\"]) * x_scale,\n",
        "                    (bbox[\"ymax\"] - bbox[\"ymin\"]) * y_scale,\n",
        "                    linewidth=2, edgecolor='r', facecolor='none'\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "        plt.suptitle(\"Sample Triplet with Bounding Boxes\")\n",
        "        plt.show()\n",
        "\n",
        "    show_triplet_with_bboxes(anchor, positive, negative)\n",
        "\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "#def diagnostic_main():\n",
        "#    print(\"--- [1] INITIALIZATION & SETUP ---\")\n",
        "#    random.seed(SEED)\n",
        "#    torch.manual_seed(SEED)\n",
        "#\n",
        "#    # Path to your dataset\n",
        "#    dataset_dir = \"LogoDet-3K/LogoDet-3K\"\n",
        "#\n",
        "#    # Check if directory exists before proceeding to avoid crash\n",
        "#    if not os.path.exists(dataset_dir):\n",
        "#        print(f\"ERROR: Directory '{dataset_dir}' not found. Please adjust the path.\")\n",
        "#        return\n",
        "#\n",
        "#    print(f\"Data Source: {dataset_dir}\")\n",
        "#\n",
        "#    # --- FEATURE TEST 1: Class-Disjoint Splitting ---\n",
        "#    print(\"\\n--- [2] TESTING SPLIT LOGIC (Feature: Zero-Overlap Brands) ---\")\n",
        "#\n",
        "#    # Using strict parameters to force the logic to work hard\n",
        "#    train_paths, val_paths, test_paths = getPathsSetsByBrand(\n",
        "#        dataset_dir,\n",
        "#        val_split=0.1,\n",
        "#        test_split=0.1,\n",
        "#        total_set_size=500,  # Small size for quick debugging\n",
        "#        min_images_per_brand=5\n",
        "#    )\n",
        "#\n",
        "#    # Extract Brand Names from paths to verify no overlap\n",
        "#    def get_brands(path_list):\n",
        "#        return set([p.replace('\\\\', '/').split('/')[-2] for p in path_list])\n",
        "#\n",
        "#    train_brands = get_brands(train_paths)\n",
        "#    val_brands = get_brands(val_paths)\n",
        "#    test_brands = get_brands(test_paths)\n",
        "#\n",
        "#    print(f\"Unique Brands in Train: {len(train_brands)}\")\n",
        "#    print(f\"Unique Brands in Val:   {len(val_brands)}\")\n",
        "#    print(f\"Unique Brands in Test:  {len(test_brands)}\")\n",
        "#\n",
        "#    # intersection verification\n",
        "#    overlap = train_brands.intersection(test_brands)\n",
        "#    if len(overlap) == 0:\n",
        "#        print(\" SUCCESS: No data leakage. Train and Test brands are completely disjoint.\")\n",
        "#    else:\n",
        "#        print(f\" FAILURE: Data leakage detected! Overlapping brands: {overlap}\")\n",
        "#\n",
        "#    # --- FEATURE TEST 2: Triplet Logic ---\n",
        "#    print(\"\\n--- [3] TESTING TRIPLET INTEGRITY (Feature: A-P same class, A-N different) ---\")\n",
        "#\n",
        "#    transform = transforms.Compose([\n",
        "#        transforms.Resize((224, 224)),\n",
        "#        transforms.ToTensor()\n",
        "#    ])\n",
        "#\n",
        "#    # Create the dataset\n",
        "#    train_ds = DatasetTriplet(train_paths, transform)\n",
        "#\n",
        "#    # Fetch a random sample\n",
        "#    idx = random.randint(0, len(train_ds)-1)\n",
        "#    anchor, positive, negative = train_ds[idx]\n",
        "#\n",
        "#    # Check 1: Anchor and Positive must have the same label\n",
        "#    label_A = anchor['labels'][0]\n",
        "#    label_P = positive['labels'][0]\n",
        "#    label_N = negative['labels'][0]\n",
        "#\n",
        "#    print(f\"Sample Index: {idx}\")\n",
        "#    print(f\"Anchor Label:   {label_A}\")\n",
        "#    print(f\"Positive Label: {label_P}\")\n",
        "#    print(f\"Negative Label: {label_N}\")\n",
        "#\n",
        "#    if label_A == label_P:\n",
        "#        print(\" SUCCESS: Anchor and Positive are the same class.\")\n",
        "#    else:\n",
        "#        print(\" FAILURE: Anchor and Positive mismatch!\")\n",
        "#\n",
        "#    if label_A != label_N:\n",
        "#        print(\" SUCCESS: Anchor and Negative are different classes.\")\n",
        "#    else:\n",
        "#        print(\" FAILURE: Anchor and Negative are the same class (Hard Negative or Error)!\")\n",
        "#\n",
        "#    # --- FEATURE TEST 3: Bounding Box Rescaling ---\n",
        "#    print(\"\\n--- [4] TESTING BBOX RESCALING (Feature: Coordinates matching 224x224) ---\")\n",
        "#\n",
        "#    # The image is resized to 224x224. BBoxes must be within [0, 224].\n",
        "#    bbox_A = anchor['bbs'][0]\n",
        "#    print(f\"Resized BBox coordinates: {bbox_A}\")\n",
        "#\n",
        "#    valid_coords = (0 <= bbox_A['xmin'] < bbox_A['xmax'] <= 224) and \\\n",
        "#                   (0 <= bbox_A['ymin'] < bbox_A['ymax'] <= 224)\n",
        "#\n",
        "#    if valid_coords:\n",
        "#        print(\" SUCCESS: Bounding box coordinates are valid within the 224x224 tensor.\")\n",
        "#    else:\n",
        "#        print(\" FAILURE: Bounding box coordinates are out of bounds or inverted.\")\n",
        "#\n",
        "#    print(\"\\n--- TEST COMPLETE ---\")\n",
        "#\n",
        "#    # Optional: Visualize the verified triplet\n",
        "#    # show_triplet_with_bboxes(anchor, positive, negative)\n",
        "#\n",
        "#if __name__ == \"__main__\":\n",
        "#    diagnostic_main()\n"
      ]
    }
  ]
}