{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2PxXj8JgvT7"
      },
      "source": [
        "# Few-Shot Evaluation Notebook\n",
        "\n",
        "This notebook implements the evaluation logic for a few-shot learning model using a ResNet50 backbone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bib25CWFgvT8"
      },
      "source": [
        "### Imports and configuration settings\n",
        "\n",
        "**Key operations:**\n",
        "1. The libraries needed are imported\n",
        "2. **Config**: Config contains all the configurations necessary to run the script\n",
        "3. Seeding to obtain a repeatable experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3CGki6RgvT9",
        "outputId": "471f8d79-9d67-4f73-cb90-2bfe77c886f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset folder '/content/LogoDet-3K' already exists. Skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "from itertools import cycle\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "class Config:\n",
        "    # 1. SETUP\n",
        "    project_name = \"FewShot_Evaluation\"\n",
        "    seed = 42\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 2. DATASET PATH\n",
        "    dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
        "\n",
        "    # 3. MODEL PARAMETERS\n",
        "    embedding_dim = 128\n",
        "    pretrained = True\n",
        "    freeze_layers = 5\n",
        "    trained_model_path = \"\"\n",
        "\n",
        "    # 4. EVALUATION SETTINGS\n",
        "    prediciton_threashold = 0.5\n",
        "    n_shot = 1\n",
        "    num_episodes = 100\n",
        "\n",
        "torch.manual_seed(Config.seed)\n",
        "random.seed(Config.seed)\n",
        "\n",
        "def setup_dataset(zip_path, extract_to):\n",
        "    \"\"\"\n",
        "    Mounts Google Drive and extracts the dataset if not already present.\n",
        "    \"\"\"\n",
        "    # 1. Mount Google Drive\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # 2. Check if the folder already exists\n",
        "    if os.path.exists(extract_to):\n",
        "        print(f\"Dataset folder '{extract_to}' already exists. Skipping extraction.\")\n",
        "    else:\n",
        "        print(f\"Extracting dataset from {zip_path}...\")\n",
        "        if os.path.exists(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "            print(\"Extraction complete.\")\n",
        "        else:\n",
        "            print(f\"ERROR: Zip file not found at {zip_path}. Check your path.\")\n",
        "\n",
        "setup_dataset(\"/content/drive/MyDrive/LogoDet-3K-divided.zip\", \"/content/LogoDet-3K\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74w2UW_bgvT-"
      },
      "source": [
        "### Dataset and Model Architecture\n",
        "\n",
        "**Key operations:**\n",
        "1. **DatasetTest**: Specialized loader for test images that parses XML files to retrieve label indices.\n",
        "2. **LogoResNet50**: A modified ResNet50 architecture that replaces the final classifier with a layer generating feature embeddings.\n",
        "3. **load_model**: A function that given the path of a saved model loads it and returns it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Ls0Bk17QgvT-"
      },
      "outputs": [],
      "source": [
        "class DatasetTest(Dataset):\n",
        "  def __init__(self, file_list, transform=None):\n",
        "      self.file_list = file_list\n",
        "      self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.file_list)\n",
        "\n",
        "  def load_image(self, image_path):\n",
        "      xml_path = image_path.replace(\".jpg\", \".xml\")\n",
        "      img = Image.open(image_path)\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "\n",
        "      # Parse XML\n",
        "      tree = ET.parse(xml_path)\n",
        "      root = tree.getroot()\n",
        "      objects = root.findall(\"object\")\n",
        "\n",
        "      # Take first label's index only\n",
        "      index_text = objects[0].find(\"index\").text\n",
        "      label_idx = int(index_text)  # Convert string to int\n",
        "\n",
        "      return {\"image\": img, \"label\": label_idx}\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.load_image(self.file_list[idx])\n",
        "\n",
        "class LogoResNet50(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True, num_of_freeze_layer=5, activation_fn=None):\n",
        "        super(LogoResNet50, self).__init__()\n",
        "\n",
        "        # 1. Load Pre-trained Weights\n",
        "        # Initialize the model with weights pretrained on ImageNet for transfer learning\n",
        "        if pretrained:\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "            self.model = models.resnet50(weights=weights)\n",
        "        else:\n",
        "            self.model = models.resnet50(weights=None)\n",
        "\n",
        "        # 2. Modify the Head (Fully Connected Layer)\n",
        "        # We need to produce feature embeddings instead of class probabilities\n",
        "        input_features_fc = self.model.fc.in_features # Typically 2048 for ResNet50\n",
        "\n",
        "        head_layers = []\n",
        "        # Project features to the desired embedding dimension (e.g., 128)\n",
        "        head_layers.append(nn.Linear(input_features_fc, embedding_dim))\n",
        "\n",
        "        # Add an optional activation function if provided\n",
        "        if activation_fn is not None:\n",
        "            head_layers.append(activation_fn)\n",
        "\n",
        "        # Replace the original classifier with our custom embedding head\n",
        "        self.model.fc = nn.Sequential(*head_layers)\n",
        "\n",
        "        # 3. Freezing Management\n",
        "        # Define the blocks here to access them in the freeze method.\n",
        "        # This structure allows progressive freezing/unfreezing strategies\n",
        "        self.blocks = [\n",
        "            ['conv1', 'bn1'],   # Level 1\n",
        "            ['layer1'],         # Level 2\n",
        "            ['layer2'],         # Level 3\n",
        "            ['layer3'],         # Level 4\n",
        "            ['layer4'],         # Level 5: Entire backbone frozen\n",
        "        ]\n",
        "\n",
        "        # Apply the initial freezing configuration\n",
        "        self.freeze_numer_of_layer(num_of_freeze_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def freeze_numer_of_layer(self, num_of_freeze_layer):\n",
        "        \"\"\"\n",
        "        Manages layer freezing for transfer learning strategies.\n",
        "\n",
        "        Args:\n",
        "            num_of_freeze_layer (int):\n",
        "              0   -> All layers unlocked (Full Fine-Tuning)\n",
        "              1-5 -> Progressively freezes the backbone layers from shallow to deep\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: RESET. Unfreeze everything (requires_grad = True).\n",
        "        # This ensures we start from a clean state before applying new constraints.\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # If num is 0, exit immediately (Full Fine-Tuning mode)\n",
        "        if num_of_freeze_layer == 0:\n",
        "            print(\"Configuration: Full Fine-Tuning (All layers are trainable)\")\n",
        "            return\n",
        "\n",
        "        # Safety check to avoid index out of bounds\n",
        "        limit = min(num_of_freeze_layer, len(self.blocks))\n",
        "\n",
        "        frozen_list = []\n",
        "\n",
        "        # STEP 2: Progressively freeze the requested blocks\n",
        "        for i in range(limit):\n",
        "            current_blocks = self.blocks[i]\n",
        "            for block_name in current_blocks:\n",
        "                # Retrieve the layer by name\n",
        "                layer = getattr(self.model, block_name)\n",
        "\n",
        "                # Freeze parameters for this specific block\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                frozen_list.append(block_name)\n",
        "\n",
        "        print(f\"Freezing Level {limit}. Frozen blocks: {frozen_list}\")\n",
        "\n",
        "def load_model(model_path, device):\n",
        "    model = LogoResNet50(embedding_dim=Config.embedding_dim, pretrained=Config.pretrained, num_of_freeze_layer=Config.freeze_layers)\n",
        "    # state = torch.load(model_path)\n",
        "    # model.load_state_dict(state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XARVVcgIgvT-"
      },
      "source": [
        "### Evaluation Utilities\n",
        "\n",
        "**Key operations:**\n",
        "1. **MetricEvaluator**: Calculates standard metrics like Discriminant Ration, mAP, Precision, Recall, and F1 Score.\n",
        "2. **FewShotIterator**: Manages the sampling of N-Shot tasks from the test dataset.\n",
        "3. **getTestPaths**: A function that returns a set of paths from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "n5wdZW04gvT-"
      },
      "outputs": [],
      "source": [
        "class MetricEvaluator:\n",
        "    \"\"\"\n",
        "    A class to calculate evaluation metrics for Few-Shot Learning and Metric Learning.\n",
        "\n",
        "    Implements:\n",
        "    1. Discriminant Ratio (J): Optimized scalar implementation (O(d) memory).\n",
        "    2. Mean Average Precision (mAP): Ranking quality metric.\n",
        "    3. Recall at Fixed Precision (R@P): Operational metric.\n",
        "    4. Precision & Recall: Raw metrics at a specific similarity threshold.\n",
        "    5. F1 Score: Harmonic mean of Precision and Recall.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        \"\"\"\n",
        "        Initialize the evaluator.\n",
        "\n",
        "        Args:\n",
        "            device (str): 'cuda' or 'cpu'. If None, detects automatically.\n",
        "        \"\"\"\n",
        "        if device:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.epsilon = 1e-6  # For numerical stability\n",
        "\n",
        "    def compute_discriminant_ratio(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Calculates the Discriminant Ratio (J) using the optimized Scalar approach.\n",
        "\n",
        "        Theory:\n",
        "            J = Tr(Sb) / Tr(Sw)\n",
        "            Using the Trace Trick: Tr(Sw) = Tr(St) - Tr(Sb)\n",
        "\n",
        "        Args:\n",
        "            embeddings (torch.Tensor): Tensor of shape (Batch_Size, Dimension).\n",
        "            labels (torch.Tensor): Tensor of class labels.\n",
        "\n",
        "        Returns:\n",
        "            float: The Discriminant Ratio score.\n",
        "        \"\"\"\n",
        "        embeddings = embeddings.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # 1. Global Mean Computation\n",
        "        global_mean = embeddings.mean(dim=0)\n",
        "\n",
        "        # 2. Calculate Trace of Total Scatter (St)\n",
        "        # Sum of squared Euclidean distances of all points from the global mean.\n",
        "        tr_st = torch.sum((embeddings - global_mean) ** 2)\n",
        "\n",
        "        # 3. Calculate Trace of Between-Class Scatter (Sb)\n",
        "        tr_sb = 0\n",
        "        unique_classes = torch.unique(labels)\n",
        "\n",
        "        for c in unique_classes:\n",
        "            class_mask = (labels == c)\n",
        "            class_embeddings = embeddings[class_mask]\n",
        "            n_c = class_embeddings.size(0)\n",
        "\n",
        "            if n_c > 0:\n",
        "                mu_c = class_embeddings.mean(dim=0)\n",
        "                tr_sb += n_c * torch.sum((mu_c - global_mean) ** 2)\n",
        "\n",
        "        # 4. Calculate Trace of Within-Class Scatter (Sw)\n",
        "        tr_sw = tr_st - tr_sb\n",
        "\n",
        "        # Calculate J\n",
        "        j_score = tr_sb / (tr_sw + self.epsilon)\n",
        "\n",
        "        return j_score.item()\n",
        "\n",
        "    def compute_map(self, query_emb, gallery_emb, query_labels, gallery_labels):\n",
        "        \"\"\"\n",
        "        Calculates Mean Average Precision (mAP).\n",
        "        \"\"\"\n",
        "        query_emb = query_emb.to(self.device)\n",
        "        gallery_emb = gallery_emb.to(self.device)\n",
        "        query_labels = query_labels.to(self.device)\n",
        "        gallery_labels = gallery_labels.to(self.device)\n",
        "\n",
        "        # L2 Normalize for Cosine Similarity\n",
        "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
        "        gallery_emb = F.normalize(gallery_emb, p=2, dim=1)\n",
        "\n",
        "        # Similarity Matrix: S = Q * G^T\n",
        "        similarity_matrix = torch.matmul(query_emb, gallery_emb.T)\n",
        "\n",
        "        num_queries = query_labels.size(0)\n",
        "        average_precisions = []\n",
        "\n",
        "        for i in range(num_queries):\n",
        "            scores = similarity_matrix[i]\n",
        "            target_label = query_labels[i]\n",
        "\n",
        "            # Ranking\n",
        "            sorted_indices = torch.argsort(scores, descending=True)\n",
        "            sorted_gallery_labels = gallery_labels[sorted_indices]\n",
        "\n",
        "            # Relevance Mask\n",
        "            relevance_mask = (sorted_gallery_labels == target_label).float()\n",
        "\n",
        "            total_relevant = relevance_mask.sum()\n",
        "            if total_relevant == 0:\n",
        "                average_precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # Cumulative Precision\n",
        "            cumsum = torch.cumsum(relevance_mask, dim=0)\n",
        "            ranks = torch.arange(1, len(relevance_mask) + 1).to(self.device)\n",
        "            precisions = cumsum / ranks\n",
        "\n",
        "            # Average Precision (AP)\n",
        "            ap = (precisions * relevance_mask).sum() / total_relevant\n",
        "            average_precisions.append(ap.item())\n",
        "\n",
        "        if not average_precisions:\n",
        "            return 0.0\n",
        "        return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "    def compute_precision_recall(self, similarity_scores, is_match, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Calculates raw Precision and Recall at a specific similarity threshold.\n",
        "\n",
        "        Definitions:\n",
        "            Precision = TP / (TP + FP)\n",
        "            Recall    = TP / (TP + FN)\n",
        "\n",
        "        Args:\n",
        "            similarity_scores (torch.Tensor): 1D tensor of scores (0.0 to 1.0).\n",
        "            is_match (torch.Tensor): 1D binary tensor (Ground Truth).\n",
        "            threshold (float): Cutoff for deciding if a retrieval is Positive.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (precision, recall)\n",
        "        \"\"\"\n",
        "        similarity_scores = similarity_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        # Binarize predictions: 1 if score >= threshold (Positive), else 0 (Negative)\n",
        "        predicted_positive = (similarity_scores >= threshold).float()\n",
        "\n",
        "        # True Positives (TP): Predicted Positive AND Actually Match\n",
        "        tp = (predicted_positive * is_match).sum()\n",
        "\n",
        "        # False Positives (FP): Predicted Positive BUT Actually Non-Match\n",
        "        fp = (predicted_positive * (1 - is_match)).sum()\n",
        "\n",
        "        # False Negatives (FN): Predicted Negative BUT Actually Match\n",
        "        # (We invert the prediction mask to find negatives)\n",
        "        fn = ((1 - predicted_positive) * is_match).sum()\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        return precision.item(), recall.item()\n",
        "\n",
        "    def compute_recall_at_fixed_precision(self, similarity_scores, is_match, min_precision=0.95):\n",
        "        \"\"\"\n",
        "        Calculates Recall at a Fixed Precision (R@P).\n",
        "        Finds the lowest threshold where Precision >= min_precision.\n",
        "        \"\"\"\n",
        "        similarity_scores = similarity_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
        "        sorted_matches = is_match[sorted_indices]\n",
        "\n",
        "        tps = torch.cumsum(sorted_matches, dim=0)\n",
        "        total_retrieved = torch.arange(1, len(sorted_matches) + 1).to(self.device)\n",
        "\n",
        "        precisions = tps / total_retrieved\n",
        "\n",
        "        # Find indices where Precision satisfies the constraint\n",
        "        valid_indices = torch.where(precisions >= min_precision)[0]\n",
        "\n",
        "        if len(valid_indices) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cutoff_index = valid_indices[-1]\n",
        "\n",
        "        # Recall = TP_at_cutoff / Total_Relevant_In_Dataset\n",
        "        total_relevant_in_dataset = is_match.sum()\n",
        "\n",
        "        if total_relevant_in_dataset == 0:\n",
        "            return 0.0\n",
        "\n",
        "        recall = tps[cutoff_index] / total_relevant_in_dataset\n",
        "\n",
        "        return recall.item()\n",
        "\n",
        "    def compute_f1_score(self, precision, recall):\n",
        "        \"\"\"\n",
        "        Calculates F1 Score (Harmonic Mean).\n",
        "        \"\"\"\n",
        "        if (precision + recall) == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "class FewShotIterator:\n",
        "    def __init__(self, file_list, n_shot):\n",
        "        \"\"\"\n",
        "        Initializes the iterator class.\n",
        "        It prepares the global testset and creates a cyclic iterator over the valid brands.\n",
        "        \"\"\"\n",
        "        self.n_shot = n_shot\n",
        "\n",
        "        # 1. Validation: Check if input list is empty\n",
        "        if not file_list:\n",
        "            raise ValueError(\"The test file list is empty.\")\n",
        "\n",
        "        #    (Dataset - SupportSet)  is significantly faster with sets (O(1)) compared to lists.\n",
        "        self.all_files_set = set(file_list)\n",
        "\n",
        "        # 3. Organize data by Brand\n",
        "        #    We create a dictionary mapping: { 'BrandName': [list_of_image_paths] }\n",
        "        self.brands_map = {}\n",
        "\n",
        "        for file_path in file_list:\n",
        "            # Extract brand name assuming structure: .../Category/Brand/Image.jpg\n",
        "            brand_name = os.path.basename(os.path.dirname(file_path))\n",
        "\n",
        "            if brand_name not in self.brands_map:\n",
        "                self.brands_map[brand_name] = []\n",
        "            self.brands_map[brand_name].append(file_path)\n",
        "\n",
        "        self.valid_brands_list = list(self.brands_map.keys())\n",
        "\n",
        "        if not self.valid_brands_list:\n",
        "            raise ValueError(f\"No brand found with more than {n_shot} images.\")\n",
        "\n",
        "        #    'itertools.cycle' creates an infinite loop over the valid brands list.\n",
        "        self.brand_iterator = cycle(self.valid_brands_list)\n",
        "\n",
        "    def __call__(self):\n",
        "        \"\"\"\n",
        "        Executed when the class instance is called.\n",
        "        Logic:\n",
        "        1. Pick next brand (Sequential).\n",
        "        2. Pick Support Set (Random 5 images from that brand).\n",
        "        3. Pick Query Set (EVERYTHING else in the testset).\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            # A. Get the next brand sequentially from the cycle\n",
        "            selected_brand_name = next(self.brand_iterator)\n",
        "\n",
        "            # B. Retrieve all images specific to this chosen brand\n",
        "            images_of_current_brand = self.brands_map[selected_brand_name]\n",
        "\n",
        "            # Select a random number between 1 to 5 which is the number of images of the support brand guaranteed in the query set\n",
        "            num_query_guarantee = random.randint(1, 5)\n",
        "\n",
        "            # Check if this brand has enough images\n",
        "            if len(images_of_current_brand) >= self.n_shot + num_query_guarantee:\n",
        "                break  # Brand is suitable, exit the loop\n",
        "            else:\n",
        "                print(f\"The brand {selected_brand_name} wa skipped because it doesnt have enough images to create a query set with {num_query_guarantee} images of the support brand\")\n",
        "\n",
        "        # C. Create SUPPORT SET\n",
        "        #    Select 'n_shot' unique images randomly from the current brand.\n",
        "        support_set_list = random.sample(images_of_current_brand, self.n_shot)\n",
        "        support_set_set = set(support_set_list)\n",
        "\n",
        "        # D. Create QUERY SET (Global Subtraction)\n",
        "        #    Requirement: The Query Set contains 50 images of which at least 1 is from the support brand\n",
        "        #    Step 1: initialize the Query list\n",
        "        query_set_list = []\n",
        "\n",
        "        #    Step 2: Sample randomly the images to guarantee in the query set\n",
        "        remaining_brand_images = list(set(images_of_current_brand) - support_set_set)\n",
        "        guaranteed_images_in_query = random.sample(remaining_brand_images, num_query_guarantee)\n",
        "\n",
        "        #    Step 3: Sample the Negative Queries (Distractors from OTHER brands)\n",
        "        # We subtract ALL images of the current brand to ensure zero accidental matches\n",
        "        remaining_images_in_query = list(self.all_files_set - set(images_of_current_brand) - set(guaranteed_images_in_query))\n",
        "\n",
        "        total_query_size = 50\n",
        "        num_remaining_query = total_query_size - num_query_guarantee\n",
        "        query_remaining = random.sample(remaining_images_in_query,  min(len(remaining_images_in_query),num_remaining_query))\n",
        "\n",
        "        #    Step 4: Combine and Shuffle\n",
        "        query_set_list = guaranteed_images_in_query + query_remaining\n",
        "        random.shuffle(query_set_list)\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"brand_name\": selected_brand_name,\n",
        "            \"support_set\": support_set_list,\n",
        "            \"query_set\": query_set_list\n",
        "        }\n",
        "\n",
        "def getTestPaths(root_dir, total_set_size=None, min_images_per_brand=2):\n",
        "    test_path = os.path.join(root_dir, 'test')\n",
        "    test_brand_list = []\n",
        "\n",
        "    # Collect brand folders\n",
        "    if not os.path.exists(test_path):\n",
        "        print(f\"Warning: {test_path} not found.\")\n",
        "        return []\n",
        "\n",
        "    for category in os.listdir(test_path):\n",
        "        cat_path = os.path.join(test_path, category)\n",
        "        if os.path.isdir(cat_path):\n",
        "            for brand in os.listdir(cat_path):\n",
        "                brand_full_path = os.path.join(cat_path, brand)\n",
        "                if os.path.isdir(brand_full_path):\n",
        "                    test_brand_list.append(brand_full_path)\n",
        "\n",
        "    test_data_list = []\n",
        "\n",
        "    # Sampling Logic\n",
        "    if total_set_size is not None:\n",
        "        images_per_brand = round(total_set_size / len(test_brand_list))\n",
        "\n",
        "        if images_per_brand < min_images_per_brand:\n",
        "            new_test_brand_count = round(total_set_size / min_images_per_brand)\n",
        "            test_brand_list = random.sample(test_brand_list, min(len(test_brand_list), new_test_brand_count))\n",
        "            images_per_brand = min_images_per_brand\n",
        "\n",
        "        for brand in test_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the TEST set\")\n",
        "\n",
        "            test_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "    else:\n",
        "        for brand in test_brand_list:\n",
        "            test_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "\n",
        "    return test_data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3MinJ8igvT_"
      },
      "source": [
        "### Evaluation Loop\n",
        "\n",
        "**Key operations:**\n",
        "1. **evaluate_few_shot**: Runs the evaluation episodes, computes embeddings, and calculates aggregated metrics.\n",
        "2. **cosine_similarity**: A funciton that computes the cosine similarity between two passed embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tKX_RvrDgvT_",
        "outputId": "ae2be77b-838e-4a62-8799-f9daa488ef46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing Level 5. Frozen blocks: ['conv1', 'bn1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
            "images are less than 6 for this brand: LogoDet-3K/LogoDet-3K-divided/test/Clothes/regatta in the TEST set\n",
            "images are less than 6 for this brand: LogoDet-3K/LogoDet-3K-divided/test/Others/stage stores inc in the TEST set\n",
            "images are less than 6 for this brand: LogoDet-3K/LogoDet-3K-divided/test/Others/Carpathia in the TEST set\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([338])\n",
            "query labels: tensor([ 209, 1562,  668,  466,    9,   97, 1540, 2245,  668,  533,  387, 1521,\n",
            "         668, 2458, 1962,  691, 2354, 2147,  209, 1962, 1586, 1471, 1628,  338,\n",
            "        1586,  338,    9, 2245, 2536, 1918, 1096, 1145, 1232, 1474, 1540,  423,\n",
            "        2147, 1995, 2666, 1302,  338, 1093,  149, 2632, 2487,  209,  560,  338,\n",
            "        2567, 2200])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([576])\n",
            "query labels: tensor([1540, 1446,  576,  949, 1628,  576,  912, 2354,  838,  149, 1093, 2841,\n",
            "         182,  504, 1779, 1918, 2023, 1446, 2666,  576,  713,  387, 1562, 2666,\n",
            "        1190, 1586,  949,    9, 2310,  209, 1433, 1433, 2523,  433,  713,  433,\n",
            "         560, 1471, 1540, 2523,  209,  727, 2240, 1521,    9, 2023, 1537,  540,\n",
            "        2536, 2245])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1218])\n",
            "query labels: tensor([1335,  934, 1628,  149, 1471, 1918, 2458, 2567, 1096, 2310, 1145, 2523,\n",
            "         431,  200,  912, 2200,  209, 1537, 2147,  209,  423, 1218,  934, 2741,\n",
            "        1647,  620, 1232,  437, 1537, 2487,  593,  437,  431, 2536, 2536, 1474,\n",
            "        1218, 1218, 1521,  182, 2741, 1218, 1586, 1335, 1962,  668, 1471, 1190,\n",
            "        2536, 1145])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1567])\n",
            "query labels: tensor([1962, 1302, 1456, 2567, 1190, 1190,  433, 2458,  141,  149,  363,  209,\n",
            "         182,  949,  620, 2523, 1562, 1567, 2503,  504, 1190, 2458, 1567,  338,\n",
            "        1567,  576,  912, 2023,  560, 1779,    0, 2741, 2632, 2567, 1093, 1471,\n",
            "         727, 1442,  853, 1096, 1232, 1567, 1567,  209,    0,  338, 1471, 2310,\n",
            "        2495,    0])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1586])\n",
            "query labels: tensor([1456, 2705,  182, 2310, 1059, 2536,  593, 1302, 1779, 1562, 1586, 2567,\n",
            "         149,  560,  141, 1096, 1160, 1190, 1567, 2729, 1647, 1400,    9,  576,\n",
            "        1218, 1537, 2741, 1567, 1218, 1586, 2729, 1537,  466, 1521, 2495, 1586,\n",
            "         560, 2458, 2741, 1442, 1995, 1446, 1586,    9, 2458, 1446, 1145, 1456,\n",
            "         540, 1474])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([149])\n",
            "query labels: tensor([1962,  437,  504, 2354, 1586,  560, 2306, 2503, 1962,  593, 1471, 2023,\n",
            "          72,  533, 2523,  149,  149, 2495, 1218, 1537, 1567,  423,  149, 1567,\n",
            "         668, 1059, 2841, 1562, 1446, 2503,  209, 1446,  149, 2741, 1433, 2741,\n",
            "        2495,  620, 1918, 1474,  691, 2741,    9,  949,  149, 2536,  209,  504,\n",
            "        2632, 1456])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([9])\n",
            "query labels: tensor([2503, 1918, 1647,  838,  593, 2147, 2306, 1628,  338, 1628, 2200, 2523,\n",
            "        1962,  576,  593, 1471,  838, 1145, 1586,  668, 2495, 1093,  620,  713,\n",
            "        1232, 1540, 2841, 1562, 1433, 2666,  387, 2245, 1232, 1471, 1537, 2487,\n",
            "        1628, 2240,  576, 2200, 2503,  363,  423,  713,  540, 1962,    9,  540,\n",
            "        1442,  949])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2523])\n",
            "query labels: tensor([   9, 1446, 2147, 1918, 1471, 2240, 2495, 1190, 2523,   72, 1093, 2147,\n",
            "         182, 1335, 1335,  437, 2306,  838,    9, 2503, 2523, 1232, 1567, 2536,\n",
            "         431,  504, 2523,  560, 2245,  593, 2458,  576, 2741,  466, 2523,   97,\n",
            "        2487,  912,  691, 2741, 1995, 2200, 1400, 1160,  934,  727, 2147, 1442,\n",
            "        2841,  540])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([540])\n",
            "query labels: tensor([ 540,  620, 1779, 1586, 2741,  504,  431, 2245, 1471, 1400, 2023,  668,\n",
            "        1474, 1540, 1335, 1779, 1190,  727, 1096, 1562,  593, 2354, 2306,  949,\n",
            "         363, 2245, 2666, 1521,  853, 2567,  200,    9,  533, 2023,  576,   72,\n",
            "          97,  200,  593,  387,  209,  533, 1562, 2666,  363, 2306, 1400, 2495,\n",
            "        1456, 2536])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1962])\n",
            "query labels: tensor([ 437, 1962, 1995,    0, 1190, 1446,  668,  533,  949,   97, 1456,    0,\n",
            "        1400,  209, 1302,  437,  593, 2245, 1521, 1779, 2536, 1962,  504,  423,\n",
            "        1474, 1218,  431, 1962,  182,  504,  431,  338,  576, 1145,  387, 1474,\n",
            "        1962, 2240, 2147,   72, 1096, 1567, 1232,  504,  934, 1400, 1096, 2240,\n",
            "        1918, 2487])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([713])\n",
            "query labels: tensor([2666, 1400, 1540, 1456,  200,  437,  713,  209, 1628, 2487,  209,  363,\n",
            "        1586, 2240,  853, 1471, 2458, 1190, 2245, 1442, 1647,  149, 1647, 1567,\n",
            "         141, 1537, 1586,  338, 1096, 2306,  149, 1218,   97, 1446,  533,  668,\n",
            "        1400,  338,  620, 1918, 1471,  934, 1218,  912,  713, 2200,  713,  668,\n",
            "         466, 1433])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([593])\n",
            "query labels: tensor([1537, 2741, 1232, 2310, 2354, 2523, 1456, 2503,  593, 1093, 2487,  363,\n",
            "        2567,  387, 1446,  691, 1537, 2495,  209,   72, 1096, 2567,  209, 1232,\n",
            "         149,  504,  912, 2523, 1586,  593, 1145,  949,   97, 1474, 1232, 2567,\n",
            "         533, 1540,  149, 2200, 1521, 2841, 1456, 2200,   72, 1096,  363,  949,\n",
            "        2705,  182])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([97])\n",
            "query labels: tensor([ 504,   97,   97, 1096, 2495, 1059,   97,  387, 2523, 2632,   97, 2495,\n",
            "         182, 2023, 1093,   72, 1562,  149, 2240,  504,  912, 1962,  691,  576,\n",
            "         437, 1190, 2245,    9,  200,  149, 1059, 1442,  338, 1400, 2632, 1537,\n",
            "          72,  620, 1433, 2666, 2310,    0, 1567, 2245, 2354,   97, 1160,  363,\n",
            "        1093, 2705])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1093])\n",
            "query labels: tensor([1567, 2666,  466, 1647, 2245,  149,  713, 1096, 2666, 2200, 2729,  431,\n",
            "        2729, 1456,  912, 1647, 1567,  504, 2503, 2536, 1433, 2310, 1093, 1093,\n",
            "         533,  437,  363,  182, 2023,  838, 1160,  838,  713,  504,  668, 2495,\n",
            "         504, 1433, 2841, 2567, 1446, 2240, 1093,    9, 1093, 2147,   97, 2741,\n",
            "         576, 1335])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([912])\n",
            "query labels: tensor([ 912,  209,  141,  149,  593, 2666, 1962, 1059,  912,  540,  533,  934,\n",
            "        1540,  182, 1628, 1537, 2729, 2310, 2487, 1145, 1218, 1335, 1567,  533,\n",
            "        1093, 2536, 1562,  727,  338,  141,  593,  934,  200, 2458,  727,  200,\n",
            "        1059,  912,  209, 2310,  466, 1442,  912, 1232, 1190, 1586,  387, 2306,\n",
            "        1918,  200])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2495])\n",
            "query labels: tensor([1190, 2495, 2666, 1537,  338, 2023,  363,  141, 1474, 2523, 1190,    9,\n",
            "        1093, 2023,  912, 2523,  533, 1628, 2306,  838, 1540, 1433, 2705,  423,\n",
            "        2245,  363, 1586, 2523,  533,  141, 1145, 1521,  560,  540,  504,  533,\n",
            "        1471, 2495, 2354,  504, 1160, 1537, 1160, 2567, 1586, 2245, 2147,  727,\n",
            "        2705, 1145])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([668])\n",
            "query labels: tensor([ 338, 1218,  338, 2729, 1302, 2729,  727,  691, 1145, 1446, 2567, 2487,\n",
            "        1567, 1628, 2245,  466, 2023,  668, 1562, 2458,  853, 2741,  691,  593,\n",
            "         504, 1096, 2245, 1446,  912,  540,  620,    0, 1586, 1145,  149,    0,\n",
            "        1096, 1145,  363, 1145, 2523,  576, 1442, 2240, 1446, 1302, 1232, 2567,\n",
            "         338, 1433])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1433])\n",
            "query labels: tensor([1456, 1628,  912, 2240, 1521, 2310, 1433, 2666, 1442, 1096, 2741,  576,\n",
            "        1779, 2240,  423, 2567,  466, 1190, 1059, 1433, 1433, 1059, 2841, 1567,\n",
            "        2306,  540,  949,  141,  593,  209, 1647,    9, 1096,  149, 1093, 1995,\n",
            "        2841,  620, 1433,  560,  466, 2729, 1562, 1918,  431,  387, 1567, 1456,\n",
            "         200, 1232])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2147])\n",
            "query labels: tensor([2240, 1537,  668, 2741,  727, 1779, 1918,  437, 2147, 2503,  593,  387,\n",
            "        1628, 1567, 1537, 1446,  504, 2245, 1962,  431, 1093, 2147, 1995,  560,\n",
            "        1059, 2729, 1433,  576,   72, 1562, 2147, 1302, 2841, 2200, 2147, 2023,\n",
            "        1540, 1586, 1446, 1628,  668,    0,  433,   72, 1160, 1918, 2147,   97,\n",
            "         934, 2729])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1442])\n",
            "query labels: tensor([2240, 1442, 2200, 1190, 1521, 2306,  437, 2310,  853, 2458,  504, 1442,\n",
            "        2503, 1628, 1059, 2147, 1962,   97, 1474, 2705, 2310, 1779,  533, 1442,\n",
            "         387,  727, 1628, 2523, 1628, 1302,  182,  149, 1562, 1540,  912, 1647,\n",
            "         423, 1442, 1456, 1335, 2200, 2503, 1059, 1093, 2306,  437,  433, 2245,\n",
            "        1059,  182])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2200])\n",
            "query labels: tensor([ 668, 1537, 2240, 1446, 2666, 2632, 2495, 2666,  691,  182, 1647, 2023,\n",
            "        2536,   72, 2536,  912,  423, 2495, 2523, 2310, 2632, 2245, 1647,  182,\n",
            "        2147,  466, 2200,    9, 1962,  363, 1093,  466, 2487,  504,  149,  209,\n",
            "         593, 1096, 1540,  387, 2200, 1160,  540, 1540, 2147, 1471, 2841, 1433,\n",
            "          72,  209])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([934])\n",
            "query labels: tensor([2245,  934,  363, 1160, 1562, 1446, 2495,  466, 2666, 2666, 1433,  149,\n",
            "        1059, 2705,  504, 1442, 2666, 1471, 2240, 1433, 2523,    0,  853,  713,\n",
            "        2666, 2503, 1096,  433, 1335, 2306,  838, 1456,  533, 2487, 1995,  387,\n",
            "        1628,  593, 1232, 1160, 1995, 1302, 1995,  437,  576,  209, 2458, 2240,\n",
            "         934, 2567])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([691])\n",
            "query labels: tensor([1190, 2523, 1456,  691, 2632, 2536, 1521, 1433,  691, 1537, 1096,  437,\n",
            "        1446, 1586,  620, 1647,  504, 2487,  727,  363,  437,   72, 1995, 2841,\n",
            "         141, 1442, 2306,  466, 1145, 1647,    9,  466, 1190, 1995,  423,  533,\n",
            "        1093, 2705,  713,  209,  727, 1059,  431, 1562,  949, 2245,  431, 2487,\n",
            "         200, 2458])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([433])\n",
            "query labels: tensor([ 433, 2200, 2729,  387, 2632,  363,  620, 2567, 1456,  949,  533, 2023,\n",
            "         576,  200, 1918, 1779, 1471, 2487, 1059,  620, 1456, 1335, 1302, 1446,\n",
            "         433, 2523,  620, 1471, 1995, 2741,  433, 1779, 2487,  387,    9, 1302,\n",
            "          72, 1995, 1962, 2200, 2536, 1995, 2729,  437, 2567,  713, 2310, 1471,\n",
            "        1567,  504])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1537])\n",
            "query labels: tensor([1400,   97, 1160,  853,  200,  576, 2536, 1093, 2487,  593,  593, 1647,\n",
            "        1537, 1145,  433, 2503,  727,  504, 2729, 1093, 1400,  540, 1962,  853,\n",
            "        2503,  668,    0, 1995,  713, 1586,  838, 2632,  620,  209, 2310,  437,\n",
            "        2458,  504, 1218, 1190, 2495, 1586, 2354, 2023, 2200, 1962, 1567, 1093,\n",
            "        1537,  668])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([437])\n",
            "query labels: tensor([1586,  540, 1474,  934, 1160,  727, 2741,   72,  423, 1962, 2200,  912,\n",
            "         437, 1918,  431,  423, 2705, 1471,  727,  141,  912, 1918, 2310, 1540,\n",
            "        2354, 1647,  363, 2458, 2536, 2741,   72, 2632,  949, 2841,    9, 2495,\n",
            "         363, 1918, 1190, 2536,  437, 1628, 1562, 1918,  338,  949,  466,  149,\n",
            "        2200,  668])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1474])\n",
            "query labels: tensor([1647, 1302, 1471, 1562,  209,  200, 2458, 2310, 2729, 1145, 2240, 1586,\n",
            "        1160,  593, 2666,  504,    0,  560, 1446,  423, 1442,  576,  149,    9,\n",
            "         533, 1335, 2306, 2741,  838, 2245,  209,   72, 1628, 2487, 1562,  691,\n",
            "          72, 1918,   97,  576, 1190, 1160, 1962, 1962, 1218, 1474, 1647, 1521,\n",
            "        2523,  182])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1540])\n",
            "query labels: tensor([1400,  576, 1059, 1160,  540,  437,  593,  533, 1962, 2147,  363, 1218,\n",
            "        1540, 2240,  431,  949,  838,  727, 1145,  727, 2741,  423, 1190,  912,\n",
            "        1540,  691, 1474, 1540, 1628,  437,  949,  912, 2632, 2458, 2536, 1442,\n",
            "         949,  431,  949, 1218, 2354, 1471, 1474, 2503, 2705, 1540,  466,  363,\n",
            "        1628, 1586])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2729])\n",
            "query labels: tensor([1918, 2729,  182,  141,   72, 2729, 1995,  466, 1442,    9, 1302, 2354,\n",
            "         668, 2200, 2741, 2458, 1442, 2841,  691, 1433, 1962, 1433,  437, 1537,\n",
            "         853, 2147, 2487,  838, 1145, 2495, 2666,  853, 2523, 2495,  200,  149,\n",
            "         200, 1232,  593,  560,  141,  209, 1521,  934, 1446,  593, 1537,  727,\n",
            "        2147,  423])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([387])\n",
            "query labels: tensor([ 423, 2487,  209, 2487,  338,  853,  387, 2741, 2495, 1647,  540, 1433,\n",
            "         727, 1540, 2458,  540, 2245, 2306, 2023, 1647,  691, 2458, 1628,  838,\n",
            "         387,  149, 2705,  141, 1218, 2729, 1537, 1474,  431,  466, 2632, 1562,\n",
            "        1540, 1567, 1537, 2354, 1059,  387, 1400,  149, 1918,  504,  437, 2495,\n",
            "         949, 1995])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([853])\n",
            "query labels: tensor([1456, 1474,  363, 2310, 1995,  691, 2729,  576,  668, 2147,   97,  182,\n",
            "        1335, 1918, 1567, 2523, 2458, 1442,  838, 2495, 1537, 1442, 2147,  149,\n",
            "         593,  593, 1160,    0, 1521,  560,  853, 2200,  433, 1962,  691,  853,\n",
            "        1562, 1232,  853, 2147,  141, 2741,    9, 2632, 1471,  387, 2487,  533,\n",
            "         200,    0])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1456])\n",
            "query labels: tensor([2705, 2632,  540, 2200, 1218,  363, 1456,  338, 1474,  853, 1471, 1995,\n",
            "         149, 1456,  912, 1456, 1446, 1160, 1456,  533, 1096, 2567,  149, 2240,\n",
            "        1521,  949, 1096, 1779, 1628,  576,  141,  149,  560, 1456,  560,  533,\n",
            "        2245,  149,  433, 2741, 1446,  209, 1218,  423, 2458,  363,  200,  560,\n",
            "           9, 1218])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1232])\n",
            "query labels: tensor([2023, 2567, 1218, 2523, 1160, 2147, 1232, 1918,  853, 2503,  466, 1779,\n",
            "        1190, 1145,  149, 2632,    9, 1995, 1232, 2245,  727,  949, 2495,  387,\n",
            "         466, 1628, 1918, 1779, 1096, 1232, 2147, 2310, 1442,  423, 1093, 2240,\n",
            "        2310, 1562,  437, 1232,  912, 2310, 2306, 2741, 1232, 1562, 2245, 2458,\n",
            "         387, 1160])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([727])\n",
            "query labels: tensor([2023, 2705,  691, 2729,  560, 2200, 1628, 1218,  853, 2495,  200,  949,\n",
            "        2705,  338, 1474, 2458, 2632,   72, 2503, 2503, 2705,  691,  576,  466,\n",
            "        2741, 1962, 1218, 2666, 2023,    0, 2729,  576, 1779,  934, 2458,  727,\n",
            "        1456, 1779, 1779,  560, 1474,  540, 1059, 2523,  200,  387, 2729,    9,\n",
            "         437,  433])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1918])\n",
            "query labels: tensor([ 713, 1302,    9,  209, 1400, 1190,  540, 1096, 2567, 2245, 2458,  423,\n",
            "        1995,   97, 2240, 1096,   72, 1096, 2741, 1918, 2147, 1918, 1442, 2503,\n",
            "        1474,    9,  949,  504,  691, 1433,  727,  504, 1918, 1400, 2666, 1160,\n",
            "        1918, 2841, 1400,  200, 2306,    0, 1059,  691, 2666, 1918,  853,  560,\n",
            "        1442, 2741])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2306])\n",
            "query labels: tensor([1562, 1190, 1093, 2306, 1918, 1190,  149, 2458, 2487, 2306,  141, 2245,\n",
            "        2523,  691,  387, 1537, 1586,  149, 1647, 2666, 1442,  431, 2245, 2200,\n",
            "        2536, 1918,  560,  668,  200, 1190, 1400,  620, 1302,  912, 2741, 2458,\n",
            "        1433,  853,  433, 1093,  504, 1628, 2729, 2147,  182,  838,  141,  593,\n",
            "        2245, 2306])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([200])\n",
            "query labels: tensor([1918, 2567,  576,  437, 2245,    9, 2741, 1537,  200, 1093, 2023, 1456,\n",
            "           0,  387, 2023,  200, 2729,  200, 2240,  620, 1400, 1442,  182,  200,\n",
            "        1567,  466, 2841, 2503,  466,  200, 1059, 1093,  363, 1779, 2567,   72,\n",
            "        2240, 1190, 1779, 2306, 1456, 1059,  668, 1302, 1232, 2666,  560, 1562,\n",
            "        2841, 1474])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2705])\n",
            "query labels: tensor([2523, 1567, 1190,  934, 2354, 1521, 2666, 2632,  141,  540, 2310,  182,\n",
            "         431, 2495,   97, 2306, 2666, 2705,  691,  560, 2841, 1628, 2240, 1628,\n",
            "        1232,  431,    0,  437, 1160,  853, 2023,  560, 2245, 2458,  182,  504,\n",
            "        2729, 2523,  141, 1567, 2023,  691, 2567,  838, 2705,  437, 1400, 2841,\n",
            "        2841, 2729])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([560])\n",
            "query labels: tensor([1442, 2487,  149,  540, 1521,   97, 1962,  691, 1059, 2354,  560, 2567,\n",
            "        2536, 1400,  533,  620, 1059, 1995,  363,  387, 2632, 1218, 2487,  949,\n",
            "         437, 2240,  934,  338, 1995,  387, 1567, 2354, 1442,  949, 1335, 2523,\n",
            "         668, 2354, 1586, 1302, 2536,  433,  466, 2306, 1628, 2503,    0,  540,\n",
            "           9, 1232])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([2458])\n",
            "query labels: tensor([ 433,  504, 1160, 1474, 2841, 1918,  620, 1232,   97,  387, 1521,  853,\n",
            "         691,  727, 1474, 1562,    0, 2523,  200, 1628,  934, 2458,  431, 1918,\n",
            "        1540, 2147,  713, 2741, 2245, 1918, 1647, 2741, 1567, 1335, 1628, 1474,\n",
            "        1647, 1433, 1456, 2354,  560, 1218, 1567, 2705,  200, 1059, 1521,  363,\n",
            "        2487, 2147])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1647])\n",
            "query labels: tensor([ 838,  949, 1647, 1456, 2666, 2495, 1160,  668, 1096, 2495, 2487, 2841,\n",
            "        2240, 2200, 1096, 1145, 1302, 1628,  504, 1335, 1093,  433, 1096, 2567,\n",
            "        2310, 1540, 2666, 1647, 1586, 1647, 1433,  209,  540,  149,  560, 1647,\n",
            "        1540, 1586,    9, 2310,  727, 1096, 2458,  433, 1567, 1400,  437, 1918,\n",
            "        1302, 2666])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1190])\n",
            "query labels: tensor([1918, 1647, 2354, 1918, 1521, 1096, 2240,  934, 1442, 2495, 1190, 1335,\n",
            "        2147,  727,  934, 2487, 1562, 1190,  620,  540,  209, 2200, 2632,  437,\n",
            "         387, 1471, 1647, 2705, 1779,    0, 1190, 2200, 1302, 1567,  533, 1586,\n",
            "          97, 1474, 1160,  668,  182,  437, 1190, 1567, 1302, 2240,  853,   72,\n",
            "         200, 1232])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([1779])\n",
            "query labels: tensor([2310, 2240, 2741,  466, 1218, 1096,  141, 1471,  593,  437, 2536, 2306,\n",
            "           0, 1586,  853, 1446, 2458, 1474, 1567, 1779,  141, 2306, 1335, 1145,\n",
            "        1059, 1540, 2729,  934, 1093, 1628, 2536,  363, 1335, 2741, 1160, 1446,\n",
            "           0,  620,  853, 1190, 1145, 1779, 2354,  691, 1537, 2310,  141, 1586,\n",
            "        1995, 2023])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([141])\n",
            "query labels: tensor([2487, 2354, 2523,   97, 1471, 1302, 1474, 2567,  141, 1521,  182, 1433,\n",
            "        1586,  387,  576, 2503, 1995, 1537,  338, 1962, 1190,  838, 1647, 1540,\n",
            "         363, 2240,  668,  912,  620,  338,    9, 2666, 1779,  141,  504, 1521,\n",
            "        1400, 1918, 1540, 2310, 1647,  141, 1918,  504,  141, 1059, 2495, 1446,\n",
            "        1562, 2458])\n",
            "New iteration ___________________________________\n",
            "support labels: tensor([363])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3187959427.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3187959427.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFewShotIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_shot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_few_shot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n=== Evaluation Results ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3187959427.py\u001b[0m in \u001b[0;36mevaluate_few_shot\u001b[0;34m(model, fewshot_iterator, transform, device, num_episodes)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Compute embeddings for query set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"query labels: {data[\"\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\"]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-465986051.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLogoResNet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-465986051.py\u001b[0m in \u001b[0;36mload_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;31m# Parse XML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2299\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mResampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def evaluate_few_shot(model, fewshot_iterator, transform, device, num_episodes=100):\n",
        "    evaluator = MetricEvaluator(device=device)\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    r_at_95p = []\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 2. Set to eval mode and disable gradient tracking\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_episodes):\n",
        "            print(\"New iteration ___________________________________\")\n",
        "            task = fewshot_iterator()\n",
        "            support_paths = task[\"support_set\"]\n",
        "            query_paths = task[\"query_set\"]\n",
        "\n",
        "            # Build datasets and loaders\n",
        "            support_dataset = DatasetTest(support_paths, transform)\n",
        "            query_dataset = DatasetTest(query_paths, transform)\n",
        "\n",
        "            support_loader = DataLoader(support_dataset, batch_size=32)\n",
        "            query_loader = DataLoader(query_dataset, batch_size=64)\n",
        "\n",
        "            # Extract embeddings\n",
        "            support_embeddings = []\n",
        "            query_embeddings = []\n",
        "            query_labels = []\n",
        "\n",
        "            # Compute embeddings for support set\n",
        "            for data in support_loader:\n",
        "                print(f\"support labels: {data[\"label\"]}\")\n",
        "                images = data[\"image\"].to(device)\n",
        "                support_embeddings.append(model(images))\n",
        "\n",
        "                batch_labels = data[\"label\"]\n",
        "                support_brand = batch_labels[0]\n",
        "\n",
        "            support_embeddings_tensor = torch.cat(support_embeddings)\n",
        "\n",
        "            # Average embeddings\n",
        "            averaged_support_embeddings = support_embeddings_tensor.mean(dim=0)\n",
        "\n",
        "            # Compute embeddings for query set\n",
        "            for data in query_loader:\n",
        "                print(f\"query labels: {data[\"label\"]}\")\n",
        "                images = data[\"image\"].to(device)\n",
        "                query_embeddings.append(model(images))\n",
        "\n",
        "                batch_labels = data[\"label\"]\n",
        "                query_labels.append(batch_labels)\n",
        "\n",
        "            # query_embeddings and query_labels are list of tensors, this unrolls them\n",
        "            query_embeddings_tensor = torch.cat(query_embeddings)\n",
        "            query_labels_tensor = torch.cat(query_labels)\n",
        "\n",
        "            # Compute similarity\n",
        "            sims = cosine_similarity(averaged_support_embeddings, query_embeddings_tensor)\n",
        "\n",
        "            # Ground truth: query belongs to support brand?\n",
        "            gt = (query_labels_tensor == support_brand).float()\n",
        "\n",
        "            # Predictions, does the model predict it is the same brand?\n",
        "            pred = (sims >= Config.prediciton_threashold).float().cpu()\n",
        "\n",
        "            # Accuracy\n",
        "            acc = (pred == gt).float().mean().item()\n",
        "            accuracies.append(acc)\n",
        "\n",
        "            # Precision, Recall, F1\n",
        "            prec, rec = evaluator.compute_precision_recall(sims, gt, threshold=Config.prediciton_threashold)\n",
        "            f1 = evaluator.compute_f1_score(prec, rec)\n",
        "            r95 = evaluator.compute_recall_at_fixed_precision(sims, gt, min_precision=0.95)\n",
        "\n",
        "            precisions.append(prec)\n",
        "            recalls.append(rec)\n",
        "            f1_scores.append(f1)\n",
        "            r_at_95p.append(r95)\n",
        "\n",
        "    # Aggregate results\n",
        "    results = {\n",
        "        \"accuracy\": sum(accuracies) / len(accuracies),\n",
        "        \"precision\": sum(precisions) / len(precisions),\n",
        "        \"recall\": sum(recalls) / len(recalls),\n",
        "        \"f1\": sum(f1_scores) / len(f1_scores),\n",
        "        \"r@95p\": sum(r_at_95p) / len(r_at_95p),\n",
        "    }\n",
        "    return results\n",
        "\n",
        "def cosine_similarity(averaged_support_embeddings, query_embeddings_tensor):\n",
        "\n",
        "    # Normalize embeddings if you want cosine similarity\n",
        "    support_emb_norm = F.normalize(averaged_support_embeddings, p=2, dim=0)       # [embedding_dim]\n",
        "    query_emb_norm = F.normalize(query_embeddings_tensor, p=2, dim=1)             # [num_queries, embedding_dim]\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    sims = torch.matmul(query_emb_norm, support_emb_norm)  # [num_queries]\n",
        "    return sims\n",
        "\n",
        "def main():\n",
        "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),\n",
        "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    device = torch.device(Config.device)\n",
        "    model = load_model(Config.trained_model_path, device)\n",
        "    # model.load_state_dict(torch.load(Config.trained_model_path)) # Uncomment to load trained weights\n",
        "\n",
        "    test_paths = getTestPaths(Config.dataset_root, total_set_size=500, min_images_per_brand=6) # Optionally add total_set_size and min_images_per_brand\n",
        "    iterator = FewShotIterator(test_paths, n_shot=Config.n_shot)\n",
        "\n",
        "    results = evaluate_few_shot(model, iterator, transform, device, Config.num_episodes)\n",
        "    print(\"\\n=== Evaluation Results ===\")\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k.capitalize():<10}: {v:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}