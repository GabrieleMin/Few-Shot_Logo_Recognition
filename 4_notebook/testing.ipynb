{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2PxXj8JgvT7"
      },
      "source": [
        "# Few-Shot Evaluation Notebook\n",
        "\n",
        "This notebook implements the evaluation logic for a few-shot learning model using a ResNet50 backbone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bib25CWFgvT8"
      },
      "source": [
        "### Imports and configuration settings\n",
        "\n",
        "**Key operations:**\n",
        "1. The libraries needed are imported\n",
        "2. **Config**: Config contains all the configurations necessary to run the script\n",
        "3. Seeding to obtain a repeatable experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3CGki6RgvT9",
        "outputId": "3b0b66a1-e16c-4f23-e926-d49fb2eb5a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset folder '/content/LogoDet-3K' already exists. Skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import glob\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "from itertools import cycle\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "class Config:\n",
        "    # 1. SETUP\n",
        "    project_name = \"FewShot_Evaluation\"\n",
        "    seed = 42\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 2. DATASET PATH\n",
        "    dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
        "\n",
        "    # 3. MODEL PARAMETERS\n",
        "    embedding_dim = 128\n",
        "    pretrained = True\n",
        "    freeze_layers = 5\n",
        "    trained_model_path = \"\"\n",
        "\n",
        "    # 4. EVALUATION SETTINGS\n",
        "    prediciton_threashold = 0.5\n",
        "    n_shot = 1\n",
        "    num_episodes = 100\n",
        "\n",
        "torch.manual_seed(Config.seed)\n",
        "random.seed(Config.seed)\n",
        "\n",
        "def setup_dataset(zip_path, extract_to):\n",
        "    \"\"\"\n",
        "    Mounts Google Drive and extracts the dataset if not already present.\n",
        "    \"\"\"\n",
        "    # 1. Mount Google Drive\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # 2. Check if the folder already exists\n",
        "    if os.path.exists(extract_to):\n",
        "        print(f\"Dataset folder '{extract_to}' already exists. Skipping extraction.\")\n",
        "    else:\n",
        "        print(f\"Extracting dataset from {zip_path}...\")\n",
        "        if os.path.exists(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "            print(\"Extraction complete.\")\n",
        "        else:\n",
        "            print(f\"ERROR: Zip file not found at {zip_path}. Check your path.\")\n",
        "\n",
        "setup_dataset(\"/content/drive/MyDrive/LogoDet-3K-divided.zip\", \"/content/LogoDet-3K\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74w2UW_bgvT-"
      },
      "source": [
        "### Dataset and Model Architecture\n",
        "\n",
        "**Key operations:**\n",
        "1. **DatasetTest**: Specialized loader for test images that parses XML files to retrieve label indices.\n",
        "2. **LogoResNet50**: A modified ResNet50 architecture that replaces the final classifier with a layer generating feature embeddings.\n",
        "3. **load_model**: A function that given the path of a saved model loads it and returns it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Ls0Bk17QgvT-"
      },
      "outputs": [],
      "source": [
        "class DatasetTest(Dataset):\n",
        "  def __init__(self, file_list, transform=None):\n",
        "      self.file_list = file_list\n",
        "      self.transform = transform\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.file_list)\n",
        "\n",
        "  def load_image(self, image_path):\n",
        "      xml_path = image_path.replace(\".jpg\", \".xml\")\n",
        "      img = Image.open(image_path)\n",
        "      if self.transform:\n",
        "          img = self.transform(img)\n",
        "\n",
        "      # Parse XML\n",
        "      tree = ET.parse(xml_path)\n",
        "      root = tree.getroot()\n",
        "      objects = root.findall(\"object\")\n",
        "\n",
        "      # Take first label's index only\n",
        "      index_text = objects[0].find(\"index\").text\n",
        "      label_idx = int(index_text)  # Convert string to int\n",
        "\n",
        "      return {\"image\": img, \"label\": label_idx}\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.load_image(self.file_list[idx])\n",
        "\n",
        "class LogoResNet50(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True, num_of_freeze_layer=5, activation_fn=None):\n",
        "        super(LogoResNet50, self).__init__()\n",
        "\n",
        "        # 1. Load Pre-trained Weights\n",
        "        # Initialize the model with weights pretrained on ImageNet for transfer learning\n",
        "        if pretrained:\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "            self.model = models.resnet50(weights=weights)\n",
        "        else:\n",
        "            self.model = models.resnet50(weights=None)\n",
        "\n",
        "        # 2. Modify the Head (Fully Connected Layer)\n",
        "        # We need to produce feature embeddings instead of class probabilities\n",
        "        input_features_fc = self.model.fc.in_features # Typically 2048 for ResNet50\n",
        "\n",
        "        head_layers = []\n",
        "        # Project features to the desired embedding dimension (e.g., 128)\n",
        "        head_layers.append(nn.Linear(input_features_fc, embedding_dim))\n",
        "\n",
        "        # Add an optional activation function if provided\n",
        "        if activation_fn is not None:\n",
        "            head_layers.append(activation_fn)\n",
        "\n",
        "        # Replace the original classifier with our custom embedding head\n",
        "        self.model.fc = nn.Sequential(*head_layers)\n",
        "\n",
        "        # 3. Freezing Management\n",
        "        # Define the blocks here to access them in the freeze method.\n",
        "        # This structure allows progressive freezing/unfreezing strategies\n",
        "        self.blocks = [\n",
        "            ['conv1', 'bn1'],   # Level 1\n",
        "            ['layer1'],         # Level 2\n",
        "            ['layer2'],         # Level 3\n",
        "            ['layer3'],         # Level 4\n",
        "            ['layer4'],         # Level 5: Entire backbone frozen\n",
        "        ]\n",
        "\n",
        "        # Apply the initial freezing configuration\n",
        "        self.freeze_numer_of_layer(num_of_freeze_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def freeze_numer_of_layer(self, num_of_freeze_layer):\n",
        "        \"\"\"\n",
        "        Manages layer freezing for transfer learning strategies.\n",
        "\n",
        "        Args:\n",
        "            num_of_freeze_layer (int):\n",
        "              0   -> All layers unlocked (Full Fine-Tuning)\n",
        "              1-5 -> Progressively freezes the backbone layers from shallow to deep\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: RESET. Unfreeze everything (requires_grad = True).\n",
        "        # This ensures we start from a clean state before applying new constraints.\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # If num is 0, exit immediately (Full Fine-Tuning mode)\n",
        "        if num_of_freeze_layer == 0:\n",
        "            print(\"Configuration: Full Fine-Tuning (All layers are trainable)\")\n",
        "            return\n",
        "\n",
        "        # Safety check to avoid index out of bounds\n",
        "        limit = min(num_of_freeze_layer, len(self.blocks))\n",
        "\n",
        "        frozen_list = []\n",
        "\n",
        "        # STEP 2: Progressively freeze the requested blocks\n",
        "        for i in range(limit):\n",
        "            current_blocks = self.blocks[i]\n",
        "            for block_name in current_blocks:\n",
        "                # Retrieve the layer by name\n",
        "                layer = getattr(self.model, block_name)\n",
        "\n",
        "                # Freeze parameters for this specific block\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                frozen_list.append(block_name)\n",
        "\n",
        "        print(f\"Freezing Level {limit}. Frozen blocks: {frozen_list}\")\n",
        "\n",
        "def load_model(model_path, device):\n",
        "    model = LogoResNet50(embedding_dim=Config.embedding_dim, pretrained=Config.pretrained, num_of_freeze_layer=Config.freeze_layers)\n",
        "    # state = torch.load(model_path)\n",
        "    # model.load_state_dict(state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XARVVcgIgvT-"
      },
      "source": [
        "### Evaluation Utilities\n",
        "\n",
        "**Key operations:**\n",
        "1. **MetricEvaluator**: Calculates standard metrics like Discriminant Ration, mAP, Precision, Recall, and F1 Score.\n",
        "2. **FewShotIterator**: Manages the sampling of N-Shot tasks from the test dataset.\n",
        "3. **getTestPaths**: A function that returns a set of paths from the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "n5wdZW04gvT-"
      },
      "outputs": [],
      "source": [
        "class MetricEvaluator:\n",
        "    \"\"\"\n",
        "    A class to calculate evaluation metrics for Few-Shot Learning and Metric Learning.\n",
        "\n",
        "    Implements:\n",
        "    1. Discriminant Ratio (J): Optimized scalar implementation (O(d) memory).\n",
        "    2. Mean Average Precision (mAP): Ranking quality metric.\n",
        "    3. Recall at Fixed Precision (R@P): Operational metric.\n",
        "    4. Precision & Recall: Raw metrics at a specific similarity threshold.\n",
        "    5. F1 Score: Harmonic mean of Precision and Recall.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        \"\"\"\n",
        "        Initialize the evaluator.\n",
        "\n",
        "        Args:\n",
        "            device (str): 'cuda' or 'cpu'. If None, detects automatically.\n",
        "        \"\"\"\n",
        "        if device:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.epsilon = 1e-6  # For numerical stability\n",
        "\n",
        "    def compute_discriminant_ratio(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Calculates the Discriminant Ratio (J) using the optimized Scalar approach.\n",
        "\n",
        "        Theory:\n",
        "            J = Tr(Sb) / Tr(Sw)\n",
        "            Using the Trace Trick: Tr(Sw) = Tr(St) - Tr(Sb)\n",
        "\n",
        "        Args:\n",
        "            embeddings (torch.Tensor): Tensor of shape (Batch_Size, Dimension).\n",
        "            labels (torch.Tensor): Tensor of class labels.\n",
        "\n",
        "        Returns:\n",
        "            float: The Discriminant Ratio score.\n",
        "        \"\"\"\n",
        "        embeddings = embeddings.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # 1. Global Mean Computation\n",
        "        global_mean = embeddings.mean(dim=0)\n",
        "\n",
        "        # 2. Calculate Trace of Total Scatter (St)\n",
        "        # Sum of squared Euclidean distances of all points from the global mean.\n",
        "        tr_st = torch.sum((embeddings - global_mean) ** 2)\n",
        "\n",
        "        # 3. Calculate Trace of Between-Class Scatter (Sb)\n",
        "        tr_sb = 0\n",
        "        unique_classes = torch.unique(labels)\n",
        "\n",
        "        for c in unique_classes:\n",
        "            class_mask = (labels == c)\n",
        "            class_embeddings = embeddings[class_mask]\n",
        "            n_c = class_embeddings.size(0)\n",
        "\n",
        "            if n_c > 0:\n",
        "                mu_c = class_embeddings.mean(dim=0)\n",
        "                tr_sb += n_c * torch.sum((mu_c - global_mean) ** 2)\n",
        "\n",
        "        # 4. Calculate Trace of Within-Class Scatter (Sw)\n",
        "        tr_sw = tr_st - tr_sb\n",
        "\n",
        "        # Calculate J\n",
        "        j_score = tr_sb / (tr_sw + self.epsilon)\n",
        "\n",
        "        return j_score.item()\n",
        "\n",
        "    def compute_map(self, query_emb, gallery_emb, query_labels, gallery_labels):\n",
        "        \"\"\"\n",
        "        Calculates Mean Average Precision (mAP).\n",
        "        \"\"\"\n",
        "        query_emb = query_emb.to(self.device)\n",
        "        gallery_emb = gallery_emb.to(self.device)\n",
        "        query_labels = query_labels.to(self.device)\n",
        "        gallery_labels = gallery_labels.to(self.device)\n",
        "\n",
        "        # L2 Normalize for Cosine Similarity\n",
        "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
        "        gallery_emb = F.normalize(gallery_emb, p=2, dim=1)\n",
        "\n",
        "        # Similarity Matrix: S = Q * G^T\n",
        "        similarity_matrix = torch.matmul(query_emb, gallery_emb.T)\n",
        "\n",
        "        num_queries = query_labels.size(0)\n",
        "        average_precisions = []\n",
        "\n",
        "        for i in range(num_queries):\n",
        "            scores = similarity_matrix[i]\n",
        "            target_label = query_labels[i]\n",
        "\n",
        "            # Ranking\n",
        "            sorted_indices = torch.argsort(scores, descending=True)\n",
        "            sorted_gallery_labels = gallery_labels[sorted_indices]\n",
        "\n",
        "            # Relevance Mask\n",
        "            relevance_mask = (sorted_gallery_labels == target_label).float()\n",
        "\n",
        "            total_relevant = relevance_mask.sum()\n",
        "            if total_relevant == 0:\n",
        "                average_precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # Cumulative Precision\n",
        "            cumsum = torch.cumsum(relevance_mask, dim=0)\n",
        "            ranks = torch.arange(1, len(relevance_mask) + 1).to(self.device)\n",
        "            precisions = cumsum / ranks\n",
        "\n",
        "            # Average Precision (AP)\n",
        "            ap = (precisions * relevance_mask).sum() / total_relevant\n",
        "            average_precisions.append(ap.item())\n",
        "\n",
        "        if not average_precisions:\n",
        "            return 0.0\n",
        "        return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "    def compute_precision_recall(self, similarity_scores, is_match, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Calculates raw Precision and Recall at a specific similarity threshold.\n",
        "\n",
        "        Definitions:\n",
        "            Precision = TP / (TP + FP)\n",
        "            Recall    = TP / (TP + FN)\n",
        "\n",
        "        Args:\n",
        "            similarity_scores (torch.Tensor): 1D tensor of scores (0.0 to 1.0).\n",
        "            is_match (torch.Tensor): 1D binary tensor (Ground Truth).\n",
        "            threshold (float): Cutoff for deciding if a retrieval is Positive.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (precision, recall)\n",
        "        \"\"\"\n",
        "        similarity_scores = similarity_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        # Binarize predictions: 1 if score >= threshold (Positive), else 0 (Negative)\n",
        "        predicted_positive = (similarity_scores >= threshold).float()\n",
        "\n",
        "        # True Positives (TP): Predicted Positive AND Actually Match\n",
        "        tp = (predicted_positive * is_match).sum()\n",
        "\n",
        "        # False Positives (FP): Predicted Positive BUT Actually Non-Match\n",
        "        fp = (predicted_positive * (1 - is_match)).sum()\n",
        "\n",
        "        # False Negatives (FN): Predicted Negative BUT Actually Match\n",
        "        # (We invert the prediction mask to find negatives)\n",
        "        fn = ((1 - predicted_positive) * is_match).sum()\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        return precision.item(), recall.item()\n",
        "\n",
        "    def compute_recall_at_fixed_precision(self, similarity_scores, is_match, min_precision=0.95):\n",
        "        \"\"\"\n",
        "        Calculates Recall at a Fixed Precision (R@P).\n",
        "        Finds the lowest threshold where Precision >= min_precision.\n",
        "        \"\"\"\n",
        "        similarity_scores = similarity_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
        "        sorted_matches = is_match[sorted_indices]\n",
        "\n",
        "        tps = torch.cumsum(sorted_matches, dim=0)\n",
        "        total_retrieved = torch.arange(1, len(sorted_matches) + 1).to(self.device)\n",
        "\n",
        "        precisions = tps / total_retrieved\n",
        "\n",
        "        # Find indices where Precision satisfies the constraint\n",
        "        valid_indices = torch.where(precisions >= min_precision)[0]\n",
        "\n",
        "        if len(valid_indices) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cutoff_index = valid_indices[-1]\n",
        "\n",
        "        # Recall = TP_at_cutoff / Total_Relevant_In_Dataset\n",
        "        total_relevant_in_dataset = is_match.sum()\n",
        "\n",
        "        if total_relevant_in_dataset == 0:\n",
        "            return 0.0\n",
        "\n",
        "        recall = tps[cutoff_index] / total_relevant_in_dataset\n",
        "\n",
        "        return recall.item()\n",
        "\n",
        "    def compute_f1_score(self, precision, recall):\n",
        "        \"\"\"\n",
        "        Calculates F1 Score (Harmonic Mean).\n",
        "        \"\"\"\n",
        "        if (precision + recall) == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "class FewShotIterator:\n",
        "    def __init__(self, file_list, n_shot):\n",
        "        \"\"\"\n",
        "        Initializes the iterator class.\n",
        "        It prepares the global testset and creates a cyclic iterator over the valid brands.\n",
        "        \"\"\"\n",
        "        self.n_shot = n_shot\n",
        "\n",
        "        # 1. Validation: Check if input list is empty\n",
        "        if not file_list:\n",
        "            raise ValueError(\"The test file list is empty.\")\n",
        "\n",
        "        #    (Dataset - SupportSet)  is significantly faster with sets (O(1)) compared to lists.\n",
        "        self.all_files_set = set(file_list)\n",
        "\n",
        "        # 3. Organize data by Brand\n",
        "        #    We create a dictionary mapping: { 'BrandName': [list_of_image_paths] }\n",
        "        self.brands_map = {}\n",
        "\n",
        "        for file_path in file_list:\n",
        "            # Extract brand name assuming structure: .../Category/Brand/Image.jpg\n",
        "            brand_name = os.path.basename(os.path.dirname(file_path))\n",
        "\n",
        "            if brand_name not in self.brands_map:\n",
        "                self.brands_map[brand_name] = []\n",
        "            self.brands_map[brand_name].append(file_path)\n",
        "\n",
        "        self.valid_brands_list = list(self.brands_map.keys())\n",
        "\n",
        "        if not self.valid_brands_list:\n",
        "            raise ValueError(f\"No brand found with more than {n_shot} images.\")\n",
        "\n",
        "        #    'itertools.cycle' creates an infinite loop over the valid brands list.\n",
        "        self.brand_iterator = cycle(self.valid_brands_list)\n",
        "\n",
        "    def __call__(self):\n",
        "        \"\"\"\n",
        "        Executed when the class instance is called.\n",
        "        Logic:\n",
        "        1. Pick next brand (Sequential).\n",
        "        2. Pick Support Set (Random 5 images from that brand).\n",
        "        3. Pick Query Set (EVERYTHING else in the testset).\n",
        "        \"\"\"\n",
        "        # A. Get the next brand sequentially from the cycle\n",
        "        selected_brand_name = next(self.brand_iterator)\n",
        "\n",
        "        # B. Retrieve all images specific to this chosen brand\n",
        "        images_of_current_brand = self.brands_map[selected_brand_name]\n",
        "\n",
        "        # C. Create SUPPORT SET\n",
        "        #    Select 'n_shot' unique images randomly from the current brand.\n",
        "        support_set_list = random.sample(images_of_current_brand, self.n_shot)\n",
        "\n",
        "        # D. Create QUERY SET (Global Subtraction)\n",
        "        #    Requirement: The Query Set must contain ALL images from the original file_list\n",
        "        #    EXCEPT the ones chosen for the Support Set.\n",
        "        #    Step 1: Convert support list to set for operation\n",
        "        support_set_set = set(support_set_list)\n",
        "        #    Step 2: Mathematical Set Difference: {All Files} - {Support Set}\n",
        "        #    This leaves us with the entire dataset excluding the 5 selected images.\n",
        "        query_set_set = self.all_files_set - support_set_set\n",
        "        #    Step 3: Convert back to list for the return object\n",
        "        query_set_list = list(query_set_set)\n",
        "\n",
        "        return {\n",
        "            \"brand_name\": selected_brand_name,\n",
        "            \"support_set\": support_set_list,\n",
        "            \"query_set\": query_set_list\n",
        "        }\n",
        "\n",
        "def getTestPaths(root_dir, total_set_size=None, min_images_per_brand=2):\n",
        "    test_path = os.path.join(root_dir, 'test')\n",
        "    test_brand_list = []\n",
        "\n",
        "    # Collect brand folders\n",
        "    if not os.path.exists(test_path):\n",
        "        print(f\"Warning: {test_path} not found.\")\n",
        "        return []\n",
        "\n",
        "    for category in os.listdir(test_path):\n",
        "        cat_path = os.path.join(test_path, category)\n",
        "        if os.path.isdir(cat_path):\n",
        "            for brand in os.listdir(cat_path):\n",
        "                brand_full_path = os.path.join(cat_path, brand)\n",
        "                if os.path.isdir(brand_full_path):\n",
        "                    test_brand_list.append(brand_full_path)\n",
        "\n",
        "    test_data_list = []\n",
        "\n",
        "    # Sampling Logic\n",
        "    if total_set_size is not None:\n",
        "        images_per_brand = round(total_set_size / len(test_brand_list))\n",
        "\n",
        "        if images_per_brand < min_images_per_brand:\n",
        "            new_test_brand_count = round(total_set_size / min_images_per_brand)\n",
        "            test_brand_list = random.sample(test_brand_list, min(len(test_brand_list), new_test_brand_count))\n",
        "            images_per_brand = min_images_per_brand\n",
        "\n",
        "        for brand in test_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the TEST set\")\n",
        "\n",
        "            test_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "    else:\n",
        "        for brand in test_brand_list:\n",
        "            test_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "\n",
        "    return test_data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3MinJ8igvT_"
      },
      "source": [
        "### Evaluation Loop\n",
        "\n",
        "**Key operations:**\n",
        "1. **evaluate_few_shot**: Runs the evaluation episodes, computes embeddings, and calculates aggregated metrics.\n",
        "2. **cosine_similarity**: A funciton that computes the cosine similarity between two passed embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKX_RvrDgvT_",
        "outputId": "303aab2a-c929-4d63-83ce-1dc498b029e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing Level 5. Frozen blocks: ['conv1', 'bn1', 'layer1', 'layer2', 'layer3', 'layer4']\n",
            "\n",
            "=== Evaluation Results ===\n",
            "Accuracy  : 0.8581\n",
            "Precision : 0.3378\n",
            "Recall    : 0.2800\n",
            "F1        : 0.2633\n",
            "R@95p     : 0.1960\n"
          ]
        }
      ],
      "source": [
        "def evaluate_few_shot(model, fewshot_iterator, transform, device, num_episodes=100):\n",
        "    evaluator = MetricEvaluator(device=device)\n",
        "    accuracies = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    r_at_95p = []\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 2. Set to eval mode and disable gradient tracking\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_episodes):\n",
        "            task = fewshot_iterator()\n",
        "            support_paths = task[\"support_set\"]\n",
        "            query_paths = task[\"query_set\"]\n",
        "\n",
        "            # Build datasets and loaders\n",
        "            support_dataset = DatasetTest(support_paths, transform)\n",
        "            query_dataset = DatasetTest(query_paths, transform)\n",
        "\n",
        "            support_loader = DataLoader(support_dataset, batch_size=32)\n",
        "            query_loader = DataLoader(query_dataset, batch_size=64)\n",
        "\n",
        "            # Extract embeddings\n",
        "            support_embeddings = []\n",
        "            query_embeddings = []\n",
        "            query_labels = []\n",
        "\n",
        "            # Compute embeddings for support set\n",
        "            for data in support_loader:\n",
        "                images = data[\"image\"].to(device)\n",
        "                support_embeddings.append(model(images))\n",
        "\n",
        "                batch_labels = data[\"label\"]\n",
        "                support_brand = batch_labels[0]\n",
        "\n",
        "            support_embeddings_tensor = torch.cat(support_embeddings)\n",
        "\n",
        "            # Average embeddings\n",
        "            averaged_support_embeddings = support_embeddings_tensor.mean(dim=0)\n",
        "\n",
        "            # Compute embeddings for query set\n",
        "            for data in query_loader:\n",
        "                images = data[\"image\"].to(device)\n",
        "                query_embeddings.append(model(images))\n",
        "\n",
        "                batch_labels = data[\"label\"]\n",
        "                query_labels.append(batch_labels)\n",
        "\n",
        "            # query_embeddings and query_labels are list of tensors, this unrolls them\n",
        "            query_embeddings_tensor = torch.cat(query_embeddings)\n",
        "            query_labels_tensor = torch.cat(query_labels)\n",
        "\n",
        "            # Compute similarity\n",
        "            sims = cosine_similarity(averaged_support_embeddings, query_embeddings_tensor)\n",
        "\n",
        "            # Ground truth: query belongs to support brand?\n",
        "            gt = (query_labels_tensor == support_brand).float()\n",
        "\n",
        "            # Predictions, does the model predict it is the same brand?\n",
        "            pred = (sims >= Config.prediciton_threashold).float().cpu()\n",
        "\n",
        "            # Accuracy\n",
        "            acc = (pred == gt).float().mean().item()\n",
        "            accuracies.append(acc)\n",
        "\n",
        "            # Precision, Recall, F1\n",
        "            prec, rec = evaluator.compute_precision_recall(sims, gt, threshold=Config.prediciton_threashold)\n",
        "            f1 = evaluator.compute_f1_score(prec, rec)\n",
        "            r95 = evaluator.compute_recall_at_fixed_precision(sims, gt, min_precision=0.95)\n",
        "\n",
        "            precisions.append(prec)\n",
        "            recalls.append(rec)\n",
        "            f1_scores.append(f1)\n",
        "            r_at_95p.append(r95)\n",
        "\n",
        "    # Aggregate results\n",
        "    results = {\n",
        "        \"accuracy\": sum(accuracies) / len(accuracies),\n",
        "        \"precision\": sum(precisions) / len(precisions),\n",
        "        \"recall\": sum(recalls) / len(recalls),\n",
        "        \"f1\": sum(f1_scores) / len(f1_scores),\n",
        "        \"r@95p\": sum(r_at_95p) / len(r_at_95p),\n",
        "    }\n",
        "    return results\n",
        "\n",
        "def cosine_similarity(averaged_support_embeddings, query_embeddings_tensor):\n",
        "\n",
        "    # Normalize embeddings if you want cosine similarity\n",
        "    support_emb_norm = F.normalize(averaged_support_embeddings, p=2, dim=0)       # [embedding_dim]\n",
        "    query_emb_norm = F.normalize(query_embeddings_tensor, p=2, dim=1)             # [num_queries, embedding_dim]\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    sims = torch.matmul(query_emb_norm, support_emb_norm)  # [num_queries]\n",
        "    return sims\n",
        "\n",
        "def main():\n",
        "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),\n",
        "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    device = torch.device(Config.device)\n",
        "    model = load_model(Config.trained_model_path, device)\n",
        "    # model.load_state_dict(torch.load(Config.trained_model_path)) # Uncomment to load trained weights\n",
        "\n",
        "    test_paths = getTestPaths(Config.dataset_root, total_set_size=50, min_images_per_brand=6) # Optionally add total_set_size and min_images_per_brand\n",
        "    iterator = FewShotIterator(test_paths, n_shot=Config.n_shot)\n",
        "\n",
        "    results = evaluate_few_shot(model, iterator, transform, device, Config.num_episodes)\n",
        "    print(\"\\n=== Evaluation Results ===\")\n",
        "    for k, v in results.items():\n",
        "        print(f\"{k.capitalize():<10}: {v:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}