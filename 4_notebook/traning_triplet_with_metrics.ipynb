{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6twMq3-_3lo"
      },
      "source": [
        "SCRIPT ANALYSIS: TRIPLET NETWORK TRAINING\n",
        "=================================================================\n",
        "\n",
        "Overview:\n",
        "This script implements a Deep Metric Learning pipeline using a Triplet Network architecture.\n",
        "The goal is to learn a 128-dimensional embedding space where intraclass distances are minimized\n",
        "and interclass distances are maximized via a margin-based ranking loss.\n",
        "\n",
        "Key Components:\n",
        "\n",
        "1. Data Sampling Strategy (DatasetTriplet):\n",
        "   - Efficient Indexing: Pre-computes a `label_to_images` map to enable O(1) retrieval of\n",
        "     positive/negative samples.\n",
        "   - Dynamic Triplet Construction: For every iteration, the dataset generates:\n",
        "     * Anchor: The current image.\n",
        "     * Positive: Randomly sampled from the same class (handles single-image classes by duplication).\n",
        "     * Negative: Randomly sampled from a disjoint class.\n",
        "\n",
        "2. Model Architecture (LogoResNet50):\n",
        "   - Backbone: ResNet50 initialized with ImageNet weights.\n",
        "   - Projection Head: The classification head is replaced by a linear layer projecting\n",
        "     features to `embedding_dim=128`.\n",
        "   - Progressive Fine-Tuning: Includes `freeze_numer_of_layer` logic to selectively freeze\n",
        "     ResNet blocks (from conv1 up to layer4) during transfer learning.\n",
        "\n",
        "3. Optimization Objective:\n",
        "   - Loss Function: `TripletMarginLoss` with `margin=1.0` and `p=2` (Euclidean distance).\n",
        "     The loss ensures: d(a, p) + margin < d(a, n).\n",
        "   - Optimizer: Adam with a conservative learning rate (1e-5) to preserve pretrained feature quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dczxSEDF_3lt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Config:\n",
        "    # 1. SETUP\n",
        "    project_name = \"FewShot\"\n",
        "\n",
        "    # Paths for saving results and checkpoints\n",
        "    logs_dir = \"./logs\"\n",
        "    checkpoints_dir = \"./checkpoints\"\n",
        "\n",
        "    # Device configuration\n",
        "    if torch.backends.mps.is_available():\n",
        "     device = \"mps\"\n",
        "    elif torch.cuda.is_available():\n",
        "     device = \"cuda\"\n",
        "    else:\n",
        "     device = \"cpu\"\n",
        "    seed = 42  # For reproducibility\n",
        "\n",
        "    # 2. DATASET PATH\n",
        "    dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
        "    csv_index_path = \"LogoDet-3K/brand_to_index.csv\"\n",
        "\n",
        "    # Split Ratios: 70% Train, 20% Validation\n",
        "    train_split_ratio = 0.7\n",
        "    val_split_ratio = 0.2\n",
        "\n",
        "    # 3. TRAINING HYPERPARAMETERS\n",
        "    epochs = 20\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-5\n",
        "\n",
        "    # 4. MODEL ARCHITECTURE\n",
        "    backbone = \"resnet50\"\n",
        "    pretrained = True\n",
        "    embedding_dim = 128\n",
        "\n",
        "    # TRAINED MODEL PATH\n",
        "    trained_model_path = \"\"\n",
        "\n",
        "    # Prediciton threadshold used to decide if two logos are the same during inference\n",
        "    prediciton_threashold = 0.5\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    freeze_layers = 0\n",
        "    # Transfer Learning Strategy\n",
        "    freeze_early_layers = True\n",
        "    # Unfreeze all layers after this specific epoch for fine-tuning\n",
        "    unfreeze_at_epoch = 5\n",
        "\n",
        "    # 5. LOSS FUNCTION\n",
        "    margin = 0.2           # Minimal distance between different logos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BBZw7A7_3lt",
        "outputId": "46364267-323d-451f-c1f8-3522499bfd6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset folder '/content/LogoDet-3K' already exists. Skipping extraction.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    def tqdm(iterator, desc=\"\"): return iterator\n",
        "import glob\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "from google.colab import drive\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(Config.seed)\n",
        "random.seed(Config.seed)\n",
        "\n",
        "def setup_dataset(zip_path, extract_to):\n",
        "    \"\"\"\n",
        "    Mounts Google Drive and extracts the dataset if not already present.\n",
        "    \"\"\"\n",
        "    # 1. Mount Google Drive\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # 2. Check if the folder already exists\n",
        "    if os.path.exists(extract_to):\n",
        "        print(f\"Dataset folder '{extract_to}' already exists. Skipping extraction.\")\n",
        "    else:\n",
        "        print(f\"Extracting dataset from {zip_path}...\")\n",
        "        if os.path.exists(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "            print(\"Extraction complete.\")\n",
        "        else:\n",
        "            print(f\"ERROR: Zip file not found at {zip_path}. Check your path.\")\n",
        "\n",
        "setup_dataset(\"/content/drive/MyDrive/LogoDet-3K-divided.zip\", \"/content/LogoDet-3K\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf5F42Ii_3lu"
      },
      "source": [
        "Custom Dataset for Triplet Learning (Anchor, Positive, Negative).\n",
        "\n",
        "Key operations:\n",
        "1. Efficient Indexing: Pre-computes a {label: [paths]} dictionary for fast positive/negative retrieval.\n",
        "2. Triplet Sampling:\n",
        "   - Anchor: Image at current index.\n",
        "   - Positive: Random different image from the same class (handles single-image classes).\n",
        "   - Negative: Random image from a different class.\n",
        "3. Robustness: Includes try-except block to return black fallback images if file loading fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQUZh80J_3lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DatasetTriplet(Dataset):\n",
        "    def __init__(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load label string to index mapping\n",
        "        df = pd.read_csv(Config.csv_index_path)\n",
        "        self.label_to_id = {row['brand']: int(row['index']) for _, row in df.iterrows()}\n",
        "\n",
        "\n",
        "        # --- OTTIMIZZAZIONE ---\n",
        "        # Creiamo un dizionario {label: [lista_di_percorsi]}\n",
        "        # Questo serve per trovare velocemente i positivi e i negativi senza scorrere tutto ogni volta\n",
        "        self.label_to_images = {}\n",
        "        for img_path in image_paths:\n",
        "            # Assumiamo struttura: .../BrandName/img.jpg\n",
        "            # Adatta questo split se le tue cartelle sono diverse!\n",
        "            label = os.path.basename(os.path.dirname(img_path))\n",
        "\n",
        "            if label not in self.label_to_images:\n",
        "                self.label_to_images[label] = []\n",
        "            self.label_to_images[label].append(img_path)\n",
        "\n",
        "        self.labels = list(self.label_to_images.keys())\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 1. ANCHOR (Immagine di partenza)\n",
        "        anchor_path = self.image_paths[index]\n",
        "        anchor_label = os.path.basename(os.path.dirname(anchor_path))\n",
        "        anchor_id = self.label_to_id[anchor_label]\n",
        "\n",
        "        # 2. POSITIVE (Stesso brand, immagine diversa)\n",
        "        potential_positives = self.label_to_images[anchor_label]\n",
        "\n",
        "        # Se c'è solo un'immagine per quel brand (caso limite), usiamo la stessa\n",
        "        if len(potential_positives) > 1:\n",
        "            while True:\n",
        "                pos_path = random.choice(potential_positives)\n",
        "                if pos_path != anchor_path:\n",
        "                    break\n",
        "        else:\n",
        "            pos_path = anchor_path\n",
        "\n",
        "        # 3. NEGATIVE (Brand diverso)\n",
        "        while True:\n",
        "            neg_label = random.choice(self.labels)\n",
        "            if neg_label != anchor_label:\n",
        "                break\n",
        "        neg_id = self.label_to_id[neg_label]\n",
        "        neg_path = random.choice(self.label_to_images[neg_label])\n",
        "\n",
        "        # Caricamento immagini con gestione errori (se un file è corrotto non crasha tutto)\n",
        "        try:\n",
        "            anchor_img = Image.open(anchor_path).convert('RGB')\n",
        "            pos_img = Image.open(pos_path).convert('RGB')\n",
        "            neg_img = Image.open(neg_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Errore caricamento: {e}. Uso immagini nere di fallback.\")\n",
        "            anchor_img = Image.new('RGB', (224, 224))\n",
        "            pos_img = Image.new('RGB', (224, 224))\n",
        "            neg_img = Image.new('RGB', (224, 224))\n",
        "\n",
        "        if self.transform:\n",
        "            anchor_img = self.transform(anchor_img)\n",
        "            pos_img = self.transform(pos_img)\n",
        "            neg_img = self.transform(neg_img)\n",
        "\n",
        "        # Ritorna le 3 immagini + la label (utile per debug)\n",
        "        return anchor_img, pos_img, neg_img, anchor_id, neg_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkaeGSAa_3lu"
      },
      "source": [
        "\n",
        "ResNet50-based architecture modified for Metric Learning (generating embeddings).\n",
        "\n",
        "Key operations:\n",
        "1. Backbone Initialization: Loads a standard ResNet50 (optionally with ImageNet weights).\n",
        "2. Head Replacement: Swaps the original 1000-class classifier with a linear projection layer to output embeddings of size `embedding_dim`.\n",
        "3. Progressive Freezing: Implements a custom `freeze_numer_of_layer` method to selectively freeze backbone blocks (from shallow 'conv1' to deep 'layer4') for controlled fine-tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvuHx3mt_3lu"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LogoResNet50(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True, num_of_freeze_layer=5, activation_fn=None):\n",
        "        super(LogoResNet50, self).__init__()\n",
        "\n",
        "        # 1. Load Pre-trained Weights\n",
        "        # Initialize the model with weights pretrained on ImageNet for transfer learning\n",
        "        if pretrained:\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "            self.model = models.resnet50(weights=weights)\n",
        "        else:\n",
        "            self.model = models.resnet50(weights=None)\n",
        "\n",
        "        # 2. Modify the Head (Fully Connected Layer)\n",
        "        # We need to produce feature embeddings instead of class probabilities\n",
        "        input_features_fc = self.model.fc.in_features # Typically 2048 for ResNet50\n",
        "\n",
        "        head_layers = []\n",
        "        # Project features to the desired embedding dimension (e.g., 128)\n",
        "        head_layers.append(nn.Linear(input_features_fc, embedding_dim))\n",
        "\n",
        "        # Add an optional activation function if provided\n",
        "        if activation_fn is not None:\n",
        "            head_layers.append(activation_fn)\n",
        "\n",
        "        # Replace the original classifier with our custom embedding head\n",
        "        self.model.fc = nn.Sequential(*head_layers)\n",
        "\n",
        "        # 3. Freezing Management\n",
        "        # Define the blocks here to access them in the freeze method.\n",
        "        # This structure allows progressive freezing/unfreezing strategies\n",
        "        self.blocks = [\n",
        "            ['conv1', 'bn1'],   # Level 1\n",
        "            ['layer1'],         # Level 2\n",
        "            ['layer2'],         # Level 3\n",
        "            ['layer3'],         # Level 4\n",
        "            ['layer4'],         # Level 5: Entire backbone frozen\n",
        "        ]\n",
        "\n",
        "        # Apply the initial freezing configuration\n",
        "        self.freeze_numer_of_layer(num_of_freeze_layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def freeze_numer_of_layer(self, num_of_freeze_layer):\n",
        "        \"\"\"\n",
        "        Manages layer freezing for transfer learning strategies.\n",
        "\n",
        "        Args:\n",
        "            num_of_freeze_layer (int):\n",
        "              0   -> All layers unlocked (Full Fine-Tuning)\n",
        "              1-5 -> Progressively freezes the backbone layers from shallow to deep\n",
        "        \"\"\"\n",
        "\n",
        "        # STEP 1: RESET. Unfreeze everything (requires_grad = True).\n",
        "        # This ensures we start from a clean state before applying new constraints.\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # If num is 0, exit immediately (Full Fine-Tuning mode)\n",
        "        if num_of_freeze_layer == 0:\n",
        "            print(\"Configuration: Full Fine-Tuning (All layers are trainable)\")\n",
        "            return\n",
        "\n",
        "        # Safety check to avoid index out of bounds\n",
        "        limit = min(num_of_freeze_layer, len(self.blocks))\n",
        "\n",
        "        frozen_list = []\n",
        "\n",
        "        # STEP 2: Progressively freeze the requested blocks\n",
        "        for i in range(limit):\n",
        "            current_blocks = self.blocks[i]\n",
        "            for block_name in current_blocks:\n",
        "                # Retrieve the layer by name\n",
        "                layer = getattr(self.model, block_name)\n",
        "\n",
        "                # Freeze parameters for this specific block\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                frozen_list.append(block_name)\n",
        "\n",
        "        print(f\"Freezing Level {limit}. Frozen blocks: {frozen_list}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO7s3QN-_3lu"
      },
      "source": [
        "Splits the dataset into training and validation sets at the brand level, ensuring no class overlap.\n",
        "\n",
        "Key operations:\n",
        "1. Brand Separation: Divides brand folders into train/val subsets based on `val_split` using a fixed seed to ensure reproducibility.\n",
        "2. Adaptive Downsampling: If `total_set_size` is enforced, calculates the quota of images per brand. If this falls below `min_images_per_brand`, it reduces the number of participating brands to ensure the remaining ones meet the minimum image count.\n",
        "3. Image Collection: Randomly samples the calculated number of images for each selected brand, or retrieves all images if no total size limit is set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-diGQo5O_3lv"
      },
      "outputs": [],
      "source": [
        "def getTrainValPaths(root_dir, val_split, total_set_size=None, min_images_per_brand=2):\n",
        "    train_val_path = os.path.join(root_dir, 'train_val')\n",
        "    train_val_brands = []\n",
        "\n",
        "    # Collect brand folders\n",
        "    if not os.path.exists(train_val_path):\n",
        "        print(f\"Warning: {train_val_path} not found.\")\n",
        "        return [], []\n",
        "\n",
        "    for category in os.listdir(train_val_path):\n",
        "        cat_path = os.path.join(train_val_path, category)\n",
        "        if os.path.isdir(cat_path):\n",
        "            for brand in os.listdir(cat_path):\n",
        "                brand_full_path = os.path.join(cat_path, brand)\n",
        "                if os.path.isdir(brand_full_path):\n",
        "                    train_val_brands.append(brand_full_path)\n",
        "\n",
        "    # Split brands into Train and Val\n",
        "    val_size = int(len(train_val_brands) * val_split)\n",
        "    train_size = len(train_val_brands) - val_size\n",
        "    generator = torch.Generator().manual_seed(Config.seed)\n",
        "    train_subset, val_subset = random_split(train_val_brands, [train_size, val_size], generator=generator)\n",
        "\n",
        "    train_brand_list = [train_val_brands[i] for i in train_subset.indices]\n",
        "    val_brand_list = [train_val_brands[i] for i in val_subset.indices]\n",
        "\n",
        "    train_data_list = []\n",
        "    val_data_list = []\n",
        "\n",
        "    # Sampling Logic\n",
        "    if total_set_size is not None:\n",
        "        images_per_brand = round(total_set_size / len(train_val_brands))\n",
        "\n",
        "        if images_per_brand < min_images_per_brand:\n",
        "            print(f\"Not enough images per brand ({images_per_brand}), downscaling brand sets to ensure {min_images_per_brand} images/brand.\")\n",
        "\n",
        "            # Calculate how many brands we can actually afford\n",
        "            new_total_brand_count = round(total_set_size / min_images_per_brand)\n",
        "            new_val_size = round(new_total_brand_count * val_split)\n",
        "            new_train_size = new_total_brand_count - new_val_size\n",
        "\n",
        "            train_brand_list = random.sample(train_brand_list, min(len(train_brand_list), new_train_size))\n",
        "            val_brand_list = random.sample(val_brand_list, min(len(val_brand_list), new_val_size))\n",
        "            images_per_brand = min_images_per_brand\n",
        "\n",
        "        for brand in train_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the TRAIN set\")\n",
        "\n",
        "            train_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "\n",
        "        for brand in val_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the VALIDATION set\")\n",
        "\n",
        "            val_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "    else:\n",
        "        for brand in train_brand_list:\n",
        "            train_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "        for brand in val_brand_list:\n",
        "            val_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "\n",
        "    return train_data_list, val_data_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_ZRTHjx_3lv"
      },
      "source": [
        "Define the metric evaluator used to guide training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUxtBIQ6_3lv"
      },
      "outputs": [],
      "source": [
        "class MetricEvaluator:\n",
        "    \"\"\"\n",
        "    A class to calculate evaluation metrics for Few-Shot Learning and Metric Learning.\n",
        "\n",
        "    Implements:\n",
        "    1. Discriminant Ratio (J): Optimized scalar implementation (O(d) memory).\n",
        "    2. Mean Average Precision (mAP): Ranking quality metric.\n",
        "    3. Recall at Fixed Precision (R@P): Operational metric.\n",
        "    4. Precision & Recall: Raw metrics at a specific similarity threshold.\n",
        "    5. F1 Score: Harmonic mean of Precision and Recall.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device=None):\n",
        "        \"\"\"\n",
        "        Initialize the evaluator.\n",
        "\n",
        "        Args:\n",
        "            device (str): 'cuda' or 'cpu'. If None, detects automatically.\n",
        "        \"\"\"\n",
        "        if device:\n",
        "            self.device = device\n",
        "        else:\n",
        "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.epsilon = 1e-6  # For numerical stability\n",
        "\n",
        "    def compute_discriminant_ratio(self, embeddings, labels):\n",
        "        \"\"\"\n",
        "        Calculates the Discriminant Ratio (J) using the optimized Scalar approach.\n",
        "\n",
        "        Theory:\n",
        "            J = Tr(Sb) / Tr(Sw)\n",
        "            Using the Trace Trick: Tr(Sw) = Tr(St) - Tr(Sb)\n",
        "\n",
        "        Args:\n",
        "            embeddings (torch.Tensor): Tensor of shape (Batch_Size, Dimension).\n",
        "            labels (torch.Tensor): Tensor of class labels.\n",
        "\n",
        "        Returns:\n",
        "            float: The Discriminant Ratio score.\n",
        "        \"\"\"\n",
        "        embeddings = embeddings.to(self.device)\n",
        "        labels = labels.to(self.device)\n",
        "\n",
        "        # 1. Global Mean Computation\n",
        "        global_mean = embeddings.mean(dim=0)\n",
        "\n",
        "        # 2. Calculate Trace of Total Scatter (St)\n",
        "        # Sum of squared Euclidean distances of all points from the global mean.\n",
        "        tr_st = torch.sum((embeddings - global_mean) ** 2)\n",
        "\n",
        "        # 3. Calculate Trace of Between-Class Scatter (Sb)\n",
        "        tr_sb = 0\n",
        "        unique_classes = torch.unique(labels)\n",
        "\n",
        "        for c in unique_classes:\n",
        "            class_mask = (labels == c)\n",
        "            class_embeddings = embeddings[class_mask]\n",
        "            n_c = class_embeddings.size(0)\n",
        "\n",
        "            if n_c > 0:\n",
        "                mu_c = class_embeddings.mean(dim=0)\n",
        "                tr_sb += n_c * torch.sum((mu_c - global_mean) ** 2)\n",
        "\n",
        "        # 4. Calculate Trace of Within-Class Scatter (Sw)\n",
        "        tr_sw = tr_st - tr_sb\n",
        "\n",
        "        # Calculate J\n",
        "        j_score = tr_sb / (tr_sw + self.epsilon)\n",
        "\n",
        "        return j_score.item()\n",
        "\n",
        "    def compute_map(self, query_emb, gallery_emb, query_labels, gallery_labels):\n",
        "        \"\"\"\n",
        "        Calculates Mean Average Precision (mAP).\n",
        "        \"\"\"\n",
        "        query_emb = query_emb.to(self.device)\n",
        "        gallery_emb = gallery_emb.to(self.device)\n",
        "        query_labels = query_labels.to(self.device)\n",
        "        gallery_labels = gallery_labels.to(self.device)\n",
        "\n",
        "        # L2 Normalize for Cosine Similarity\n",
        "        query_emb = F.normalize(query_emb, p=2, dim=1)\n",
        "        gallery_emb = F.normalize(gallery_emb, p=2, dim=1)\n",
        "\n",
        "        # Similarity Matrix: S = Q * G^T\n",
        "        similarity_matrix = torch.matmul(query_emb, gallery_emb.T)\n",
        "\n",
        "        num_queries = query_labels.size(0)\n",
        "        average_precisions = []\n",
        "\n",
        "        for i in range(num_queries):\n",
        "            scores = similarity_matrix[i]\n",
        "            target_label = query_labels[i]\n",
        "\n",
        "            # Ranking\n",
        "            sorted_indices = torch.argsort(scores, descending=True)\n",
        "            sorted_gallery_labels = gallery_labels[sorted_indices]\n",
        "\n",
        "            # Relevance Mask\n",
        "            relevance_mask = (sorted_gallery_labels == target_label).float()\n",
        "\n",
        "            total_relevant = relevance_mask.sum()\n",
        "            if total_relevant == 0:\n",
        "                average_precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            # Cumulative Precision\n",
        "            cumsum = torch.cumsum(relevance_mask, dim=0)\n",
        "            ranks = torch.arange(1, len(relevance_mask) + 1).to(self.device)\n",
        "            precisions = cumsum / ranks\n",
        "\n",
        "            # Average Precision (AP)\n",
        "            ap = (precisions * relevance_mask).sum() / total_relevant\n",
        "            average_precisions.append(ap.item())\n",
        "\n",
        "        if not average_precisions:\n",
        "            return 0.0\n",
        "        return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "    def compute_precision_recall(self, similarity_scores, is_match, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Calculates raw Precision and Recall at a specific similarity threshold.\n",
        "\n",
        "        Definitions:\n",
        "            Precision = TP / (TP + FP)\n",
        "            Recall    = TP / (TP + FN)\n",
        "\n",
        "        Args:\n",
        "            similarity_scores (torch.Tensor): 1D tensor of scores (0.0 to 1.0).\n",
        "            is_match (torch.Tensor): 1D binary tensor (Ground Truth).\n",
        "            threshold (float): Cutoff for deciding if a retrieval is Positive.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (precision, recall)\n",
        "        \"\"\"\n",
        "        similarity_scores = similarity_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        # Binarize predictions: 1 if score >= threshold (Positive), else 0 (Negative)\n",
        "        predicted_positive = (similarity_scores >= threshold).float()\n",
        "\n",
        "        # True Positives (TP): Predicted Positive AND Actually Match\n",
        "        tp = (predicted_positive * is_match).sum()\n",
        "\n",
        "        # False Positives (FP): Predicted Positive BUT Actually Non-Match\n",
        "        fp = (predicted_positive * (1 - is_match)).sum()\n",
        "\n",
        "        # False Negatives (FN): Predicted Negative BUT Actually Match\n",
        "        # (We invert the prediction mask to find negatives)\n",
        "        fn = ((1 - predicted_positive) * is_match).sum()\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "\n",
        "        return precision.item(), recall.item()\n",
        "\n",
        "    def compute_recall_at_fixed_precision(self, similarity_scores, is_match, min_precision=0.95):\n",
        "        \"\"\"\n",
        "        Calculates Recall at a Fixed Precision (R@P).\n",
        "        Finds the lowest threshold where Precision >= min_precision.\n",
        "        \"\"\"\n",
        "        similarity_scores = similarity_scores.to(self.device)\n",
        "        is_match = is_match.to(self.device)\n",
        "\n",
        "        sorted_indices = torch.argsort(similarity_scores, descending=True)\n",
        "        sorted_matches = is_match[sorted_indices]\n",
        "\n",
        "        tps = torch.cumsum(sorted_matches, dim=0)\n",
        "        total_retrieved = torch.arange(1, len(sorted_matches) + 1).to(self.device)\n",
        "\n",
        "        precisions = tps / total_retrieved\n",
        "\n",
        "        # Find indices where Precision satisfies the constraint\n",
        "        valid_indices = torch.where(precisions >= min_precision)[0]\n",
        "\n",
        "        if len(valid_indices) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        cutoff_index = valid_indices[-1]\n",
        "\n",
        "        # Recall = TP_at_cutoff / Total_Relevant_In_Dataset\n",
        "        total_relevant_in_dataset = is_match.sum()\n",
        "\n",
        "        if total_relevant_in_dataset == 0:\n",
        "            return 0.0\n",
        "\n",
        "        recall = tps[cutoff_index] / total_relevant_in_dataset\n",
        "\n",
        "        return recall.item()\n",
        "\n",
        "    def compute_f1_score(self, precision, recall):\n",
        "        \"\"\"\n",
        "        Calculates F1 Score (Harmonic Mean).\n",
        "        \"\"\"\n",
        "        if (precision + recall) == 0:\n",
        "            return 0.0\n",
        "        return 2 * (precision * recall) / (precision + recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-laJfD1_3lw"
      },
      "source": [
        "Training script for a Triplet Network (Anchor, Positive, Negative).\n",
        "\n",
        "Key operations:\n",
        "1. Data Pipeline:\n",
        "   - Splits data into train/val and applies heavy augmentation (ColorJitter, Flip) to training data.\n",
        "   - Initializes `DatasetTriplet` which yields triplets of images.\n",
        "2. Model Setup: Initializes `LogoResNet50` on the specified device.\n",
        "3. Optimization:\n",
        "   - Loss: Uses `TripletMarginLoss` (margin=1.0) to ensure the Anchor is closer to the Positive than the Negative by at least the margin.\n",
        "   - Optimizer: Adam with a low learning rate (1e-5).\n",
        "4. Training Loop: Feeds triplets into the network to generate three embeddings, calculates loss, and updates weights.\n",
        "5. Monitoring: Evaluates on the validation set and saves the 'best' model (lowest validation loss) and periodic checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEddYV6o_3lw"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(emb1, emb2):\n",
        "    \"\"\"\n",
        "    1. Triplet (Batch-to-Batch): emb1 [N, D], emb2 [N, D] -> returns [N]\n",
        "    \"\"\"\n",
        "    # Normalize along the embedding dimension\n",
        "    emb1_norm = F.normalize(emb1, p=2, dim=-1)\n",
        "    emb2_norm = F.normalize(emb2, p=2, dim=-1)\n",
        "\n",
        "    # Now return the dot product\n",
        "    return (emb1_norm * emb2_norm).sum(dim=-1)\n",
        "\n",
        "def train_triplet():\n",
        "\n",
        "    save_dir = os.path.join(\"checkpoints\", \"triplet_run\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    device = torch.device(Config.device)\n",
        "\n",
        "    metrics_to_plot = {\n",
        "        \"loss\": [], \"mAP\": [], \"J\": [], \"accuracy\": [],\n",
        "        \"precision\": [], \"recall\": [], \"f1\": [], \"r@95p\": []\n",
        "    }\n",
        "\n",
        "    # 1. Dataset e Dataloader\n",
        "\n",
        "    train_files, val_files = getTrainValPaths(\n",
        "        Config.dataset_root,\n",
        "        val_split=Config.val_split_ratio,\n",
        "        total_set_size=100,\n",
        "        min_images_per_brand=2\n",
        "    )\n",
        "\n",
        "    # Transformations\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = DatasetTriplet(train_files, transform=train_transform)\n",
        "    val_dataset = DatasetTriplet(val_files, transform=val_transform)\n",
        "\n",
        "    # Dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 2. Model\n",
        "    print(\"Model Initialization (Triplet)...\")\n",
        "    # Using Freeze=0\n",
        "    model = LogoResNet50(embedding_dim=Config.embedding_dim, pretrained=Config.pretrained, num_of_freeze_layer=Config.freeze_layers)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 3. Loss e Optimizer\n",
        "    # Margin 1.0\n",
        "    criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "\n",
        "    # Optimizer\n",
        "    # Using 0.00001 (1e-5)\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001)\n",
        "    # 4. Training Loop\n",
        "    best_val_loss = float('inf')\n",
        "    num_epochs = 100\n",
        "\n",
        "    print(f\"Starting training Triplet for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Progress bar\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # The dataset returns: anchor, positive, negative, label\n",
        "        for anchor, positive, negative, _, _ in pbar:\n",
        "            anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass triplo\n",
        "            out_a = model(anchor)\n",
        "            out_p = model(positive)\n",
        "            out_n = model(negative)\n",
        "\n",
        "            # Calculate Loss\n",
        "            loss = criterion(out_a, out_p, out_n)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # --- VALIDATION ---\n",
        "        model.eval()\n",
        "        evaluator = MetricEvaluator(device=device)\n",
        "\n",
        "        # Metric accumulators\n",
        "        accuracies = []\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "        f1_scores = []\n",
        "        r_at_95p = []\n",
        "\n",
        "        # For Discriminant Ratio\n",
        "        j_embeddings = []\n",
        "        j_labels = []\n",
        "\n",
        "        # For mAP\n",
        "        map_scores = []\n",
        "        # For loss\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for anchor, positive, negative, anchor_label, neg_label in val_loader:\n",
        "                anchor, positive, negative, anchor_label, neg_label = anchor.to(device), positive.to(device), negative.to(device), anchor_label.to(device), neg_label.to(device)\n",
        "\n",
        "                out_a = model(anchor)\n",
        "                out_p = model(positive)\n",
        "                out_n = model(negative)\n",
        "\n",
        "                # Compute validation loss\n",
        "                loss = criterion(out_a, out_p, out_n)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Discrimination ratio J\n",
        "                j_embeddings.append(out_a)\n",
        "                j_embeddings.append(out_p)\n",
        "                j_embeddings.append(out_n)\n",
        "                j_labels.append(anchor_label)\n",
        "                j_labels.append(anchor_label)\n",
        "                j_labels.append(neg_label)\n",
        "\n",
        "                # mAP\n",
        "                qa = F.normalize(out_a, dim=1)\n",
        "                gp = F.normalize(out_p, dim=1)\n",
        "                gn = F.normalize(out_n, dim=1)\n",
        "\n",
        "                gallery_emb = torch.cat([gp, gn], dim=0)\n",
        "                gallery_labels = torch.cat([anchor_label, neg_label])\n",
        "\n",
        "                batch_map = evaluator.compute_map(\n",
        "                    query_emb=qa,\n",
        "                    gallery_emb=gallery_emb,\n",
        "                    query_labels=anchor_label,\n",
        "                    gallery_labels=gallery_labels\n",
        "                )\n",
        "                map_scores.append(batch_map)\n",
        "\n",
        "                # Similarities scores\n",
        "                sim_pos = cosine_similarity(out_p, out_a)\n",
        "                sim_neg = cosine_similarity(out_n, out_a)\n",
        "\n",
        "                sims = torch.cat([sim_pos, sim_neg], dim=0)\n",
        "                gt = torch.cat([\n",
        "                    torch.ones(len(sim_pos), device=device),\n",
        "                    torch.zeros(len(sim_neg), device=device)\n",
        "                ])\n",
        "\n",
        "                pred = (sims >= Config.prediciton_threashold).float()\n",
        "\n",
        "                # Accuracy\n",
        "                acc = (pred == gt).float().mean().item()\n",
        "                accuracies.append(acc)\n",
        "\n",
        "                # Precision\n",
        "                prec, rec = evaluator.compute_precision_recall(\n",
        "                    sims, gt, threshold=Config.prediciton_threashold\n",
        "                )\n",
        "\n",
        "                # F1 score\n",
        "                f1 = evaluator.compute_f1_score(prec, rec)\n",
        "\n",
        "                # Recall at 95 precision\n",
        "                r95 = evaluator.compute_recall_at_fixed_precision(\n",
        "                    sims, gt, min_precision=0.95\n",
        "                )\n",
        "\n",
        "                precisions.append(prec)\n",
        "                recalls.append(rec)\n",
        "                f1_scores.append(f1)\n",
        "                r_at_95p.append(r95)\n",
        "\n",
        "\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        j_embeddings_tensor = torch.cat(j_embeddings, dim=0)\n",
        "        j_labels_tensor = torch.cat(j_labels, dim=0)\n",
        "        j_score = evaluator.compute_discriminant_ratio(j_embeddings_tensor, j_labels_tensor)\n",
        "\n",
        "        val_results = {\n",
        "            \"loss\": avg_val_loss,\n",
        "            \"mAP\": sum(map_scores) / len(map_scores),\n",
        "            \"J\": j_score,\n",
        "            \"accuracy\": sum(accuracies) / len(accuracies),\n",
        "            \"precision\": sum(precisions) / len(precisions),\n",
        "            \"recall\": sum(recalls) / len(recalls),\n",
        "            \"f1\": sum(f1_scores) / len(f1_scores),\n",
        "            \"r@95p\": sum(r_at_95p) / len(r_at_95p),\n",
        "        }\n",
        "\n",
        "        for key in metrics_to_plot.keys():\n",
        "            metrics_to_plot[key].append(val_results[key])\n",
        "\n",
        "        print(\n",
        "            f\"VALIDATION Epoch {epoch+1} | \"\n",
        "            f\"Loss: {val_results['loss']:.4f} | \"\n",
        "            f\"Acc: {val_results['accuracy']:.4f} | \"\n",
        "            f\"Prec: {val_results['precision']:.4f} | \"\n",
        "            f\"Rec: {val_results['recall']:.4f} | \"\n",
        "            f\"F1: {val_results['f1']:.4f} | \"\n",
        "            f\"R@95P: {val_results['r@95p']:.4f} | \"\n",
        "            f\"mAP: {val_results['mAP']:.4f} | \"\n",
        "            f\"J: {val_results['J']:.4f}\"\n",
        "        )\n",
        "\n",
        "\n",
        "        # Saving checkpoint\n",
        "        checkpoint_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        # Saving Best Model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), os.path.join(save_dir, \"best_model_triplet.pth\"))\n",
        "            print(\"New Best Triplet Model Saved!\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "    plot_training_results(metrics_to_plot, save_dir)\n",
        "\n",
        "def plot_training_results(history, save_dir):\n",
        "    epochs = range(1, len(history['loss']) + 1)\n",
        "\n",
        "    # Create a 2x4 grid to fit all 8 metrics\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'cyan', 'magenta']\n",
        "\n",
        "    for i, (key, values) in enumerate(history.items()):\n",
        "        axes[i].plot(epochs, values, marker='o', color=colors[i], linestyle='-', linewidth=2)\n",
        "        axes[i].set_title(key.upper(), fontsize=14, fontweight='bold')\n",
        "        axes[i].set_xlabel('Epoch')\n",
        "        axes[i].set_ylabel('Score')\n",
        "        axes[i].grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(save_dir, \"metric_plots.png\")\n",
        "    plt.savefig(plot_path)\n",
        "    plt.show() # This will display the plot if you are in a Notebook/IDE\n",
        "    print(f\"Metrics plot saved to: {plot_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_triplet()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
