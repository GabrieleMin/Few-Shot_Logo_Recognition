{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Contrastive Learning\n",
        "---------------------------------------------------\n",
        "This script implements training via a Siamese Network.\n",
        "\n",
        "- Training Logic: The model processes pairs of images with a binary label (0/1).\n",
        "\n",
        "- Objective (Loss): Uses `CosineEmbeddingLoss`.\n",
        "  Maximizes cosine similarity for positive pairs and minimizes it for negative ones.\n",
        "\n",
        "- Data Management: The `DatasetContrastive` class has two distinct features:\n",
        "  1. Supports XML parsing to extract bounding boxes and crop objects.\n",
        "  2. Enforces a 50/50 balance between positive and negative pairs to avoid statistical bias.\n",
        "\n",
        "LogoResNet50\n",
        "----------------------------------\n",
        "`LogoResNet50` class:\n",
        "- Backbone: Pre-trained ResNet50.\n",
        "- Head: Replaces the classifier with a linear layer for 128-dimensional embedding output.\n",
        "- Fine-tuning: `freeze_numer_of_layer` method for progressive block freezing.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class Config:\n",
        "    # 1. SETUP\n",
        "    project_name = \"FewShot\"\n",
        "    \n",
        "    # Paths for saving results and checkpoints\n",
        "    logs_dir = \"./logs\"\n",
        "    checkpoints_dir = \"./checkpoints\"\n",
        "    \n",
        "    # Device configuration\n",
        "    if torch.backends.mps.is_available():\n",
        "     device = \"mps\"\n",
        "    elif torch.cuda.is_available():\n",
        "     device = \"cuda\"\n",
        "    else:\n",
        "     device = \"cpu\"\n",
        "    seed = 42  # For reproducibility\n",
        "\n",
        "    # 2. DATASET PATH\n",
        "    dataset_root = \"LogoDet-3K/LogoDet-3K-divided\"\n",
        "    csv_index_path = \"LogoDet-3K\"\n",
        "\n",
        "    # Split Ratios: 70% Train, 20% Validation \n",
        "    train_split_ratio = 0.7\n",
        "    val_split_ratio = 0.2\n",
        "\n",
        "    # 3. TRAINING HYPERPARAMETERS\n",
        "    epochs = 20\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-5\n",
        "\n",
        "    # 4. MODEL ARCHITECTURE\n",
        "    backbone = \"resnet50\" \n",
        "    pretrained = True     \n",
        "    embedding_dim = 128    \n",
        "\n",
        "    # TRAINED MODEL PATH\n",
        "    trained_model_path = \"\"\n",
        "\n",
        "    # Prediciton threadshold used to decide if two logos are the same during inference\n",
        "    prediciton_threashold = 0.5\n",
        " \n",
        "    \n",
        "   \n",
        "\n",
        "    freeze_layers = 0\n",
        "    # Transfer Learning Strategy\n",
        "    freeze_early_layers = True\n",
        "    # Unfreeze all layers after this specific epoch for fine-tuning\n",
        "    unfreeze_at_epoch = 5\n",
        "\n",
        "    # 5. LOSS FUNCTION\n",
        "    margin = 0.2           # Minimal distance between different logos \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import ssl\n",
        "import random\n",
        "import torch.nn as nn\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "SEED = 101\n",
        "random.seed(Config.seed)\n",
        "torch.manual_seed(Config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Custom PyTorch Dataset for Contrastive Learning with Bounding Box support (XML).\n",
        "\n",
        "Key operations:\n",
        "1. XML Parsing: Loads images and extracts object bounding boxes from Pascal VOC-style XML files.\n",
        "2. Coordinate Scaling: Adjusts bounding box coordinates to align with the resized/transformed image.\n",
        "3. Pair Generation: Returns a pair of images (Anchor + Second Image) and a binary label (0/1).\n",
        "   - Uses a forced 50/50 probability for Positive/Negative pairs.\n",
        "   - This balance is crucial: using the natural distribution would result in ~99% negatives, causing the model to simply learn to predict \"different\" for everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DatasetContrastive(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "      self.file_list = file_list\n",
        "      self.transform = transform\n",
        "\n",
        "      self.label_to_indices = defaultdict(list)\n",
        "      for idx, img_path in enumerate(self.file_list):\n",
        "        # Extract label from path: LogoDet-3K\\LogoDet-3K\\Clothes\\panerai\\21.jpg\n",
        "        # Label is the second-to-last part of the path\n",
        "        label = img_path.replace('\\\\', '/').split('/')[-2]\n",
        "        self.label_to_indices[label].append(idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        self.filelength =len(self.file_list)\n",
        "        return self.filelength\n",
        "\n",
        "    def load_image(self, image_path):\n",
        "        xml_path = image_path.replace(\".jpg\", \".xml\")\n",
        "        img = Image.open(image_path)\n",
        "        orig_w, orig_h = img.size\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        labels_list = []\n",
        "        bb_list = []\n",
        "\n",
        "        try:\n",
        "          tree = ET.parse(xml_path)\n",
        "          root = tree.getroot()\n",
        "        except Exception as e:\n",
        "          raise Exception(f\"Failed to parse XML file: {xml_path} | Error: {e}\")\n",
        "        \n",
        "        objects = root.findall(\"object\")\n",
        "\n",
        "        for obj in objects:\n",
        "          label = obj.find(\"name\").text\n",
        "          bbox = obj.find(\"bndbox\")\n",
        "          xmin = int(bbox.find(\"xmin\").text)\n",
        "          ymin = int(bbox.find(\"ymin\").text)\n",
        "          xmax = int(bbox.find(\"xmax\").text)\n",
        "          ymax = int(bbox.find(\"ymax\").text)\n",
        "\n",
        "          # Scale bounding boxes to match the resized image\n",
        "          new_w, new_h = img_transformed.shape[2], img_transformed.shape[1]\n",
        "          x_scale = new_w / orig_w\n",
        "          y_scale = new_h / orig_h\n",
        "\n",
        "          bbox_scaled = {\n",
        "              \"xmin\": int(xmin * x_scale),\n",
        "              \"ymin\": int(ymin * y_scale),\n",
        "              \"xmax\": int(xmax * x_scale),\n",
        "              \"ymax\": int(ymax * y_scale)\n",
        "          }\n",
        "        \n",
        "        # ERRORE DI INDENTAZIONE E IDX LABELS DA INTEGRARE\n",
        "        labels_list.append(label)\n",
        "        bb_list.append(bbox_scaled)\n",
        "\n",
        "        \n",
        "        return {\"image\": img_transformed, \"labels\": labels_list, \"bbs\": bb_list}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_path = self.file_list[idx]\n",
        "      label = img_path.replace('\\\\', '/').split('/')[-2]\n",
        "\n",
        "      img1 = self.load_image(img_path)\n",
        "\n",
        "      # decide if pair is positive or negative\n",
        "      # NON SO SE è MEGLIO UN 50/50 O SEE è MEGLIO LA DISTRIBUZIONE NATURALE DEL DATASE.\n",
        "      # CON LA DISTRIBUZIONE NATURALE è MOLTO PROBABILE CHE SOLO IMMAGINI NEGATIVE VENGANO ESTRATTE CHE NON CREDO SIA UN BENE PER IL TRAINING.\n",
        "      is_positive_pair = random.choice([0, 1])\n",
        "\n",
        "      if is_positive_pair:\n",
        "          # sample positive\n",
        "          pos_indices = [i for i in self.label_to_indices[label] if i != idx]\n",
        "          if len(pos_indices) == 0:\n",
        "              print(\"ERROR LOADING A POSITIVE MATCH FOR THE LOADED IMAGE\")\n",
        "              exit()\n",
        "          else:\n",
        "              idx2 = random.choice(pos_indices)\n",
        "              img2_path = self.file_list[idx2]\n",
        "              img2 = self.load_image(img2_path)\n",
        "      else:\n",
        "          # sample negative\n",
        "          neg_label = random.choice([l for l in self.label_to_indices.keys() if l != label])\n",
        "          idx2 = random.choice(self.label_to_indices[neg_label])\n",
        "          img2_path = self.file_list[idx2]\n",
        "          img2 = self.load_image(img2_path)\n",
        "\n",
        "      # is_positive_pair is returned for quick access so you dont have to compare labels after loading the images\n",
        "      return img1, img2, is_positive_pair\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splits the dataset into training and validation sets at the brand level, ensuring no class overlap.\n",
        "\n",
        "Key operations:\n",
        "1. Brand Separation: Divides brand folders into train/val subsets based on `val_split` using a fixed seed to ensure reproducibility.\n",
        "2. Adaptive Downsampling: If `total_set_size` is enforced, calculates the quota of images per brand. If this falls below `min_images_per_brand`, it reduces the number of participating brands to ensure the remaining ones meet the minimum image count.\n",
        "3. Image Collection: Randomly samples the calculated number of images for each selected brand, or retrieves all images if no total size limit is set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getTrainValPaths(root_dir, val_split, total_set_size=None, min_images_per_brand=2):\n",
        "    train_val_path = os.path.join(root_dir, 'train_val')\n",
        "    train_val_brands = []\n",
        "\n",
        "    # Collect brand folders\n",
        "    if not os.path.exists(train_val_path):\n",
        "        print(f\"Warning: {train_val_path} not found.\")\n",
        "        return [], []\n",
        "\n",
        "    for category in os.listdir(train_val_path):\n",
        "        cat_path = os.path.join(train_val_path, category)\n",
        "        if os.path.isdir(cat_path):\n",
        "            for brand in os.listdir(cat_path):\n",
        "                brand_full_path = os.path.join(cat_path, brand)\n",
        "                if os.path.isdir(brand_full_path):\n",
        "                    train_val_brands.append(brand_full_path)\n",
        "\n",
        "    # Split brands into Train and Val\n",
        "    val_size = int(len(train_val_brands) * val_split)\n",
        "    train_size = len(train_val_brands) - val_size\n",
        "    generator = torch.Generator().manual_seed(Config.seed)\n",
        "    train_subset, val_subset = random_split(train_val_brands, [train_size, val_size], generator=generator)\n",
        "    \n",
        "    train_brand_list = [train_val_brands[i] for i in train_subset.indices]\n",
        "    val_brand_list = [train_val_brands[i] for i in val_subset.indices]\n",
        "\n",
        "    train_data_list = []\n",
        "    val_data_list = []\n",
        "\n",
        "    # Sampling Logic\n",
        "    if total_set_size is not None:\n",
        "        images_per_brand = round(total_set_size / len(train_val_brands))\n",
        "        \n",
        "        if images_per_brand < min_images_per_brand:\n",
        "            print(f\"Not enough images per brand ({images_per_brand}), downscaling brand sets to ensure {min_images_per_brand} images/brand.\")\n",
        "            \n",
        "            # Calculate how many brands we can actually afford\n",
        "            new_total_brand_count = round(total_set_size / min_images_per_brand)\n",
        "            new_val_size = round(new_total_brand_count * val_split)\n",
        "            new_train_size = new_total_brand_count - new_val_size\n",
        "\n",
        "            train_brand_list = random.sample(train_brand_list, min(len(train_brand_list), new_train_size))\n",
        "            val_brand_list = random.sample(val_brand_list, min(len(val_brand_list), new_val_size))\n",
        "            images_per_brand = min_images_per_brand\n",
        "\n",
        "        for brand in train_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the TRAIN set\")\n",
        "\n",
        "            train_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "            \n",
        "        for brand in val_brand_list:\n",
        "            imgs = glob.glob(os.path.join(brand, '*.jpg'))\n",
        "\n",
        "            if len(imgs) < min_images_per_brand:\n",
        "                print(f\"images are less than {min_images_per_brand} for this brand: {brand} in the VALIDATION set\")\n",
        "            \n",
        "            val_data_list.extend(random.sample(imgs, min(images_per_brand, len(imgs))))\n",
        "    else:\n",
        "        for brand in train_brand_list:\n",
        "            train_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "        for brand in val_brand_list:\n",
        "            val_data_list.extend(glob.glob(os.path.join(brand, '*.jpg')))\n",
        "\n",
        "    return train_data_list, val_data_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "ResNet50-based architecture modified for Metric Learning (generating embeddings).\n",
        "\n",
        "Key operations:\n",
        "1. Backbone Initialization: Loads a standard ResNet50 (optionally with ImageNet weights).\n",
        "2. Head Replacement: Swaps the original 1000-class classifier with a linear projection layer to output embeddings of size `embedding_dim`.\n",
        "3. Progressive Freezing: Implements a custom `freeze_numer_of_layer` method to selectively freeze backbone blocks (from shallow 'conv1' to deep 'layer4') for controlled fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class LogoResNet50(nn.Module):\n",
        "    def __init__(self, embedding_dim=128, pretrained=True, num_of_freeze_layer=5, activation_fn=None):\n",
        "        super(LogoResNet50, self).__init__()\n",
        "        \n",
        "        # 1. Load Pre-trained Weights\n",
        "        # Initialize the model with weights pretrained on ImageNet for transfer learning\n",
        "        if pretrained:\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "            self.model = models.resnet50(weights=weights)\n",
        "        else:\n",
        "            self.model = models.resnet50(weights=None)\n",
        "            \n",
        "        # 2. Modify the Head (Fully Connected Layer)\n",
        "        # We need to produce feature embeddings instead of class probabilities\n",
        "        input_features_fc = self.model.fc.in_features # Typically 2048 for ResNet50\n",
        "        \n",
        "        head_layers = []\n",
        "        # Project features to the desired embedding dimension (e.g., 128)\n",
        "        head_layers.append(nn.Linear(input_features_fc, embedding_dim))\n",
        "        \n",
        "        # Add an optional activation function if provided\n",
        "        if activation_fn is not None:\n",
        "            head_layers.append(activation_fn)\n",
        "        \n",
        "        # Replace the original classifier with our custom embedding head\n",
        "        self.model.fc = nn.Sequential(*head_layers)\n",
        "\n",
        "        # 3. Freezing Management\n",
        "        # Define the blocks here to access them in the freeze method.\n",
        "        # This structure allows progressive freezing/unfreezing strategies\n",
        "        self.blocks = [\n",
        "            ['conv1', 'bn1'],   # Level 1\n",
        "            ['layer1'],         # Level 2\n",
        "            ['layer2'],         # Level 3\n",
        "            ['layer3'],         # Level 4\n",
        "            ['layer4'],         # Level 5: Entire backbone frozen\n",
        "        ]\n",
        "\n",
        "        # Apply the initial freezing configuration\n",
        "        self.freeze_numer_of_layer(num_of_freeze_layer)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def freeze_numer_of_layer(self, num_of_freeze_layer):\n",
        "        \"\"\"\n",
        "        Manages layer freezing for transfer learning strategies.\n",
        "        \n",
        "        Args:\n",
        "            num_of_freeze_layer (int):\n",
        "              0   -> All layers unlocked (Full Fine-Tuning)\n",
        "              1-5 -> Progressively freezes the backbone layers from shallow to deep\n",
        "        \"\"\"\n",
        "        \n",
        "        # STEP 1: RESET. Unfreeze everything (requires_grad = True).\n",
        "        # This ensures we start from a clean state before applying new constraints.\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # If num is 0, exit immediately (Full Fine-Tuning mode)\n",
        "        if num_of_freeze_layer == 0:\n",
        "            print(\"Configuration: Full Fine-Tuning (All layers are trainable)\")\n",
        "            return\n",
        "        \n",
        "        # Safety check to avoid index out of bounds\n",
        "        limit = min(num_of_freeze_layer, len(self.blocks))\n",
        "        \n",
        "        frozen_list = []\n",
        "\n",
        "        # STEP 2: Progressively freeze the requested blocks\n",
        "        for i in range(limit):\n",
        "            current_blocks = self.blocks[i]\n",
        "            for block_name in current_blocks:\n",
        "                # Retrieve the layer by name\n",
        "                layer = getattr(self.model, block_name)\n",
        "                \n",
        "                # Freeze parameters for this specific block\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "                \n",
        "                frozen_list.append(block_name)\n",
        "\n",
        "        print(f\"Freezing Level {limit}. Frozen blocks: {frozen_list}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main training script for a Siamese Network using Contrastive Learning.\n",
        "\n",
        "Key operations:\n",
        "1.  **Environment Setup**: Configures system paths to import custom modules (`configs`, `PY_script`) and handles SSL contexts for downloading pretrained weights.\n",
        "2.  **Data Preparation**: \n",
        "    -   Splits the dataset into training and validation sets using `getTrainValPaths`.\n",
        "    -   Applies **Data Augmentation** (RandomResizedCrop, Flip, ColorJitter) to the training set to improve model robustness.\n",
        "    -   Uses standard resizing and normalization for validation.\n",
        "3.  **Model Initialization**: Loads the `LogoResNet50` model (likely a ResNet50 backbone modified for embeddings), moving it to the configured device (GPU/CPU).\n",
        "4.  **Optimization Setup**: \n",
        "    -   **Loss**: Uses `CosineEmbeddingLoss`, which minimizes the distance between positive pairs and maximizes the distance between negative pairs.\n",
        "    -   **Optimizer**: Uses Adam to update model weights.\n",
        "5.  **Training Loop**:\n",
        "    -   Iterates through epochs and batches.\n",
        "    -   Extracts embeddings for `img1` and `img2`.\n",
        "    -   **Label Conversion**: Converts binary labels (0/1) into the format required by CosineEmbeddingLoss (1 for similar, -1 for dissimilar).\n",
        "    -   Performs backpropagation and updates weights.\n",
        "6.  **Validation Loop**: Evaluates the model on the validation set without computing gradients (`torch.no_grad`) to monitor generalization performance.\n",
        "7.  **Checkpointing**: Saves the model state dictionary every 5 epochs for future inference or resuming training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/Lenovo/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "print(\"Reading script train.py\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(f\"Starting training for: {Config.project_name}\")\n",
        "    \n",
        "    device = torch.device(Config.device)\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # 1. DATA AND AUGMENTATION \n",
        "    print(\"Loading dataset...\")\n",
        "\n",
        "    full_dataset_path = os.path.abspath(Config.dataset_root)\n",
        "    print(f\"Looking for dataset at: {full_dataset_path}\")\n",
        "    \n",
        "    if not os.path.exists(Config.dataset_root):\n",
        "        print(f\"CRITICAL ERROR: The folder '{Config.dataset_root}' does not exist.\")\n",
        "        print(\"Verify that you have downloaded the dataset and that the path in Config.dataset_root is correct.\")\n",
        "        return # Exit the function to avoid the crash\n",
        "\n",
        "        \n",
        "    train_files, val_files = getTrainValPaths(\n",
        "        Config.dataset_root, \n",
        "        val_split=Config.val_split_ratio,\n",
        "        min_images_per_brand=2\n",
        "    )\n",
        "\n",
        "# SAFETY CHECK\n",
        "    if len(train_files) == 0:\n",
        "        print(\"CRITICAL ERROR: No training files found.\")\n",
        "        print(f\"Check that '{Config.dataset_root}/train_val' contains the category and brand folders.\")\n",
        "        return # Exit the function to avoid the DataLoader error\n",
        "\n",
        "    print(f\"Training files found: {len(train_files)}\")\n",
        "    print(f\"Validation files found: {len(val_files)}\")\n",
        "\n",
        "    print(f\"Training files: {len(train_files)}\")\n",
        "\n",
        "    # Transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.5, 1.0)), \n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    train_dataset = DatasetContrastive(train_files, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=Config.batch_size, \n",
        "                          shuffle=True, \n",
        "                          num_workers=2, \n",
        "                          persistent_workers=True)\n",
        "    \n",
        "    # 2. VALIDATION DATASET \n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    val_dataset = DatasetContrastive(val_files, transform=val_transform)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # 3. MODEL \n",
        "    print(\"Inizialitation LogoResNet50\")\n",
        "    freeze_layers = getattr(Config, 'freeze_layers', 0)\n",
        "\n",
        "    model = LogoResNet50(\n",
        "        embedding_dim=Config.embedding_dim,\n",
        "        pretrained=Config.pretrained,\n",
        "        num_of_freeze_layer=freeze_layers \n",
        "    )\n",
        "    \n",
        "    model = model.to(device)\n",
        "\n",
        "    # 4. LOSS E OPTIMIZER\n",
        "    criterion = nn.CosineEmbeddingLoss(margin=Config.margin)\n",
        "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=Config.learning_rate)\n",
        "\n",
        "    # 5. TRAINING LOOP \n",
        "    print(\"Starting training cycle\")\n",
        "    \n",
        "    for epoch in range(Config.epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch_idx, (img1, img2, label) in enumerate(train_loader):\n",
        "            img1, img2, label = img1['image'].to(device), img2['image'].to(device), label.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            out1 = model(img1)\n",
        "            out2 = model(img2)\n",
        "            \n",
        "            target_label = label.float()\n",
        "            target_label[target_label == 0] = -1\n",
        "            \n",
        "            loss = criterion(out1, out2, target_label)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{Config.epochs}] Batch {batch_idx} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1} done! Average Loss: {avg_loss:.4f}\")\n",
        "        \n",
        "        # STARTING VALIDATION\n",
        "        model.eval() \n",
        "        val_loss = 0\n",
        "        with torch.no_grad(): \n",
        "            for v_img1, v_img2, v_label in val_loader:\n",
        "                v_img1, v_img2, v_label = v_img1['image'].to(device), v_img2['image'].to(device), v_label.to(device)\n",
        "                \n",
        "                v_out1 = model(v_img1)\n",
        "                v_out2 = model(v_img2)\n",
        "                \n",
        "                v_target = v_label.float()\n",
        "                v_target[v_target == 0] = -1\n",
        "                \n",
        "                loss = criterion(v_out1, v_out2, v_target)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f\"VALIDATION Epoch {epoch+1}: Loss = {avg_val_loss:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "\n",
        "        # Store checkpoint every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            if not os.path.exists(Config.checkpoints_dir):\n",
        "                os.makedirs(Config.checkpoints_dir)\n",
        "            torch.save(model.state_dict(), f\"{Config.checkpoints_dir}/model_epoch_{epoch+1}.pth\")\n",
        "            print(\"Checkpoint saved!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
